This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.ai/
  code-1.txt
  code-2.txt
  code-3.txt
  koi-howto.md
  Processor.md
config/
  docker/
    coordinator.yaml
    github-sensor.yaml
    global.env.example
    hackmd-sensor.yaml
    processor-a.yaml
    processor-b.yaml
  local/
    coordinator.yaml
    github-sensor.yaml
    global.env.example
    hackmd-sensor.yaml
    processor-a.yaml
    processor-b.yaml
nodes/
  koi-net-coordinator-node/
    coordinator_node/
      __init__.py
      __main__.py
      config_loader.py
      core.py
      handlers.py
      server.py
    .gitignore
    Dockerfile
    koi-net-coordinator-node.service
    LICENSE
    pyproject.toml
    README.md
    requirements.txt
  koi-net-github-sensor-node/
    github_sensor_node/
      handlers/
        github.py
      __init__.py
      __main__.py
      backfill.py
      config.py
      core.py
      loader.py
      server.py
      types.py
      webhook.py
    .env.example
    .gitignore
    Dockerfile
    github-node.service
    pyproject.toml
    repomix-output.xml
  koi-net-hackmd-sensor-node/
    hackmd_sensor_node/
      __init__.py
      __main__.py
      backfill.py
      config.py
      core.py
      hackmd_api.py
      handlers.py
      server.py
    .gitignore
    Dockerfile
    koi-net-hackmd-sensor-node.service
    LICENSE
    pyproject.toml
    README.md
    requirements.txt
    rid_types.py
  koi-net-processor-a-node/
    processor_a_node/
      __init__.py
      __main__.py
      config.py
      core.py
      handlers.py
      server.py
    .gitignore
    Dockerfile
    pyproject.toml
    README.md
  koi-net-processor-b-node/
    processor_b_node/
      __init__.py
      __main__.py
      config.py
      core.py
      handlers.py
      server.py
    .gitignore
    Dockerfile
    pyproject.toml
    README.md
rid_types/
  __init__.py
  github.py
  hackmd.py
  pyproject.toml
  README.md
scripts/
  prd.txt
.gitignore
code.txt
docker-compose.yaml
Makefile
plan.md
pyproject.toml
README.md
setup_path.py
test_rid_types.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="rid_types/__init__.py">
"""
Shared RID Types Package

This package re-exports RID type definitions from various sensor nodes,
providing a centralized location for accessing these types without
needing to import directly from the sensor node packages.
"""

# Import and re-export RID types
from .github import GithubCommit
from .hackmd import HackMDNote

__all__ = [
    "GithubCommit",
    "HackMDNote",
]
</file>

<file path="rid_types/github.py">
# Direct import of GithubCommit using relative import notation
# This works after installing the package with "uv pip install -e ."
try:
    from nodes.koi_net_github_sensor_node.github_sensor_node.types import GithubCommit
except ImportError:
    # Alternative direct import path - sometimes package structure varies
    from koi_net_github_sensor_node.github_sensor_node.types import GithubCommit

__all__ = ["GithubCommit"]
</file>

<file path="rid_types/hackmd.py">
# Direct import of HackMDNote using relative import notation
# This works after installing the package with "uv pip install -e ."
try:
    from nodes.koi_net_hackmd_sensor_node.rid_types import HackMDNote
except ImportError:
    # Alternative direct import path - sometimes package structure varies
    from koi_net_hackmd_sensor_node.rid_types import HackMDNote

__all__ = ["HackMDNote"]
</file>

<file path="rid_types/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "rid_types"
version = "0.1.0"
description = "Shared RID types for the KOI-net system"
authors = [
    {name = "KOI-net Team"}
]
readme = "README.md"
requires-python = ">=3.8"
dependencies = [
    "rid-lib>=3.2.1",
]

[project.optional-dependencies]
dev = [
    "pytest",
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="rid_types/README.md">

</file>

<file path="setup_path.py">
"""
Path setup module for the KOI-net project.

This module adds the project root to the Python path, enabling imports from 
the project's top-level packages like 'nodes'.

Usage:
    import setup_path  # At the beginning of your script
    
    # Now you can import from project packages
    from nodes.koi_net_github_sensor_node.github_sensor_node.types import GithubCommit
"""

import os
import sys

# Get the absolute path to the project root directory
project_root = os.path.dirname(os.path.abspath(__file__))

# Add the project root to the Python path if it's not already there
if project_root not in sys.path:
    sys.path.insert(0, project_root)
    print(f"Added {project_root} to Python path")
</file>

<file path="test_rid_types.py">
#!/usr/bin/env python3
"""
Test script to verify that the rid_types package correctly imports and
re-exports the RID types from the sensor nodes.
"""

import os
import sys

print("Testing rid_types package...")

# Ensure the project root is in the Python path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
    print(f"Added {project_root} to Python path")

# Import directly from the shared package
from rid_types import GithubCommit, HackMDNote

print("✅ Successfully imported GithubCommit and HackMDNote from rid_types package")

# Print type information
print(f"GithubCommit type: {type(GithubCommit)}")
print(f"HackMDNote type: {type(HackMDNote)}")
print(f"GithubCommit module: {GithubCommit.__module__}")
print(f"HackMDNote module: {HackMDNote.__module__}")

print("\nDone testing.")
</file>

<file path=".ai/code-1.txt">
<file_summary>
================================================
REPO: blockscience/koi-net
================================================
</file_summary>

Directory structure:
└── blockscience-koi-net/
    ├── README.md
    ├── koi-net-protocol-openapi.json
    ├── LICENSE
    ├── pyproject.toml
    ├── requirements.txt
    ├── examples/
    │   ├── basic_coordinator_node.py
    │   ├── basic_partial_node.py
    │   ├── full_node_template.py
    │   └── partial_node_template.py
    ├── src/
    │   └── koi_net/
    │       ├── __init__.py
    │       ├── config.py
    │       ├── core.py
    │       ├── identity.py
    │       ├── network/
    │       │   ├── __init__.py
    │       │   ├── graph.py
    │       │   ├── interface.py
    │       │   ├── request_handler.py
    │       │   └── response_handler.py
    │       ├── processor/
    │       │   ├── __init__.py
    │       │   ├── default_handlers.py
    │       │   ├── handler.py
    │       │   ├── interface.py
    │       │   └── knowledge_object.py
    │       └── protocol/
    │           ├── __init__.py
    │           ├── api_models.py
    │           ├── consts.py
    │           ├── edge.py
    │           ├── event.py
    │           ├── helpers.py
    │           └── node.py
    └── .github/
        └── workflows/
            └── publish-to-pypi.yml


Files Content:

================================================
FILE: README.md
================================================
# KOI-net

*This specification is the result of several iterations of KOI research, [read more here](https://github.com/BlockScience/koi).*

### Jump to Sections: 
- [Protocol](#protocol)
    - [Introduction](#introduction)
    - [Communication Methods](#communication-methods)
- [Quickstart](#quickstart)
    - [Setup](#setup)
    - [Creating a Node](#creating-a-node)
    - [Knowledge Processing](#knowledge-processing)
    - [Try It Out!](#try-it-out)
- [Advanced](#advanced)
    - [Knowledge Processing Pipeline](#knowledge-processing-pipeline)
    - [Knowledge Handlers](#knowledge-handlers)
        - [RID Handler](#rid-handler)
        - [Manifest Handler](#manifest-handler)
        - [Bundle Handler](#bundle-handler)
        - [Network Handler](#network-handler)
        - [Final Handler](#final-handler)
    - [Registering Handlers](#registering-handlers)
    - [Default Behavior](#default-behavior)
- [Implementation Reference](#implementation-reference)
    - [Node Interface](#node-interface)
    - [Node Identity](#node-identity)
    - [Network Interface](#network-interface)
        - [Network Graph](#network-graph)
        - [Request Handler](#request-handler)
        - [Response Handler](#response-handler)
    - [Processor Interface](#processor-interface)
- [Development](#development)
    - [Setup](#setup-1)
    - [Distribution](#distribution)

# Protocol
## Introduction

*This project builds upon and uses the [RID protocol](https://github.com/BlockScience/rid-lib) to identify and coordinate around knowledge objects.*

This protocol defines the standard communication patterns and coordination norms needed to establish and maintain Knowledge Organization Infrastructure (KOI) networks. KOI-nets are heterogenous compositions of KOI nodes, each of which is capable of autonomously inputting, processing, and outputting knowledge. The behavior of each node and configuration of each network can vary greatly, thus the protocol is designed to be a simple and flexible but interoperable foundation for future projects to build on. The protocol only governs communication between nodes, not how they operate internally. As a result we consider KOI-nets to be fractal-like, in that a network of nodes may act like a single node from an outside perspective.

Generated OpenAPI documentation is provided in this repository, and can be [viewed interactively with Swagger](https://generator.swagger.io/?url=https://raw.githubusercontent.com/BlockScience/koi-net/refs/heads/main/koi-net-protocol-openapi.json).

## Communication Methods

There are two classes of communication methods, event and state communication. 
- Event communication is one way, a node send an event to another node. 
- State communication is two way, a node asks another node for RIDs, manifests, or bundles and receives a response containing the requested resource (if available).

There are also two types of nodes, full and partial nodes. 
- Full nodes are web servers, implementing the endpoints defined in the KOi-net protocol. They are capable of receiving events via webhooks (another node calls their endpoint), and serving state queries. They can also call the endpoints of other full nodes to broadcast events or retrieve state. 
- Partial nodes are web clients and don't implement any API endpoints. They are capable of receiving events via polling (asking another node for events). They can also call the endpoints of full nodes to broadcast events or retrieve state.

There are five endpoints defined by the API spec. The first two are for event communication with full and partial nodes respectively. The remaining three are for state communication with full nodes. As a result, partial nodes are unable to directly transfer state and may only output events to other nodes.
- Broadcast events - `/events/broadcast`
- Poll events - `/events/poll`
- Fetch bundles - `/bundles/fetch`
- Fetch manifests - `/manifests/fetch`
- Fetch RIDs - `/rids/fetch`

All endpoints are called with via POST request with a JSON body, and will receive a response containing a JSON payload (with the exception of broadcast events, which won't return anything). The JSON schemas can be found in the attached OpenAPI specification or the Pydantic models in the "protocol" module.

The request and payload JSON objects are composed of the fundamental "knowledge types" from the RID / KOI-net system: RIDs, manifests, bundles, and events. RIDs, manifests, and bundles are defined by the RID protocol and imported from rid-lib, which you can [read about here](https://github.com/BlockScience/rid-lib). Events are now part of the KOI-net protocol, and are defined as an RID and an event type with an optional manifest and contents. 

```json
{
    "rid": "...",
    "event_type": "NEW | UPDATE | FORGET",
    "manifest": {
        "rid": "...",
        "timestamp": "...",
        "sha256_hash": "...",
    },
    "contents": {}
}
```

An event is a signalling construct that conveys information about RID objects between networked nodes. Events are composed of an RID, manifest, or bundle with an event type attached. Event types can be one of `"FORGET"`, `"UPDATE"`, or `"NEW"` forming the "FUN" acronym. 

As opposed to CRUD (create, read, update, delete), events are a series of messages, not operations. Each node has its own autonomy in deciding how to react based on the message it receives. For example, a processor node may receive a `"NEW"` event for an RID object its not interested in, and ignore it. Or it may decide that an `"UPDATE"` event should trigger fetching a bundle from another node. A node emits an event to indicate that its internal state has changed:
- `"NEW"` - indicates an previously unknown RID was cached
- `"UPDATE"` - indicates a previously known RID was cached
- `"FORGET"` - indicates a previously known RID was deleted

Nodes may broadcast events to other nodes to indicate their internal state changed. Conversely, nodes may also listen to events from other nodes and as a result decide to change their internal state, take some other action, or do nothing.


# Quickstart
## Setup

The bulk of the code in this repo is taken up by the Python reference implementation, which can be used in other projects to easily set up and configure your own KOI-net node.

This package can be installed with pip:
```shell
pip install koi-net
```

## Creating a Node

*Check out the `examples/` folder to follow along!*

All of the KOI-net functionality comes from the `NodeInterface` class which provides methods to interact with the protocol API, a local RID cache, a view of the network, and an internal processing pipeline. To create a new node, you will need to give it a name and a profile. The name will be used to generate its unique node RID, and the profile stores basic configuration data which will be shared with the other nodes that you communciate with.

Your first decision will be whether to setup a partial or full node:
- Partial nodes only need to indicate their type, and optionally the RID types of events they provide.
- Full nodes need to indicate their type, the base URL for their KOI-net API, and optionally the RID types of events and state they provide.

### Partial Node
```python
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeProvides, NodeType

node = NodeInterface(
    name="mypartialnode",
    profile=NodeProfile(
        node_type=NodeType.PARTIAL,
        provides=NodeProvides(
            event=[]
        )
    )
)
```
### Full Node
```python
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeProvides, NodeType

node = NodeInterface(
    name="myfullnode",
    profile=NodeProfile(
        base_url="http://127.0.0.1:8000",
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[],
            state=[]
        )
    ),
    use_kobj_processor_thread=True
)
```

When creating a node, you optionally enable `use_kobj_processor_thread` which will run the knowledge processing pipeline on a separate thread. This thread will automatically dequeue and process knowledge objects as they are added to the `kobj_queue`, which happenes when you call `node.process.handle(...)`. This is required to prevent race conditions in asynchronous applications, like web servers, therefore it is recommended to enable this feature for all full nodes. 

## Knowledge Processing

Next we'll set up the knowledge processing flow for our node. This is where most of the node's logic and behavior will come into play. For partial nodes this will be an event loop, and for full nodes we will use webhooks. Make sure to call `node.start()` and `node.stop()` at the beginning and end of your node's life cycle.

### Partial Node
Make sure to set `source=KnowledgeSource.External` when calling `handle` on external knowledge, this indicates to the knowledge processing pipeline that the incoming knowledge was received from another node. Where the knowledge is sourced from will impact decisions in the node's knowledge handlers.
```python
import time
from koi_net.processor.knowledge_object import KnowledgeSource

if __name__ == "__main__":
    node.start()

    try:
        while True:
            for event in node.network.poll_neighbors():
                node.processor.handle(event=event, source=KnowledgeSource.External)
            node.processor.flush_kobj_queue()
            
            time.sleep(5)
            
    finally:
        node.stop()
```

### Full Node
Setting up a full node is slightly more complex as we'll need a webserver. For this example, we'll use FastAPI and uvicorn. First we need to setup the "lifespan" of the server, to start and stop the node before and after execution, as well as the FastAPI app which will be our web server.
```python
from contextlib import asynccontextmanager
from fastapi import FastAPI

@asynccontextmanager
async def lifespan(app: FastAPI):
    node.start()
    yield
    node.stop()


app = FastAPI(lifespan=lifespan, root_path="/koi-net")
```

Next we'll add our event handling webhook endpoint, which will allow other nodes to broadcast events to us. You'll notice that we have a similar loop to our partial node, but instead of polling periodicially, we handle events asynchronously as we receive them from other nodes.

```python
from koi_net.protocol.api_models import *
from koi_net.protocol.consts import *

@app.post(BROADCAST_EVENTS_PATH)
def broadcast_events(req: EventsPayload):
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
```

Next we can add the event polling endpoint, this allows partial nodes to receive events from us.

```python
@app.post(POLL_EVENTS_PATH)
def poll_events(req: PollEvents) -> EventsPayload:
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)
```

Now for the state transfer "fetch" endpoints:
```python
@app.post(FETCH_RIDS_PATH)
def fetch_rids(req: FetchRids) -> RidsPayload:
    return node.network.response_handler.fetch_rids(req)

@app.post(FETCH_MANIFESTS_PATH)
def fetch_manifests(req: FetchManifests) -> ManifestsPayload:
    return node.network.response_handler.fetch_manifests(req)

@app.post(FETCH_BUNDLES_PATH)
def fetch_bundles(req: FetchBundles) -> BundlesPayload:
    return node.network.response_handler.fetch_bundles(req)
```

Finally we can run the server!

```python
import uvicorn

if __name__ == "__main__":
    # update this path to the Python module that defines "app"
    uvicorn.run("examples.full_node_template:app", port=8000)
```

*Note: If your node is not the first node in the network, you'll also want to set up a "first contact" in the `NodeInterface`. This is the URL of another full node that can be used to make your first connection and find out about other nodes in the network.*

## Try It Out!

In addition to the partial and full node templates, there's also example implementations that showcase a coordinator + partial node setup. You can run both of them locally after cloning this repository. First, install the koi-net library with the optional examples requirements from the root directory in the repo:
```shell
pip install .[examples]
```
Then you can start each node in a separate terminal:
```shell
python -m examples.basic_coordinator_node
```
```shell
python -m examples.basic_partial_node
```

# Advanced

## Knowledge Processing Pipeline
Beyond the `NodeInterface` setup and boiler plate for partial/full nodes, node behavior is mostly controlled through the use of knowledge handlers. Effectively creating your own handlers relies on a solid understanding of the knowledge processing pipeline, so we'll start with that. As a developer, you will interface with the pipeline through the `ProcessorInterface` accessed with `node.processor`. The pipeline handles knowledge objects, from the `KnowledgeObject` class, a container for all knowledge types in the RID / KOI-net ecosystem:
- RIDs
- Manifests
- Bundles
- Events

Here is the class definition for a knowledge object:
```python
type KnowledgeEventType = EventType | None

class KnowledgeSource(StrEnum):
    Internal = "INTERNAL"
    External = "EXTERNAL"

class KnowledgeObject(BaseModel):
    rid: RID
    manifest: Manifest | None = None
    contents: dict | None = None
    event_type: KnowledgeEventType = None
    source: KnowledgeSource
    normalized_event_type: KnowledgeEventType = None
    network_targets: set[KoiNetNode] = set()
```

In addition to the fields required to represent the knowledge types (`rid`, `manifest`, `contents`, `event_type`), knowledge objects also include a `source` field, indicating whether the knowledge originated from within the node (`KnowledgeSource.Internal`) or from another node (`KnowledgeSource.External`).

The final two fields are not inputs, but are set by handlers as the knowledge object moves through the processing pipeline. The normalized event type indicates the event type normalized to the perspective of the node's cache, and the network targets indicate where the resulting event should be broadcasted to.

Knowledge objects enter the processing pipeline through the `node.processor.handle(...)` method. Using kwargs you can pass any of the knowledge types listed above, a knowledge source, and an optional `event_type` (for non-event knowledge types). The handle function will simply normalize the provided knowledge type into a knowledge object, and put it in the `kobj_queue`, an internal, thread-safe queue of knowledge objects. If you have enabled `use_kobj_processor_thread` then the queue will be automatically processed on the processor thread, otherwise you will need to regularly call `flush_kobj_queue` to process queued knowledge objects (as in the partial node example). Both methods will process knowledge objects sequentially, in the order that they were queued in (FIFO). 


## Knowledge Handlers

Processing happens through five distinct phases, corresponding to the handler types: `RID`, `Manifest`, `Bundle`, `Network`, and `Final`. Each handler type can be understood by describing (1) what knowledge object fields are available to the handler, and (2) what action takes place after this phase, which the handler can influence. As knowledge objects pass through the pipeline, fields may be added or updated. 

Handlers are registered in a single handler array within the processor. There is no limit to the number of handlers in use, and multiple handlers can be assigned to the same handler type. At each phase of knowledge processing, we will chain together all of the handlers of the corresponding type and run them in their array order. The order handlers are registered in matters!

Each handler will be passed a knowledge object. They can choose to return one of three types: `None`, `KnowledgeObject`, or `STOP_CHAIN`. Returning `None` will pass the unmodified knowledge object (the same one the handler received) to the next handler in the chain. If a handler modified their knowledge object, they should return it to pass the new version to the next handler. Finally, a handler can return `STOP_CHAIN` to immediately stop processing the knowledge object. No further handlers will be called and it will not enter the next phase of processing.

Summary of processing pipeline:
```
RID -> Manifest -> Bundle -> [cache action] -> Network -> [network action] -> Final
           |
(skip if event type is "FORGET")
```

### RID Handler
The knowledge object passed to handlers of this type are guaranteed to have an RID and knowledge source field. This handler type acts as a filter, if none of the handlers return `STOP_CHAIN` the pipeline will progress to the next phase. The pipeline diverges slightly after this handler chain, based on the event type of the knowledge object.

If the event type is `"NEW"`, `"UPDATE"`, or `None` and the manifest is not already in the knowledge object, the node will attempt to retrieve it from (1) the local cache if the source is internal, or (2) from another node if the source is external. If it fails to retrieves the manifest, the pipeline will end. Next, the manifest handler chain will be called.

If the event type is `"FORGET"`, and the bundle (manifest + contents) is not already in the knowledge object, the node will attempt to retrieve it from the local cache, regardless of the source. In this case the knowledge object represents what we will delete from the cache, not new incoming knowledge. If it fails to retrieve the bundle, the pipeline will end. Next, the bundle handler chain will be called.

### Manifest Handler
The knowledge object passed to handlers of this type are guaranteed to have an RID, manifest, and knowledge source field. This handler type acts as a filter, if none of the handlers return `STOP_CHAIN` the pipeline will progress to the next phase.

If the bundle (manifest + contents) is not already in the knowledge object, the node will attempt to retrieve it from (1) the local cache if the source is internal, or (2) from another node if the source is external. If it fails to retrieve the bundle, the pipeline will end. Next, the bundle handler chain will be called.

### Bundle Handler
The knowledge object passed to handlers of this type are guaranteed to have an RID, manifest, bundle (manifest + contents), and knowledge source field. This handler type acts as a decider. In this phase, the knowledge object's normalized event type must be set to `"NEW"` or `"UPDATE"` to write it to cache, or `"FORGET"` to delete it from the cache. If the normalized event type remains unset (`None`), or a handler returns `STOP_CHAIN`, then the pipeline will end without taking any cache action.

The cache action will take place after the handler chain ends, so if multiple handlers set a normalized event type, the final handler will take precedence.

### Network Handler
The knowledge object passed to handlers of this type are guaranteed to have an RID, manifest, bundle (manifest + contents), normalized event type, and knowledge source field. This handler type acts as a decider. In this phase, handlers decide which nodes to broadcast this knowledge object to by appending KOI-net node RIDs to the knowledge object's `network_targets` field. If a handler returns `STOP_CHAIN`, the pipeline will end without taking any network action.

The network action will take place after the handler chain ends. The node will attempt to broadcast a "normalized event", created from the knowledge object's RID, bundle, and normalized event type, to all of the node's in the network targets array. 

### Final Handler
The knowledge object passed to handlers of this type are guaranteed to have an RID, manifest, bundle (manifest + contents), normalized event type, and knowledge source field.

This is the final handler chain that is called, it doesn't make any decisions or filter for succesive handler types. Handlers here can be useful if you want to take some action after the network broadcast has ended.

## Registering Handlers
Knowledge handlers are registered with a node's processor by decorating a handler function. There are two types of decorators, the first way converts the function into a handler object which can be manually added to a processor. This is how the default handlers are defined, and makes them more portable (could be imported from another package). The second automatically registers a handler with your node instance. This is not portable but more convenient. The input of the decorated function will be the processor instance, and a knowledge object.

```python
from .handler import KnowledgeHandler, HandlerType, STOP_CHAIN

@KnowledgeHandler.create(HandlerType.RID)
def example_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    ...

@node.processor.register_handler(HandlerType.RID)
def example_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    ...
```

While handler's only require specifying the handler type, you can also specify the RID types, knowledge source, or event types you want to handle. If a knowledge object doesn't match all of the specified parameters, it won't be called. By default, handlers will match all RID types, all event types, and both internal and external sourced knowledge.

```python
@KnowledgeHandler.create(
    handler_type=HandlerType.Bundle, 
    rid_types=[KoiNetEdge], 
    source=KnowledgeSource.External,
    event_types=[EventType.NEW, EventType.UPDATE])
def edge_negotiation_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    ...
```

The processor instance passed to your function should be used to take any necessary node actions (cache, network, etc.). It is also sometimes useful to add new knowledge objects to the queue while processing a different knowledge object. You can simply call `processor.handle(...)` in the same way as you would outside of a handler. It will put at the end of the queue and processed when it is dequeued like any other knowledge object.


## Default Behavior

The default configuration provides four default handlers which will take precedence over any handlers you add yourself. To override this behavior, you can set the `handlers` field in the `NodeInterface`:

```python
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeProvides, NodeType
from koi_net.processor.default_handlers import (
    basic_rid_handler,
    basic_manifest_handler,
    edge_negotiation_handler,
    basic_network_output_filter
)

node = NodeInterface(
    name="mypartialnode",
    profile=NodeProfile(
        node_type=NodeType.PARTIAL,
        provides=NodeProvides(
            event=[]
        )
    ),
    handlers=[
        basic_rid_handler,
        basic_manifest_handler,
        edge_negotiation_handler,
        basic_network_output_filter

        # include all or none of the default handlers
    ]
)
```

Take a look at `src/koi_net/processor/default_handlers.py` to see some more in depth examples and better understand the default node behavior.

# Implementation Reference
This section provides high level explanations of the Python implementation. More detailed explanations of methods can be found in the docstrings within the codebase itself.

## Node Interface
The node class mostly acts as a container for other classes with more specialized behavior, with special functions that should be called to start up and shut down a node. We'll take a look at each of these components in turn, but here is the class stub:
```python
class NodeInterface:
    cache: Cache
    identity: NodeIdentity
    network: NetworkInterface
    processor: ProcessorInterface
    first_contact: str
    use_kobj_processor_thread: bool

    def __init__(
        self, 
        name: str,
        profile: NodeProfile,
        identity_file_path: str = "identity.json",
        event_queues_file_path: str = "event_queues.json",
        cache_directory_path: str = "rid_cache",
        use_kobj_processor_thread: bool = False,
        first_contact: str | None = None,
        handlers: list[KnowledgeHandler] | None = None,
        cache: Cache | None = None,
        network: NetworkInterface | None = None,
        processor: ProcessorInterface | None = None
    ): ...

    def start(self): ...
    def stop(self): ...
```
As you can see, only a name and profile are required. The other fields allow for additional customization if needed.

## Node Identity
The `NodeIdentity` class provides easy access to a node's own RID, profile, and bundle. It provides access to the following properties after initialization, accessed with `node.identity`.
```python
class NodeIdentity:
    rid: KoiNetNode # an RID type
    profile: NodeProfile
    bundle: Bundle
```
This it what is initialized from the required `name` and `profile` fields in the `NodeInterface` constructor. Node RIDs take the form of `orn:koi-net.node:<name>+<uuid>`, and are generated on first use to the identity JSON file along with a the node profile.

## Network Interface
The `NetworkInterface` class provides access to high level network actions, and contains several other network related classes. It is accessed with `node.network`.
```python
class NetworkInterface:
    graph: NetworkGraph
    request_handler: RequestHandler
    response_handler: ResponseHandler

    def __init__(
        self, 
        file_path: str,
        first_contact: str | None,
        cache: Cache, 
        identity: NodeIdentity
    ): ...

    def push_event_to(self, event: Event, node: KoiNetNode, flush=False): ...
    
    def flush_poll_queue(self, node: KoiNetNode) -> list[Event]: ...
    def flush_webhook_queue(self, node: RID): ...

    def fetch_remote_bundle(self, rid: RID): ...
    def fetch_remote_manifest(self, rid: RID): ...

    def get_state_providers(self, rid_type: RIDType): ...
    def poll_neighbors(self) -> list[Event]: ...
```

Most of the provided functions are abstractions for KOI-net protocol actions. It also contains three lower level classes: `NetworkGraph`, `RequestHandler`, and `ResponseHandler`.

### Network Graph
The `NetworkGraph` class provides access to a graph view of the node's KOI network: all of the KOI-net node and edge objects it knows about (stored in local cache). This view allows us to query nodes that we have edges with to make networking decisions.
```python
class NetworkGraph:
    dg: nx.DiGraph

    def __init__(
        self,
        cache: Cache,
        identity: NodeIdentity
    ): ...

    def generate(self): ...

    def get_edges(
        self, 
        direction: Literal["in", "out"] | None = None
    ) -> list[KoiNetEdge]: ...

    def get_neighbors(
        self,
        direction: Literal["in", "out"] | None = None,
        status: EdgeStatus | None = None,
        allowed_type: RIDType | None = None
    ) -> list[KoiNetNode]: ...
    
    def get_node_profile(self, rid: KoiNetNode) -> NodeProfile | None: ...
    def get_edge_profile(
        self,
        rid: KoiNetEdge | None = None,
        source: KoiNetNode | None = None,
        target: KoiNetNode | None = None
    ) -> EdgeProfile | None: ...
```

### Request Handler
Handles raw API requests to other nodes through the KOI-net protocol. Accepts a node RID or direct URL as the target. Each method requires either a valid request model, or `kwargs` which will be converted to the correct model in `koi_net.protocol.api_models`.
```python
class RequestHandler:
    def __init__(self, cache: Cache, graph: NetworkGraph): ...

    def broadcast_events(
        self, 
        node: RID = None, 
        url: str = None, 
        req: EventsPayload | None = None,
        **kwargs
    ) -> None: ...

    def poll_events(
        self, 
        node: RID = None, 
        url: str = None, 
        req: PollEvents | None = None,
        **kwargs
    ) -> EventsPayload: ...

    def fetch_rids(
        self, 
        node: RID = None, 
        url: str = None, 
        req: FetchRids | None = None,
        **kwargs
    ) -> RidsPayload: ...

    def fetch_manifests(
        self, 
        node: RID = None, 
        url: str = None, 
        req: FetchManifests | None = None,
        **kwargs
    ) -> ManifestsPayload: ...

    def fetch_bundles(
        self, 
        node: RID = None, 
        url: str = None, 
        req: FetchBundles | None = None,
        **kwargs
    ) -> BundlesPayload: ...
```

### Response Handler
Handles raw API responses to requests from other nodes through the KOI-net protocol.
```python
class ResponseHandler:
    def __init__(self, cache: Cache): ...

    def fetch_rids(self, req: FetchRids) -> RidsPayload:
    def fetch_manifests(self, req: FetchManifests) -> ManifestsPayload:
    def fetch_bundles(self, req: FetchBundles) -> BundlesPayload:
```
Only fetch methods are provided right now, event polling and broadcasting can be handled like this:
```python
def broadcast_events(req: EventsPayload) -> None:
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    node.processor.flush_kobj_queue()

def poll_events(req: PollEvents) -> EventsPayload:
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)
```

## Processor Interface
The `ProcessorInterface` class provides access to a node's internal knowledge processing pipeline.
```python
class ProcessorInterface:
    worker_thread: threading.Thread | None = None

    def __init__(
        self, 
        cache: Cache, 
        network: NetworkInterface,
        identity: NodeIdentity,
        use_kobj_processor_thread: bool,
        default_handlers: list[KnowledgeHandler] = []
    ): ...

    def add_handler(self, handler: KnowledgeHandler): ...
    
    def register_handler(
        self,
        handler_type: HandlerType,
        rid_types: list[RIDType] | None = None
    ): ...

    def process_kobj(self, kobj: KnowledgeObject) -> None:
    def flush_kobj_queue(self): ...

    def handle(
        self,
        rid: RID | None = None,
        manifest: Manifest | None = None,
        bundle: Bundle | None = None,
        event: Event | None = None,
        kobj: KnowledgeObject | None = None,
        event_type: KnowledgeEventType = None,
        source: KnowledgeSource = KnowledgeSource.Internal
    ): ...
```

The `register_handler` method is a decorator which can wrap a function to create a new `KnowledgeHandler` and add it to the processing pipeline in a single step. The `add_handler` method adds an existing `KnowledgeHandler` to the processining pipeline.

The most commonly used functions in this class are `handle` and `flush_kobj_queue`. The `handle` method can be called on RIDs, manifests, bundles, and events to convert them to normalized to `KnowledgeObject` instances which are then added to the processing queue. If you have enabled `use_kobj_processor_thread` then the queue will be automatically processed, otherwise you will need to regularly call `flush_kobj_queue` to process queued knolwedge objects. When calling the `handle` method, knowledge objects are marked as internally source by default. If you are handling RIDs, manifests, bundles, or events sourced from other nodes, `source` should be set to `KnowledgeSource.External`.

Here is an example of how an event polling loop would be implemented using the knowledge processing pipeline:
```python
for event in node.network.poll_neighbors():
    node.processor.handle(event=event, source=KnowledgeSource.External)
node.processor.flush_kobj_queue()
```

# Development
## Setup
Clone this repository:
```console
git clone https://github.com/BlockScience/koi-net
```
Set up and activate virtual environment:
```shell
python -m venv venv
```
Windows:
```shell
.\venv\Scripts\activate
```
Linux:
```shell
source venv/bin/activate
```
Install koi-net with dev dependencies:
```shell
pip install -e .[dev]
```
## Distribution
*Be careful! All files not in `.gitignore` will be included in the distribution, even if they aren't tracked by git! Double check the `.tar.gz` after building to make sure you didn't accidently include other files.*

Build package:
```shell
python -m build
```
Push new package build to PyPI:
```shell
python -m twine upload --skip-existing dist/*
```


================================================
FILE: koi-net-protocol-openapi.json
================================================
{
  "openapi": "3.1.0",
  "info": {
    "title": "KOI-net Protocol API",
    "version": "1.0.0"
  },
  "paths": {
    "/events/broadcast": {
      "post": {
        "summary": "Broadcast Events",
        "operationId": "broadcast_events_events_broadcast_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/EventsPayload-Input"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {}
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/events/poll": {
      "post": {
        "summary": "Poll Events",
        "operationId": "poll_events_events_poll_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/PollEvents"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/EventsPayload-Output"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/rids/fetch": {
      "post": {
        "summary": "Fetch Rids",
        "operationId": "fetch_rids_rids_fetch_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/FetchRids"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/RidsPayload"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/manifests/fetch": {
      "post": {
        "summary": "Fetch Manifests",
        "operationId": "fetch_manifests_manifests_fetch_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/FetchManifests"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ManifestsPayload"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/bundles/fetch": {
      "post": {
        "summary": "Fetch Bundles",
        "operationId": "fetch_bundles_bundles_fetch_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/FetchBundles"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/BundlesPayload"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    }
  },
  "components": {
    "schemas": {
      "Bundle": {
        "properties": {
          "manifest": {
            "$ref": "#/components/schemas/Manifest"
          },
          "contents": {
            "type": "object",
            "title": "Contents"
          }
        },
        "type": "object",
        "required": [
          "manifest",
          "contents"
        ],
        "title": "Bundle",
        "description": "A Knowledge Bundle composed of a manifest and optional contents associated with an RIDed object.\n\nA container object for the cached data associated with an RID. It is \nreturned by the read function of Cache."
      },
      "BundlesPayload": {
        "properties": {
          "bundles": {
            "items": {
              "$ref": "#/components/schemas/Bundle"
            },
            "type": "array",
            "title": "Bundles"
          },
          "not_found": {
            "items": {
              "type": "string",
              "format": "rid"
            },
            "type": "array",
            "title": "Not Found",
            "default": []
          },
          "deferred": {
            "items": {
              "type": "string",
              "format": "rid"
            },
            "type": "array",
            "title": "Deferred",
            "default": []
          }
        },
        "type": "object",
        "required": [
          "bundles"
        ],
        "title": "BundlesPayload"
      },
      "Event": {
        "properties": {
          "rid": {
            "type": "string",
            "format": "rid",
            "title": "Rid"
          },
          "event_type": {
            "$ref": "#/components/schemas/EventType"
          },
          "manifest": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/Manifest"
              },
              {
                "type": "null"
              }
            ]
          },
          "contents": {
            "anyOf": [
              {
                "type": "object"
              },
              {
                "type": "null"
              }
            ],
            "title": "Contents"
          }
        },
        "type": "object",
        "required": [
          "rid",
          "event_type"
        ],
        "title": "Event"
      },
      "EventType": {
        "type": "string",
        "enum": [
          "NEW",
          "UPDATE",
          "FORGET"
        ],
        "title": "EventType"
      },
      "EventsPayload-Input": {
        "properties": {
          "events": {
            "items": {
              "$ref": "#/components/schemas/Event"
            },
            "type": "array",
            "title": "Events"
          }
        },
        "type": "object",
        "required": [
          "events"
        ],
        "title": "EventsPayload"
      },
      "EventsPayload-Output": {
        "properties": {
          "events": {
            "items": {
              "$ref": "#/components/schemas/Event"
            },
            "type": "array",
            "title": "Events"
          }
        },
        "type": "object",
        "required": [
          "events"
        ],
        "title": "EventsPayload"
      },
      "FetchBundles": {
        "properties": {
          "rids": {
            "items": {
              "type": "string",
              "format": "rid"
            },
            "type": "array",
            "title": "Rids"
          }
        },
        "type": "object",
        "required": [
          "rids"
        ],
        "title": "FetchBundles"
      },
      "FetchManifests": {
        "properties": {
          "rid_types": {
            "items": {
              "type": "string",
              "format": "rid-type"
            },
            "type": "array",
            "title": "Rid Types",
            "default": []
          },
          "rids": {
            "items": {
              "type": "string",
              "format": "rid"
            },
            "type": "array",
            "title": "Rids",
            "default": []
          }
        },
        "type": "object",
        "title": "FetchManifests"
      },
      "FetchRids": {
        "properties": {
          "rid_types": {
            "items": {
              "type": "string",
              "format": "rid-type"
            },
            "type": "array",
            "title": "Rid Types",
            "default": []
          }
        },
        "type": "object",
        "title": "FetchRids"
      },
      "HTTPValidationError": {
        "properties": {
          "detail": {
            "items": {
              "$ref": "#/components/schemas/ValidationError"
            },
            "type": "array",
            "title": "Detail"
          }
        },
        "type": "object",
        "title": "HTTPValidationError"
      },
      "Manifest": {
        "properties": {
          "rid": {
            "type": "string",
            "format": "rid",
            "title": "Rid"
          },
          "timestamp": {
            "type": "string",
            "format": "date-time",
            "title": "Timestamp"
          },
          "sha256_hash": {
            "type": "string",
            "title": "Sha256 Hash"
          }
        },
        "type": "object",
        "required": [
          "rid",
          "timestamp",
          "sha256_hash"
        ],
        "title": "Manifest"
      },
      "ManifestsPayload": {
        "properties": {
          "manifests": {
            "items": {
              "$ref": "#/components/schemas/Manifest"
            },
            "type": "array",
            "title": "Manifests"
          },
          "not_found": {
            "items": {
              "type": "string",
              "format": "rid"
            },
            "type": "array",
            "title": "Not Found",
            "default": []
          }
        },
        "type": "object",
        "required": [
          "manifests"
        ],
        "title": "ManifestsPayload"
      },
      "PollEvents": {
        "properties": {
          "rid": {
            "type": "string",
            "format": "rid",
            "title": "Rid"
          },
          "limit": {
            "type": "integer",
            "title": "Limit",
            "default": 0
          }
        },
        "type": "object",
        "required": [
          "rid"
        ],
        "title": "PollEvents"
      },
      "RidsPayload": {
        "properties": {
          "rids": {
            "items": {
              "type": "string",
              "format": "rid"
            },
            "type": "array",
            "title": "Rids"
          }
        },
        "type": "object",
        "required": [
          "rids"
        ],
        "title": "RidsPayload"
      },
      "ValidationError": {
        "properties": {
          "loc": {
            "items": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "integer"
                }
              ]
            },
            "type": "array",
            "title": "Location"
          },
          "msg": {
            "type": "string",
            "title": "Message"
          },
          "type": {
            "type": "string",
            "title": "Error Type"
          }
        },
        "type": "object",
        "required": [
          "loc",
          "msg",
          "type"
        ],
        "title": "ValidationError"
      }
    }
  }
}


================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 BlockScience

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "koi-net"
version = "1.0.0-beta.13"
description = "Implementation of KOI-net protocol in Python"
authors = [
    {name = "Luke Miller", email = "luke@block.science"}
]
readme = "README.md"
requires-python = ">=3.10"
license = {file = "LICENSE"}
dependencies = [
    "rid-lib>=3.2.1",
    "networkx>=3.4.2",
    "httpx>=0.28.1",
    "pydantic>=2.10.6"
]

[project.optional-dependencies]
dev = ["twine>=6.0", "build"]
examples = [
    "rich",
    "fastapi",
    "uvicorn"
]

[project.urls]
Homepage = "https://github.com/BlockScience/koi-net/"


================================================
FILE: requirements.txt
================================================
networkx>=3.4.2
rid-lib>=3.2.1
httpx>=0.28.1
pydantic>=2.10.6

# requirements for examples/
rich
fastapi
uvicorn


================================================
FILE: examples/basic_coordinator_node.py
================================================
import json
import logging
import uvicorn
from contextlib import asynccontextmanager
from rich.logging import RichHandler
from fastapi import FastAPI
from rid_lib.types import KoiNetNode, KoiNetEdge
from koi_net import NodeInterface
from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.event import Event, EventType
from koi_net.protocol.helpers import generate_edge_bundle
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides
from koi_net.processor import ProcessorInterface
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH
)


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[RichHandler()]
)

logging.getLogger("koi_net").setLevel(logging.DEBUG)

port = 8000

node = NodeInterface(
    name="coordinator",
    profile=NodeProfile(
        base_url=f"http://127.0.0.1:{port}/koi-net",
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[KoiNetNode, KoiNetEdge],
            state=[KoiNetNode, KoiNetEdge]
        )
    ),
    use_kobj_processor_thread=True,
    cache_directory_path="coordinator_node_rid_cache",
    event_queues_file_path="coordinator_node_event_queus.json",
    identity_file_path="coordinator_node_identity.json",
)


logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handshake_handler(proc: ProcessorInterface, kobj: KnowledgeObject):    
    logger.info("Handling node handshake")

    # only respond if node declares itself as NEW
    if kobj.event_type != EventType.NEW:
        return
        
    logger.info("Sharing this node's bundle with peer")
    proc.network.push_event_to(
        event=Event.from_bundle(EventType.NEW, proc.identity.bundle),
        node=kobj.rid,
        flush=True
    )
    
    logger.info("Proposing new edge")    
    # defer handling of proposed edge
    proc.handle(bundle=generate_edge_bundle(
        source=kobj.rid,
        target=proc.identity.rid,
        edge_type=EdgeType.WEBHOOK,
        rid_types=[KoiNetNode, KoiNetEdge]
    ))



@asynccontextmanager
async def lifespan(app: FastAPI):
    node.start()
    yield
    node.stop()

app = FastAPI(
    lifespan=lifespan, 
    root_path="/koi-net",
    title="KOI-net Protocol API",
    version="1.0.0"
)

@app.post(BROADCAST_EVENTS_PATH)
def broadcast_events(req: EventsPayload):
    logger.info(f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)")
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    
@app.post(POLL_EVENTS_PATH)
def poll_events(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)

@app.post(FETCH_RIDS_PATH)
def fetch_rids(req: FetchRids) -> RidsPayload:
    return node.network.response_handler.fetch_rids(req)

@app.post(FETCH_MANIFESTS_PATH)
def fetch_manifests(req: FetchManifests) -> ManifestsPayload:
    return node.network.response_handler.fetch_manifests(req)

@app.post(FETCH_BUNDLES_PATH)
def fetch_bundles(req: FetchBundles) -> BundlesPayload:
    return node.network.response_handler.fetch_bundles(req)
    
if __name__ == "__main__":
    openapi_spec = app.openapi()

    with open("koi-net-protocol-openapi.json", "w") as f:
        json.dump(openapi_spec, f, indent=2)
    
    uvicorn.run("examples.basic_coordinator_node:app", port=port)


================================================
FILE: examples/basic_partial_node.py
================================================
import time
import logging
from rich.logging import RichHandler
from koi_net import NodeInterface
from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeSource, KnowledgeObject
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.node import NodeProfile, NodeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[RichHandler()]
)

logging.getLogger("koi_net").setLevel(logging.DEBUG)
logger = logging.getLogger(__name__)


node = NodeInterface(
    name="partial",
    profile=NodeProfile(
        node_type=NodeType.PARTIAL
    ),
    cache_directory_path="partial_node_rid_cache",
    event_queues_file_path="parital_node_event_queus.json",
    identity_file_path="partial_node_identity.json",
    first_contact="http://127.0.0.1:8000/koi-net"
)

@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    # when I found out about a new node
    if kobj.normalized_event_type != EventType.NEW: 
        return
    
    node_profile = kobj.bundle.validate_contents(NodeProfile)
    
    # looking for event provider of nodes
    if KoiNetNode not in node_profile.provides.event:
        return
    
    logger.info("Identified a coordinator!")
    logger.info("Proposing new edge")
    
    # queued for processing
    processor.handle(bundle=generate_edge_bundle(
        source=kobj.rid,
        target=node.identity.rid,
        edge_type=EdgeType.POLL,
        rid_types=[KoiNetNode]
    ))
    
    logger.info("Catching up on network state")
    
    payload = processor.network.request_handler.fetch_rids(kobj.rid, rid_types=[KoiNetNode])
    for rid in payload.rids:
        if rid == processor.identity.rid:
            logger.info("Skipping myself")
            continue
        if processor.cache.exists(rid):
            logger.info(f"Skipping known RID '{rid}'")
            continue
        
        # marked as external since we are handling RIDs from another node
        # will fetch remotely instead of checking local cache
        processor.handle(rid=rid, source=KnowledgeSource.External)
    logger.info("Done")
    


node.start()

while True:
    for event in node.network.poll_neighbors():
        node.processor.handle(event=event, source=KnowledgeSource.External)
    node.processor.flush_kobj_queue()
    
    time.sleep(5)


================================================
FILE: examples/full_node_template.py
================================================
import logging
import uvicorn
from contextlib import asynccontextmanager
from rich.logging import RichHandler
from fastapi import FastAPI
from rid_lib.types import KoiNetNode, KoiNetEdge
from koi_net import NodeInterface
from koi_net.processor.knowledge_object import KnowledgeSource

from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH
)


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[RichHandler()]
)

logger = logging.getLogger(__name__)
logging.getLogger("koi_net").setLevel(logging.DEBUG)

port = 5000
coordinator_url = "http://127.0.0.1:8000/koi-net"

node = NodeInterface(
    name="coordinator",
    profile=NodeProfile(
        base_url=f"http://127.0.0.1:{port}/koi-net",
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[KoiNetNode, KoiNetEdge],
            state=[KoiNetNode, KoiNetEdge]
        )
    ),
    use_kobj_processor_thread=True,
    first_contact=coordinator_url
)


@asynccontextmanager
async def lifespan(app: FastAPI):
    node.start()
    yield
    node.stop()


app = FastAPI(lifespan=lifespan, root_path="/koi-net")

@app.post(BROADCAST_EVENTS_PATH)
def broadcast_events(req: EventsPayload):
    logger.info(f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)")
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    
@app.post(POLL_EVENTS_PATH)
def poll_events(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)

@app.post(FETCH_RIDS_PATH)
def fetch_rids(req: FetchRids) -> RidsPayload:
    return node.network.response_handler.fetch_rids(req)

@app.post(FETCH_MANIFESTS_PATH)
def fetch_manifests(req: FetchManifests) -> ManifestsPayload:
    return node.network.response_handler.fetch_manifests(req)

@app.post(FETCH_BUNDLES_PATH)
def fetch_bundles(req: FetchBundles) -> BundlesPayload:
    return node.network.response_handler.fetch_bundles(req)
    
    
if __name__ == "__main__":
    # update this path to the Python module that defines "app"
    uvicorn.run("examples.full_node_template:app", port=port)


================================================
FILE: examples/partial_node_template.py
================================================
import time
import logging
from rich.logging import RichHandler
from koi_net import NodeInterface
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.node import NodeProfile, NodeType


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[RichHandler()]
)

logger = logging.getLogger(__name__)


coordinator_url = "http://127.0.0.1:8000/koi-net"

node = NodeInterface(
    name="partial",
    profile=NodeProfile(
        node_type=NodeType.PARTIAL,
    ),
    first_contact=coordinator_url
)

if __name__ == "__main__":
    node.start()

    try:
        while True:
            for event in node.network.poll_neighbors():
                node.processor.handle(event=event, source=KnowledgeSource.External)
            node.processor.flush_kobj_queue()
            
            time.sleep(5)
            
    finally:
        node.stop()


================================================
FILE: src/koi_net/__init__.py
================================================
from .core import NodeInterface


================================================
FILE: src/koi_net/config.py
================================================
import os
from typing import TypeVar
from ruamel.yaml import YAML
from koi_net.protocol.node import NodeProfile
from rid_lib.types import KoiNetNode
from pydantic import BaseModel, Field, PrivateAttr
from dotenv import load_dotenv


class ServerConfig(BaseModel):
    host: str | None = "127.0.0.1"
    port: int | None = 8000
    path: str | None = None
    
    @property
    def url(self):
        return f"http://{self.host}:{self.port}{self.path or ''}"

class KoiNetConfig(BaseModel):
    node_name: str
    node_rid: KoiNetNode | None = None
    node_profile: NodeProfile
    
    cache_directory_path: str | None = ".rid_cache"
    event_queues_path: str | None = "event_queues.json"

    first_contact: str | None = None

class EnvConfig(BaseModel):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        load_dotenv()
    
    def __getattribute__(self, name):
        value = super().__getattribute__(name)
        if name in type(self).model_fields:
            env_val = os.getenv(value)
            if env_val is None:
                raise ValueError(f"Required environment variable {value} not set")
            return env_val
        return value

class Config(BaseModel):
    server: ServerConfig | None = Field(default_factory=ServerConfig)
    koi_net: KoiNetConfig
    _file_path: str = PrivateAttr(default="config.yaml")
    _file_content: str | None = PrivateAttr(default=None)
    
    @classmethod
    def load_from_yaml(
        cls, 
        file_path: str | None = None, 
        generate_missing: bool = True
    ):
        yaml = YAML()
        
        try:
            with open(file_path, "r") as f:
                file_content = f.read()
            config_data = yaml.load(file_content)
            config = cls.model_validate(config_data)
            config._file_content = file_content
            
        except FileNotFoundError:
            config = cls()
            
        config._file_path = file_path
        
        if generate_missing:            
            config.koi_net.node_rid = (
                config.koi_net.node_rid or KoiNetNode.generate(config.koi_net.node_name)
            )   
            config.koi_net.node_profile.base_url = (
                config.koi_net.node_profile.base_url or config.server.url
            )
                
            config.save_to_yaml()
                    
        return config
    
    def save_to_yaml(self):
        yaml = YAML()
        
        with open(self._file_path, "w") as f:
            try:
                config_data = self.model_dump(mode="json")
                yaml.dump(config_data, f)
            except Exception as e:
                if self._file_content:
                    f.seek(0)
                    f.truncate()
                    f.write(self._file_content)
                raise e
                
ConfigType = TypeVar("ConfigType", bound=Config)



================================================
FILE: src/koi_net/core.py
================================================
import logging
from typing import Generic
import httpx
from rid_lib.ext import Cache, Bundle
from .network import NetworkInterface
from .processor import ProcessorInterface
from .processor import default_handlers
from .processor.handler import KnowledgeHandler
from .identity import NodeIdentity
from .protocol.event import Event, EventType
from .config import ConfigType

logger = logging.getLogger(__name__)



class NodeInterface(Generic[ConfigType]):
    config: ConfigType
    cache: Cache
    identity: NodeIdentity
    network: NetworkInterface
    processor: ProcessorInterface
    
    use_kobj_processor_thread: bool
    
    def __init__(
        self, 
        config: ConfigType,
        use_kobj_processor_thread: bool = False,
        
        handlers: list[KnowledgeHandler] | None = None,
        
        cache: Cache | None = None,
        network: NetworkInterface | None = None,
        processor: ProcessorInterface | None = None
    ):
        self.config: ConfigType = config
        self.cache = cache or Cache(
            self.config.koi_net.cache_directory_path)
        
        self.identity = NodeIdentity(
            config=self.config,
            cache=self.cache)
        
        self.network = network or NetworkInterface(
            config=self.config,
            cache=self.cache, 
            identity=self.identity
        )
        
        # pull all handlers defined in default_handlers module
        if handlers is None:
            handlers = [
                obj for obj in vars(default_handlers).values() 
                if isinstance(obj, KnowledgeHandler)
            ]

        self.use_kobj_processor_thread = use_kobj_processor_thread
        self.processor = processor or ProcessorInterface(
            config=self.config,
            cache=self.cache, 
            network=self.network, 
            identity=self.identity, 
            use_kobj_processor_thread=self.use_kobj_processor_thread,
            default_handlers=handlers
        )
            
    def start(self) -> None:
        """Starts a node, call this method first.
        
        Starts the processor thread (if enabled). Loads event queues into memory. Generates network graph from nodes and edges in cache. Processes any state changes of node bundle. Initiates handshake with first contact (if provided) if node doesn't have any neighbors.
        """
        if self.use_kobj_processor_thread:
            logger.info("Starting processor worker thread")
            self.processor.worker_thread.start()
        
        self.network._load_event_queues()
        self.network.graph.generate()
        
        self.processor.handle(
            bundle=Bundle.generate(
                rid=self.identity.rid, 
                contents=self.identity.profile.model_dump()
            )
        )
        
        logger.debug("Waiting for kobj queue to empty")
        if self.use_kobj_processor_thread:
            self.processor.kobj_queue.join()
        else:
            self.processor.flush_kobj_queue()
        logger.debug("Done")
    
        if not self.network.graph.get_neighbors() and self.config.koi_net.first_contact:
            logger.debug(f"I don't have any neighbors, reaching out to first contact {self.config.koi_net.first_contact}")
            
            events = [
                Event.from_rid(EventType.FORGET, self.identity.rid),
                Event.from_bundle(EventType.NEW, self.identity.bundle)
            ]
            
            try:
                self.network.request_handler.broadcast_events(
                    url=self.config.koi_net.first_contact,
                    events=events
                )
                
            except httpx.ConnectError:
                logger.warning("Failed to reach first contact")
                return
            
                        
    def stop(self):
        """Stops a node, call this method last.
        
        Finishes processing knowledge object queue. Saves event queues to storage.
        """
        logger.info("Stopping node...")
        
        if self.use_kobj_processor_thread:
            logger.info(f"Waiting for kobj queue to empty ({self.processor.kobj_queue.unfinished_tasks} tasks remaining)")
            self.processor.kobj_queue.join()
        else:
            self.processor.flush_kobj_queue()
        
        self.network._save_event_queues()


================================================
FILE: src/koi_net/identity.py
================================================
import logging
from rid_lib.ext.bundle import Bundle
from rid_lib.ext.cache import Cache
from rid_lib.types.koi_net_node import KoiNetNode

from .config import Config
from .protocol.node import NodeProfile

logger = logging.getLogger(__name__)

    
class NodeIdentity:
    """Represents a node's identity (RID, profile, bundle)."""
    
    config: Config    
    cache: Cache
    
    def __init__(
        self,
        config: Config,
        cache: Cache
    ):
        """Initializes node identity from a name and profile.
        
        Attempts to read identity from storage. If it doesn't already exist, a new RID is generated from the provided name, and that RID and profile are written to storage. Changes to the name or profile will update the stored identity.
        
        WARNING: If the name is changed, the RID will be overwritten which will have consequences for the rest of the network.
        """
        self.config = config
        self.cache = cache
        
    @property
    def rid(self) -> KoiNetNode:
        return self.config.koi_net.node_rid
    
    @property
    def profile(self) -> NodeProfile:
        return self.config.koi_net.node_profile 
    
    @property
    def bundle(self) -> Bundle:
        return self.cache.read(self.rid)


================================================
FILE: src/koi_net/network/__init__.py
================================================
from .interface import NetworkInterface


================================================
FILE: src/koi_net/network/graph.py
================================================
import logging
from typing import Literal
import networkx as nx
from rid_lib import RIDType
from rid_lib.ext import Cache
from rid_lib.types import KoiNetEdge, KoiNetNode
from ..identity import NodeIdentity
from ..protocol.edge import EdgeProfile, EdgeStatus
from ..protocol.node import NodeProfile

logger = logging.getLogger(__name__)


class NetworkGraph:
    """Graph functions for this node's view of its network."""
    
    cache: Cache
    identity: NodeIdentity
    dg: nx.DiGraph
    
    def __init__(self, cache: Cache, identity: NodeIdentity):
        self.cache = cache
        self.dg = nx.DiGraph()
        self.identity = identity
        
    def generate(self):
        """Generates directed graph from cached KOI nodes and edges."""
        logger.debug("Generating network graph")
        self.dg.clear()
        for rid in self.cache.list_rids():
            if type(rid) == KoiNetNode:                
                self.dg.add_node(rid)
                logger.debug(f"Added node {rid}")
                
            elif type(rid) == KoiNetEdge:
                edge_profile = self.get_edge_profile(rid)
                if not edge_profile:
                    logger.warning(f"Failed to load {rid!r}")
                    continue
                self.dg.add_edge(edge_profile.source, edge_profile.target, rid=rid)
                logger.debug(f"Added edge {rid} ({edge_profile.source} -> {edge_profile.target})")
        logger.debug("Done")
        
    def get_node_profile(self, rid: KoiNetNode) -> NodeProfile | None:
        """Returns node profile given its RID."""
        bundle = self.cache.read(rid)
        if bundle:
            return bundle.validate_contents(NodeProfile)
        
    def get_edge_profile(
        self, 
        rid: KoiNetEdge | None = None,
        source: KoiNetNode | None = None, 
        target: KoiNetNode | None = None,
    ) -> EdgeProfile | None:
        """Returns edge profile given its RID, or source and target node RIDs."""
        if source and target:
            if (source, target) not in self.dg.edges: return
            edge_data = self.dg.get_edge_data(source, target)
            if not edge_data: return
            rid = edge_data.get("rid")
            if not rid: return
        elif not rid:
            raise ValueError("Either 'rid' or 'source' and 'target' must be provided")
        
        bundle = self.cache.read(rid)
        if bundle:
            return bundle.validate_contents(EdgeProfile)
        
    def get_edges(
        self,
        direction: Literal["in", "out"] | None = None,
    ) -> list[KoiNetEdge]:
        """Returns edges this node belongs to.
        
        All edges returned by default, specify `direction` to restrict to incoming or outgoing edges only."""
                
        edges = []
        if direction != "in" and self.dg.out_edges:
            out_edges = self.dg.out_edges(self.identity.rid)
            edges.extend([e for e in out_edges])
                
        if direction != "out" and self.dg.in_edges:
            in_edges = self.dg.in_edges(self.identity.rid)
            edges.extend([e for e in in_edges])
                    
        edge_rids = []
        for edge in edges:
            edge_data = self.dg.get_edge_data(*edge)
            if not edge_data: continue
            edge_rid = edge_data.get("rid")
            if not edge_rid: continue
            edge_rids.append(edge_rid)
       
        return edge_rids
    
    def get_neighbors(
        self,
        direction: Literal["in", "out"] | None = None,
        status: EdgeStatus | None = None,
        allowed_type: RIDType | None = None
    ) -> list[KoiNetNode]:
        """Returns neighboring nodes this node shares an edge with.
        
        All neighboring nodes returned by default, specify `direction` to restrict to neighbors connected by incoming or outgoing edges only."""
        
        neighbors = []
        for edge_rid in self.get_edges(direction):
            edge_profile = self.get_edge_profile(edge_rid)
            
            if not edge_profile: 
                logger.warning(f"Failed to find edge {edge_rid!r} in cache")
                continue
                        
            if status and edge_profile.status != status:
                continue
            
            if allowed_type and allowed_type not in edge_profile.rid_types:
                continue
            
            if edge_profile.target == self.identity.rid:
                neighbors.append(edge_profile.source)
            elif edge_profile.source == self.identity.rid:
                neighbors.append(edge_profile.target)
                
        return list(neighbors)
        



================================================
FILE: src/koi_net/network/interface.py
================================================
import logging
from queue import Queue
from typing import Generic
import httpx
from pydantic import BaseModel
from rid_lib import RID
from rid_lib.core import RIDType
from rid_lib.ext import Cache
from rid_lib.types import KoiNetNode

from .graph import NetworkGraph
from .request_handler import RequestHandler
from .response_handler import ResponseHandler
from ..protocol.node import NodeType
from ..protocol.edge import EdgeType
from ..protocol.event import Event
from ..identity import NodeIdentity
from ..config import Config, ConfigType

logger = logging.getLogger(__name__)


class EventQueueModel(BaseModel):
    webhook: dict[KoiNetNode, list[Event]]
    poll: dict[KoiNetNode, list[Event]]

type EventQueue = dict[RID, Queue[Event]]

class NetworkInterface(Generic[ConfigType]):
    """A collection of functions and classes to interact with the KOI network."""
    
    config: ConfigType    
    identity: NodeIdentity
    cache: Cache
    graph: NetworkGraph
    request_handler: RequestHandler
    response_handler: ResponseHandler
    poll_event_queue: EventQueue
    webhook_event_queue: EventQueue
    
    def __init__(
        self, 
        config: ConfigType,
        cache: Cache, 
        identity: NodeIdentity
    ):
        self.config = config
        self.identity = identity
        self.cache = cache
        self.graph = NetworkGraph(cache, identity)
        self.request_handler = RequestHandler(cache, self.graph)
        self.response_handler = ResponseHandler(cache)
        
        self.poll_event_queue = dict()
        self.webhook_event_queue = dict()
        self._load_event_queues()
    
    def _load_event_queues(self):
        """Loads event queues from storage."""
        try:
            with open(self.config.koi_net.event_queues_path, "r") as f:
                queues = EventQueueModel.model_validate_json(f.read())
            
            for node in queues.poll.keys():
                for event in queues.poll[node]:
                    queue = self.poll_event_queue.setdefault(node, Queue())
                    queue.put(event)
            
            for node in queues.webhook.keys():
                for event in queues.webhook[node]:
                    queue = self.webhook_event_queue.setdefault(node, Queue())
                    queue.put(event)
                                
        except FileNotFoundError:
            return
        
    def _save_event_queues(self):
        """Writes event queues to storage."""
        events_model = EventQueueModel(
            poll={
                node: list(queue.queue) 
                for node, queue in self.poll_event_queue.items()
                if not queue.empty()
            },
            webhook={
                node: list(queue.queue) 
                for node, queue in self.webhook_event_queue.items()
                if not queue.empty()
            }
        )
        
        if len(events_model.poll) == 0 and len(events_model.webhook) == 0:
            return
        
        with open(self.config.koi_net.event_queues_path, "w") as f:
            f.write(events_model.model_dump_json(indent=2))
    
    def push_event_to(self, event: Event, node: KoiNetNode, flush=False):
        """Pushes event to queue of specified node.
        
        Event will be sent to webhook or poll queue depending on the node type and edge type of the specified node. If `flush` is set to `True`, the webhook queued will be flushed after pushing the event.
        """
        logger.debug(f"Pushing event {event.event_type} {event.rid} to {node}")
            
        node_profile = self.graph.get_node_profile(node)
        if not node_profile:
            logger.warning(f"Node {node!r} unknown to me")
        
        # if there's an edge from me to the target node, override broadcast type
        edge_profile = self.graph.get_edge_profile(
            source=self.identity.rid,
            target=node
        )
        
        if edge_profile:
            if edge_profile.edge_type == EdgeType.WEBHOOK:
                event_queue = self.webhook_event_queue
            elif edge_profile.edge_type == EdgeType.POLL:
                event_queue = self.poll_event_queue
        else:
            if node_profile.node_type == NodeType.FULL:
                event_queue = self.webhook_event_queue
            elif node_profile.node_type == NodeType.PARTIAL:
                event_queue = self.poll_event_queue
        
        queue = event_queue.setdefault(node, Queue())
        queue.put(event)
                
        if flush and event_queue is self.webhook_event_queue:
            self.flush_webhook_queue(node)
            
    def _flush_queue(self, event_queue: EventQueue, node: KoiNetNode) -> list[Event]:
        """Flushes a node's queue, returning list of events."""
        queue = event_queue.get(node)
        events = list()
        if queue:
            while not queue.empty():
                event = queue.get()
                logger.debug(f"Dequeued {event.event_type} '{event.rid}'")
                events.append(event)
        
        return events
    
    def flush_poll_queue(self, node: KoiNetNode) -> list[Event]:
        """Flushes a node's poll queue, returning list of events."""
        logger.debug(f"Flushing poll queue for {node}")
        return self._flush_queue(self.poll_event_queue, node)
    
    def flush_webhook_queue(self, node: KoiNetNode):
        """Flushes a node's webhook queue, and broadcasts events.
        
        If node profile is unknown, or node type is not `FULL`, this operation will fail silently. If the remote node cannot be reached, all events will be requeued.
        """
        
        logger.debug(f"Flushing webhook queue for {node}")
        
        node_profile = self.graph.get_node_profile(node)
        
        if not node_profile:
            logger.warning(f"{node!r} not found")
            return
        
        if node_profile.node_type != NodeType.FULL:
            logger.warning(f"{node!r} is a partial node!")
            return
        
        events = self._flush_queue(self.webhook_event_queue, node)
        if not events: return
        
        logger.debug(f"Broadcasting {len(events)} events")
        
        try:  
            self.request_handler.broadcast_events(node, events=events)
            return True
        except httpx.ConnectError:
            logger.warning("Broadcast failed, dropping node")
            for event in events:
                self.push_event_to(event, node)
            return False
            
    def get_state_providers(self, rid_type: RIDType) -> list[KoiNetNode]:
        """Returns list of node RIDs which provide state for the specified RID type."""
        
        logger.debug(f"Looking for state providers of '{rid_type}'")
        provider_nodes = []
        for node_rid in self.cache.list_rids(rid_types=[KoiNetNode]):
            node = self.graph.get_node_profile(node_rid)
                        
            if node.node_type == NodeType.FULL and rid_type in node.provides.state:
                logger.debug(f"Found provider '{node_rid}'")
                provider_nodes.append(node_rid)
        
        if not provider_nodes:
            logger.debug("Failed to find providers")
        return provider_nodes
            
    def fetch_remote_bundle(self, rid: RID):
        """Attempts to fetch a bundle by RID from known peer nodes."""
        
        logger.debug(f"Fetching remote bundle '{rid}'")
        remote_bundle = None
        for node_rid in self.get_state_providers(type(rid)):
            payload = self.request_handler.fetch_bundles(
                node=node_rid, rids=[rid])
            
            if payload.bundles:
                remote_bundle = payload.bundles[0]
                logger.debug(f"Got bundle from '{node_rid}'")
                break
        
        if not remote_bundle:
            logger.warning("Failed to fetch remote bundle")
            
        return remote_bundle
    
    def fetch_remote_manifest(self, rid: RID):
        """Attempts to fetch a manifest by RID from known peer nodes."""
        
        logger.debug(f"Fetching remote manifest '{rid}'")
        remote_manifest = None
        for node_rid in self.get_state_providers(type(rid)):
            payload = self.request_handler.fetch_manifests(
                node=node_rid, rids=[rid])
            
            if payload.manifests:
                remote_manifest = payload.manifests[0]
                logger.debug(f"Got bundle from '{node_rid}'")
                break
        
        if not remote_manifest:
            logger.warning("Failed to fetch remote bundle")
            
        return remote_manifest
    
    def poll_neighbors(self) -> list[Event]:
        """Polls all neighboring nodes and returns compiled list of events.
        
        If this node has no neighbors, it will instead attempt to poll the provided first contact URL.
        """
        
        neighbors = self.graph.get_neighbors()
        
        if not neighbors and self.config.koi_net.first_contact:
            logger.debug("No neighbors found, polling first contact")
            try:
                payload = self.request_handler.poll_events(
                    url=self.config.koi_net.first_contact, 
                    rid=self.identity.rid
                )
                if payload.events:
                    logger.debug(f"Received {len(payload.events)} events from '{self.config.koi_net.first_contact}'")
                return payload.events
            except httpx.ConnectError:
                logger.debug(f"Failed to reach first contact '{self.config.koi_net.first_contact}'")
        
        events = []
        for node_rid in neighbors:
            node = self.graph.get_node_profile(node_rid)
            if not node: continue
            if node.node_type != NodeType.FULL: continue
            
            try:
                payload = self.request_handler.poll_events(
                    node=node_rid, 
                    rid=self.identity.rid
                )
                if payload.events:
                    logger.debug(f"Received {len(payload.events)} events from {node_rid!r}")
                events.extend(payload.events)
            except httpx.ConnectError:
                logger.debug(f"Failed to reach node '{node_rid}'")
                continue
            
        return events                
        
        


================================================
FILE: src/koi_net/network/request_handler.py
================================================
import logging
import httpx
from rid_lib import RID
from rid_lib.ext import Cache
from rid_lib.types.koi_net_node import KoiNetNode
from ..protocol.api_models import (
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
    EventsPayload,
    FetchRids,
    FetchManifests,
    FetchBundles,
    PollEvents,
    RequestModels,
    ResponseModels
)
from ..protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH
)
from ..protocol.node import NodeType
from .graph import NetworkGraph


logger = logging.getLogger(__name__)


class RequestHandler:
    """Handles making requests to other KOI nodes."""
    
    cache: Cache
    graph: NetworkGraph
    
    def __init__(self, cache: Cache, graph: NetworkGraph):
        self.cache = cache
        self.graph = graph
                
    def make_request(
        self, 
        url: str, 
        request: RequestModels,
        response_model: type[ResponseModels] | None = None
    ) -> ResponseModels | None:
        logger.debug(f"Making request to {url}")
        resp = httpx.post(
            url=url,
            data=request.model_dump_json()
        )
        if response_model:
            return response_model.model_validate_json(resp.text)
            
    def get_url(self, node_rid: KoiNetNode, url: str) -> str:
        """Retrieves URL of a node, or returns provided URL."""
        
        if not node_rid and not url:
            raise ValueError("One of 'node_rid' and 'url' must be provided")
        
        if node_rid:
            node_profile = self.graph.get_node_profile(node_rid)
            if not node_profile:
                raise Exception("Node not found")
            if node_profile.node_type != NodeType.FULL:
                raise Exception("Can't query partial node")
            logger.debug(f"Resolved {node_rid!r} to {node_profile.base_url}")
            return node_profile.base_url
        else:
            return url
    
    def broadcast_events(
        self, 
        node: RID = None, 
        url: str = None, 
        req: EventsPayload | None = None,
        **kwargs
    ) -> None:
        """See protocol.api_models.EventsPayload for available kwargs."""
        request = req or EventsPayload.model_validate(kwargs)
        self.make_request(
            self.get_url(node, url) + BROADCAST_EVENTS_PATH, request
        )
        logger.info(f"Broadcasted {len(request.events)} event(s) to {node or url!r}")
        
    def poll_events(
        self, 
        node: RID = None, 
        url: str = None, 
        req: PollEvents | None = None,
        **kwargs
    ) -> EventsPayload:
        """See protocol.api_models.PollEvents for available kwargs."""
        request = req or PollEvents.model_validate(kwargs)
        resp = self.make_request(
            self.get_url(node, url) + POLL_EVENTS_PATH, request,
            response_model=EventsPayload
        )
        logger.info(f"Polled {len(resp.events)} events from {node or url!r}")
        return resp
        
    def fetch_rids(
        self, 
        node: RID = None, 
        url: str = None, 
        req: FetchRids | None = None,
        **kwargs
    ) -> RidsPayload:
        """See protocol.api_models.FetchRids for available kwargs."""
        request = req or FetchRids.model_validate(kwargs)
        resp = self.make_request(
            self.get_url(node, url) + FETCH_RIDS_PATH, request,
            response_model=RidsPayload
        )
        logger.info(f"Fetched {len(resp.rids)} RID(s) from {node or url!r}")
        return resp
                
    def fetch_manifests(
        self, 
        node: RID = None, 
        url: str = None, 
        req: FetchManifests | None = None,
        **kwargs
    ) -> ManifestsPayload:
        """See protocol.api_models.FetchManifests for available kwargs."""
        request = req or FetchManifests.model_validate(kwargs)
        resp = self.make_request(
            self.get_url(node, url) + FETCH_MANIFESTS_PATH, request,
            response_model=ManifestsPayload
        )
        logger.info(f"Fetched {len(resp.manifests)} manifest(s) from {node or url!r}")
        return resp
                
    def fetch_bundles(
        self, 
        node: RID = None, 
        url: str = None, 
        req: FetchBundles | None = None,
        **kwargs
    ) -> BundlesPayload:
        """See protocol.api_models.FetchBundles for available kwargs."""
        request = req or FetchBundles.model_validate(kwargs)
        resp = self.make_request(
            self.get_url(node, url) + FETCH_BUNDLES_PATH, request,
            response_model=BundlesPayload
        )
        logger.info(f"Fetched {len(resp.bundles)} bundle(s) from {node or url!r}")
        return resp


================================================
FILE: src/koi_net/network/response_handler.py
================================================
import logging
from rid_lib import RID
from rid_lib.ext import Manifest, Cache
from rid_lib.ext.bundle import Bundle
from ..protocol.api_models import (
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
    FetchRids,
    FetchManifests,
    FetchBundles,
)

logger = logging.getLogger(__name__)


class ResponseHandler:
    """Handles generating responses to requests from other KOI nodes."""
    
    cache: Cache
    
    def __init__(self, cache: Cache):
        self.cache = cache
        
    def fetch_rids(self, req: FetchRids) -> RidsPayload:
        logger.info(f"Request to fetch rids, allowed types {req.rid_types}")
        rids = self.cache.list_rids(req.rid_types)
        
        return RidsPayload(rids=rids)
        
    def fetch_manifests(self, req: FetchManifests) -> ManifestsPayload:
        logger.info(f"Request to fetch manifests, allowed types {req.rid_types}, rids {req.rids}")
        
        manifests: list[Manifest] = []
        not_found: list[RID] = []
        
        for rid in (req.rids or self.cache.list_rids(req.rid_types)):
            bundle = self.cache.read(rid)
            if bundle:
                manifests.append(bundle.manifest)
            else:
                not_found.append(rid)
        
        return ManifestsPayload(manifests=manifests, not_found=not_found)
        
    def fetch_bundles(self, req: FetchBundles) -> BundlesPayload:
        logger.info(f"Request to fetch bundles, requested rids {req.rids}")
        
        bundles: list[Bundle] = []
        not_found: list[RID] = []

        for rid in req.rids:
            bundle = self.cache.read(rid)
            if bundle:
                bundles.append(bundle)
            else:
                not_found.append(rid)
            
        return BundlesPayload(bundles=bundles, not_found=not_found)


================================================
FILE: src/koi_net/processor/__init__.py
================================================
from .interface import ProcessorInterface


================================================
FILE: src/koi_net/processor/default_handlers.py
================================================
"""Provides implementations of default knowledge handlers."""

import logging
from rid_lib.ext.bundle import Bundle
from rid_lib.types import KoiNetNode, KoiNetEdge
from koi_net.protocol.node import NodeType
from .interface import ProcessorInterface
from .handler import KnowledgeHandler, HandlerType, STOP_CHAIN
from .knowledge_object import KnowledgeObject, KnowledgeSource
from ..protocol.event import Event, EventType
from ..protocol.edge import EdgeProfile, EdgeStatus, EdgeType

logger = logging.getLogger(__name__)

# RID handlers

@KnowledgeHandler.create(HandlerType.RID)
def basic_rid_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Default RID handler.
    
    Blocks external events about this node. Allows `FORGET` events if RID is known to this node.
    """
    if (kobj.rid == processor.identity.rid and 
        kobj.source == KnowledgeSource.External):
        logger.debug("Don't let anyone else tell me who I am!")
        return STOP_CHAIN
    
    if kobj.event_type == EventType.FORGET:
        kobj.normalized_event_type = EventType.FORGET
        return kobj

# Manifest handlers

@KnowledgeHandler.create(HandlerType.Manifest)
def basic_manifest_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Default manifest handler.
    
    Blocks manifests with the same hash, or aren't newer than the cached version. Sets the normalized event type to `NEW` or `UPDATE` depending on whether the RID was previously known to this node.
    """
    prev_bundle = processor.cache.read(kobj.rid)

    if prev_bundle:
        if kobj.manifest.sha256_hash == prev_bundle.manifest.sha256_hash:
            logger.debug("Hash of incoming manifest is same as existing knowledge, ignoring")
            return STOP_CHAIN
        if kobj.manifest.timestamp <= prev_bundle.manifest.timestamp:
            logger.debug("Timestamp of incoming manifest is the same or older than existing knowledge, ignoring")
            return STOP_CHAIN
        
        logger.debug("RID previously known to me, labeling as 'UPDATE'")
        kobj.normalized_event_type = EventType.UPDATE

    else:
        logger.debug("RID previously unknown to me, labeling as 'NEW'")
        kobj.normalized_event_type = EventType.NEW
        
    return kobj


# Bundle handlers

@KnowledgeHandler.create(
    handler_type=HandlerType.Bundle, 
    rid_types=[KoiNetEdge], 
    source=KnowledgeSource.External,
    event_types=[EventType.NEW, EventType.UPDATE])
def edge_negotiation_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles basic edge negotiation process.
    
    Automatically approves proposed edges if they request RID types this node can provide (or KOI nodes/edges). Validates the edge type is allowed for the node type (partial nodes cannot use webhooks). If edge is invalid, a `FORGET` event is sent to the other node.
    """
    
    edge_profile = EdgeProfile.model_validate(kobj.contents)

    # indicates peer subscribing to me
    if edge_profile.source == processor.identity.rid:     
        if edge_profile.status != EdgeStatus.PROPOSED:
            return
        
        logger.debug("Handling edge negotiation")
        
        peer_rid = edge_profile.target
        peer_profile = processor.network.graph.get_node_profile(peer_rid)
        
        if not peer_profile:
            logger.warning(f"Peer {peer_rid} unknown to me")
            return STOP_CHAIN
        
        # explicitly provided event RID types and (self) node + edge objects
        provided_events = (
            *processor.identity.profile.provides.event,
            KoiNetNode, KoiNetEdge
        )
        
        
        abort = False
        if (edge_profile.edge_type == EdgeType.WEBHOOK and 
            peer_profile.node_type == NodeType.PARTIAL):
            logger.debug("Partial nodes cannot use webhooks")
            abort = True
        
        if not set(edge_profile.rid_types).issubset(provided_events):
            logger.debug("Requested RID types not provided by this node")
            abort = True
        
        if abort:
            event = Event.from_rid(EventType.FORGET, kobj.rid)
            processor.network.push_event_to(event, peer_rid, flush=True)
            return STOP_CHAIN

        else:
            # approve edge profile
            logger.debug("Approving proposed edge")
            edge_profile.status = EdgeStatus.APPROVED
            updated_bundle = Bundle.generate(kobj.rid, edge_profile.model_dump())
      
            processor.handle(bundle=updated_bundle, event_type=EventType.UPDATE)
            return
              
    elif edge_profile.target == processor.identity.rid:
        if edge_profile.status == EdgeStatus.APPROVED:
            logger.debug("Edge approved by other node!")


# Network handlers

@KnowledgeHandler.create(HandlerType.Network)
def basic_network_output_filter(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Default network handler.
    
    Allows broadcasting of all RID types this node is an event provider for (set in node profile), and other nodes have subscribed to. All nodes will also broadcast about their own (internally sourced) KOI node, and KOI edges that they are part of, regardless of their node profile configuration. Finally, nodes will also broadcast about edges to the other node involved (regardless of if they are subscribed)."""
    
    involves_me = False
    if kobj.source == KnowledgeSource.Internal:
        if (type(kobj.rid) == KoiNetNode):
            if (kobj.rid == processor.identity.rid):
                involves_me = True
        
        elif type(kobj.rid) == KoiNetEdge:
            edge_profile = kobj.bundle.validate_contents(EdgeProfile)
            
            if edge_profile.source == processor.identity.rid:
                logger.debug(f"Adding edge target '{edge_profile.target!r}' to network targets")
                kobj.network_targets.update([edge_profile.target])
                involves_me = True
                
            elif edge_profile.target == processor.identity.rid:
                logger.debug(f"Adding edge source '{edge_profile.source!r}' to network targets")
                kobj.network_targets.update([edge_profile.source])
                involves_me = True
    
    if (type(kobj.rid) in processor.identity.profile.provides.event or involves_me):
        # broadcasts to subscribers if I'm an event provider of this RID type OR it involves me
        subscribers = processor.network.graph.get_neighbors(
            direction="out",
            allowed_type=type(kobj.rid)
        )
        
        logger.debug(f"Updating network targets with '{type(kobj.rid)}' subscribers: {subscribers}")
        kobj.network_targets.update(subscribers)
        
    return kobj
            


================================================
FILE: src/koi_net/processor/handler.py
================================================
from dataclasses import dataclass
from enum import StrEnum
from typing import Callable
from rid_lib import RIDType

from ..protocol.event import EventType
from .knowledge_object import KnowledgeSource, KnowledgeEventType


class StopChain:
    """Class for a sentinel value by knowledge handlers."""
    pass

STOP_CHAIN = StopChain()


class HandlerType(StrEnum):
    """Types of handlers used in knowledge processing pipeline.
    
    - RID - provided RID; if event type is `FORGET`, this handler decides whether to delete the knowledge from the cache by setting the normalized event type to `FORGET`, otherwise this handler decides whether to validate the manifest (and fetch it if not provided).
    - Manifest - provided RID, manifest; decides whether to validate the bundle (and fetch it if not provided).
    - Bundle - provided RID, manifest, contents (bundle); decides whether to write knowledge to the cache by setting the normalized event type to `NEW` or `UPDATE`.
    - Network - provided RID, manifest, contents (bundle); decides which nodes (if any) to broadcast an event about this knowledge to. (Note, if event type is `FORGET`, the manifest and contents will be retrieved from the local cache, and indicate the last state of the knowledge before it was deleted.)
    - Final - provided RID, manifests, contents (bundle); final action taken after network broadcast.
    """
    
    RID = "rid", 
    Manifest = "manifest", 
    Bundle = "bundle", 
    Network = "network", 
    Final = "final"

@dataclass
class KnowledgeHandler:
    """Handles knowledge processing events of the provided types."""
    
    func: Callable
    handler_type: HandlerType
    rid_types: list[RIDType] | None
    source: KnowledgeSource | None = None
    event_types: list[KnowledgeEventType] | None = None
    
    @classmethod
    def create(
        cls,
        handler_type: HandlerType,
        rid_types: list[RIDType] | None = None,
        source: KnowledgeSource | None = None,
        event_types: list[KnowledgeEventType] | None = None
    ):
        """Special decorator that returns a KnowledgeHandler instead of a function.
        
        The function symbol will redefined as a `KnowledgeHandler`, which can be passed into the `ProcessorInterface` constructor. This is used to register default handlers.
        """
        def decorator(func: Callable) -> KnowledgeHandler:
            handler = cls(func, handler_type, rid_types, source, event_types)
            return handler
        return decorator




================================================
FILE: src/koi_net/processor/interface.py
================================================
import logging
import queue
import threading
from typing import Callable, Generic
from rid_lib.core import RID, RIDType
from rid_lib.ext import Bundle, Cache, Manifest
from rid_lib.types.koi_net_edge import KoiNetEdge
from rid_lib.types.koi_net_node import KoiNetNode
from ..identity import NodeIdentity
from ..network import NetworkInterface
from ..protocol.event import Event, EventType
from ..config import Config
from .handler import (
    KnowledgeHandler, 
    HandlerType, 
    STOP_CHAIN,
    StopChain
)
from .knowledge_object import (
    KnowledgeObject,
    KnowledgeSource, 
    KnowledgeEventType
)

logger = logging.getLogger(__name__)


class ProcessorInterface():
    """Provides access to this node's knowledge processing pipeline."""
    
    config: Config
    cache: Cache
    network: NetworkInterface
    identity: NodeIdentity
    handlers: list[KnowledgeHandler]
    kobj_queue: queue.Queue[KnowledgeObject]
    use_kobj_processor_thread: bool
    worker_thread: threading.Thread | None = None
    
    def __init__(
        self,
        config: Config,
        cache: Cache, 
        network: NetworkInterface,
        identity: NodeIdentity,
        use_kobj_processor_thread: bool,
        default_handlers: list[KnowledgeHandler] = []
    ):
        self.config = config
        self.cache = cache
        self.network = network
        self.identity = identity
        self.use_kobj_processor_thread = use_kobj_processor_thread
        self.handlers: list[KnowledgeHandler] = default_handlers
        self.kobj_queue = queue.Queue()
        
        if self.use_kobj_processor_thread:
            self.worker_thread = threading.Thread(
                target=self.kobj_processor_worker,
                daemon=True
            )
        
    def add_handler(self, handler: KnowledgeHandler):
        self.handlers.append(handler)
            
    def register_handler(
        self,
        handler_type: HandlerType,
        rid_types: list[RIDType] | None = None,
        source: KnowledgeSource | None = None,
        event_types: list[KnowledgeEventType] | None = None
    ):
        """Assigns decorated function as handler for this processor."""
        def decorator(func: Callable) -> Callable:
            handler = KnowledgeHandler(func, handler_type, rid_types, source, event_types)
            self.add_handler(handler)
            return func
        return decorator
            
    def call_handler_chain(
        self, 
        handler_type: HandlerType,
        kobj: KnowledgeObject
    ) -> KnowledgeObject | StopChain:
        """Calls handlers of provided type, chaining their inputs and outputs together.
        
        The knowledge object provided when this function is called will be passed to the first handler. A handler may return one of three types: 
        - `KnowledgeObject` - to modify the knowledge object for the next handler in the chain
        - `None` - to keep the same knowledge object for the next handler in the chain
        - `STOP_CHAIN` - to stop the handler chain and immediately exit the processing pipeline
        
        Handlers will only be called in the chain if their handler and RID type match that of the inputted knowledge object. 
        """
        
        for handler in self.handlers:
            if handler_type != handler.handler_type: 
                continue
            
            if handler.rid_types and type(kobj.rid) not in handler.rid_types:
                continue
            
            if handler.source and handler.source != kobj.source:
                continue
            
            if handler.event_types and kobj.event_type not in handler.event_types:
                continue
            
            logger.debug(f"Calling {handler_type} handler '{handler.func.__name__}'")
            resp = handler.func(self, kobj.model_copy())
            
            # stops handler chain execution
            if resp is STOP_CHAIN:
                logger.debug(f"Handler chain stopped by {handler.func.__name__}")
                return STOP_CHAIN
            # kobj unmodified
            elif resp is None:
                continue
            # kobj modified by handler
            elif isinstance(resp, KnowledgeObject):
                kobj = resp
                logger.debug(f"Knowledge object modified by {handler.func.__name__}")
            else:
                raise ValueError(f"Handler {handler.func.__name__} returned invalid response '{resp}'")
                    
        return kobj

        
    def process_kobj(self, kobj: KnowledgeObject) -> None:
        """Sends provided knowledge obejct through knowledge processing pipeline.
        
        Handler chains are called in between major events in the pipeline, indicated by their handler type. Each handler type is guaranteed to have access to certain knowledge, and may affect a subsequent action in the pipeline. The five handler types are as follows:
        - RID - provided RID; if event type is `FORGET`, this handler decides whether to delete the knowledge from the cache by setting the normalized event type to `FORGET`, otherwise this handler decides whether to validate the manifest (and fetch it if not provided).
        - Manifest - provided RID, manifest; decides whether to validate the bundle (and fetch it if not provided).
        - Bundle - provided RID, manifest, contents (bundle); decides whether to write knowledge to the cache by setting the normalized event type to `NEW` or `UPDATE`.
        - Network - provided RID, manifest, contents (bundle); decides which nodes (if any) to broadcast an event about this knowledge to. (Note, if event type is `FORGET`, the manifest and contents will be retrieved from the local cache, and indicate the last state of the knowledge before it was deleted.)
        - Final - provided RID, manifests, contents (bundle); final action taken after network broadcast.
        
        The pipeline may be stopped by any point by a single handler returning the `STOP_CHAIN` sentinel. In that case, the process will exit immediately. Further handlers of that type and later handler chains will not be called.
        """
        
        logger.debug(f"Handling {kobj!r}")
        kobj = self.call_handler_chain(HandlerType.RID, kobj)
        if kobj is STOP_CHAIN: return
        
        if kobj.event_type == EventType.FORGET:
            bundle = self.cache.read(kobj.rid)
            if not bundle: 
                logger.debug("Local bundle not found")
                return
            
            # the bundle (to be deleted) attached to kobj for downstream analysis
            logger.debug("Adding local bundle (to be deleted) to knowledge object")
            kobj.manifest = bundle.manifest
            kobj.contents = bundle.contents
            
        else:
            # attempt to retrieve manifest
            if not kobj.manifest:
                logger.debug("Manifest not found")
                if kobj.source == KnowledgeSource.External:
                    logger.debug("Attempting to fetch remote manifest")
                    manifest = self.network.fetch_remote_manifest(kobj.rid)
                    
                elif kobj.source == KnowledgeSource.Internal:
                    logger.debug("Attempting to read manifest from cache")
                    bundle = self.cache.read(kobj.rid)
                    if bundle: 
                        manifest = bundle.manifest
                    else:
                        manifest = None
                        return
                    
                if not manifest:
                    logger.debug("Failed to find manifest")
                    return
                
                kobj.manifest = manifest
                
            kobj = self.call_handler_chain(HandlerType.Manifest, kobj)
            if kobj is STOP_CHAIN: return
            
            # attempt to retrieve bundle
            if not kobj.bundle:
                logger.debug("Bundle not found")
                if kobj.source == KnowledgeSource.External:
                    logger.debug("Attempting to fetch remote bundle")
                    bundle = self.network.fetch_remote_bundle(kobj.rid)
                    
                elif kobj.source == KnowledgeSource.Internal:
                    logger.debug("Attempting to read bundle from cache")
                    bundle = self.cache.read(kobj.rid)
                
                if not bundle: 
                    logger.debug("Failed to find bundle")
                    return
                
                if kobj.manifest != bundle.manifest:
                    logger.warning("Retrieved bundle contains a different manifest")
                
                kobj.manifest = bundle.manifest
                kobj.contents = bundle.contents                
                
        kobj = self.call_handler_chain(HandlerType.Bundle, kobj)
        if kobj is STOP_CHAIN: return
            
        if kobj.normalized_event_type in (EventType.UPDATE, EventType.NEW):
            logger.info(f"Writing to cache: {kobj!r}")
            self.cache.write(kobj.bundle)
            
        elif kobj.normalized_event_type == EventType.FORGET:
            logger.info(f"Deleting from cache: {kobj!r}")
            self.cache.delete(kobj.rid)
            
        else:
            logger.debug("Normalized event type was never set, no cache or network operations will occur")
            return
        
        if type(kobj.rid) in (KoiNetNode, KoiNetEdge):
            logger.debug("Change to node or edge, regenerating network graph")
            self.network.graph.generate()
        
        kobj = self.call_handler_chain(HandlerType.Network, kobj)
        if kobj is STOP_CHAIN: return
        
        if kobj.network_targets:
            logger.debug(f"Broadcasting event to {len(kobj.network_targets)} network target(s)")
        else:
            logger.debug("No network targets set")
        
        for node in kobj.network_targets:
            self.network.push_event_to(kobj.normalized_event, node)
            if not self.network.flush_webhook_queue(node):
                logger.warning("Dropping unresponsive node")
                self.handle(rid=node, event_type=EventType.FORGET)
        
        kobj = self.call_handler_chain(HandlerType.Final, kobj)

    def flush_kobj_queue(self):
        """Flushes all knowledge objects from queue and processes them.
        
        NOTE: ONLY CALL THIS METHOD IN SINGLE THREADED NODES, OTHERWISE THIS WILL CAUSE RACE CONDITIONS.
        """
        if self.use_kobj_processor_thread:
            logger.warning("You are using a worker thread, calling this method can cause race conditions!")
        
        while not self.kobj_queue.empty():
            kobj = self.kobj_queue.get()
            logger.debug(f"Dequeued {kobj!r}")
            
            try:
                self.process_kobj(kobj)
            finally:
                self.kobj_queue.task_done()
            logger.debug("Done")
    
    def kobj_processor_worker(self, timeout=0.1):
        while True:
            try:
                kobj = self.kobj_queue.get(timeout=timeout)
                logger.debug(f"Dequeued {kobj!r}")
                
                try:
                    self.process_kobj(kobj)
                finally:
                    self.kobj_queue.task_done()
                logger.debug("Done")
            
            except queue.Empty:
                pass
            
            except Exception as e:
                logger.warning(f"Error processing kobj: {e}")
        
    def handle(
        self,
        rid: RID | None = None,
        manifest: Manifest | None = None,
        bundle: Bundle | None = None,
        event: Event | None = None,
        kobj: KnowledgeObject | None = None,
        event_type: KnowledgeEventType = None,
        source: KnowledgeSource = KnowledgeSource.Internal
    ):
        """Queues provided knowledge to be handled by processing pipeline.
        
        Knowledge may take the form of an RID, manifest, bundle, event, or knowledge object (with an optional event type for RID, manifest, or bundle objects). All objects will be normalized into knowledge objects and queued. If `flush` is `True`, the queue will be flushed immediately after adding the new knowledge.
        """
        if rid:
            _kobj = KnowledgeObject.from_rid(rid, event_type, source)
        elif manifest:
            _kobj = KnowledgeObject.from_manifest(manifest, event_type, source)
        elif bundle:
            _kobj = KnowledgeObject.from_bundle(bundle, event_type, source)
        elif event:
            _kobj = KnowledgeObject.from_event(event, source)
        elif kobj:
            _kobj = kobj
        else:
            raise ValueError("One of 'rid', 'manifest', 'bundle', 'event', or 'kobj' must be provided")
        
        self.kobj_queue.put(_kobj)
        logger.debug(f"Queued {_kobj!r}")



================================================
FILE: src/koi_net/processor/knowledge_object.py
================================================
from enum import StrEnum
from pydantic import BaseModel
from rid_lib import RID
from rid_lib.ext import Manifest
from rid_lib.ext.bundle import Bundle
from rid_lib.types.koi_net_node import KoiNetNode
from ..protocol.event import Event, EventType


type KnowledgeEventType = EventType | None

class KnowledgeSource(StrEnum):
    Internal = "INTERNAL"
    External = "EXTERNAL"

class KnowledgeObject(BaseModel):
    """A normalized knowledge representation for internal processing.
    
    Capable of representing an RID, manifest, bundle, or event. Contains three additional fields use for decision making in the knowledge processing pipeline. 
    
    The source indicates whether this object was generated by this node, or sourced from another node in the network. 
    
    The normalized event type indicates how the knowledge object is viewed from the perspective of this node, and what cache actions will take place. `NEW`, `UPDATE` -> cache write, `FORGET` -> cache delete, `None` -> no cache action.
    
    The network targets indicate other nodes in the network this knowledge object will be sent to. The event sent to them will be constructed from this knowledge object's RID, manifest, contents, and normalized event type.
    
    Constructors are provided to create a knowledge object from an RID, manifest, bundle, or event.
    """
    rid: RID
    manifest: Manifest | None = None
    contents: dict | None = None
    event_type: KnowledgeEventType = None
    normalized_event_type: KnowledgeEventType = None
    source: KnowledgeSource
    network_targets: set[KoiNetNode] = set()
    
    def __repr__(self):
        return f"<KObj '{self.rid}' event type: '{self.event_type}' -> '{self.normalized_event_type}', source: '{self.source}'>"
    
    @classmethod
    def from_rid(
        cls, 
        rid: RID, 
        event_type: KnowledgeEventType = None, 
        source: KnowledgeSource = KnowledgeSource.Internal
    ) -> "KnowledgeObject":
        return cls(
            rid=rid,
            event_type=event_type,
            source=source
        )
        
    @classmethod
    def from_manifest(
        cls, 
        manifest: Manifest, 
        event_type: KnowledgeEventType = None, 
        source: KnowledgeSource = KnowledgeSource.Internal
    ) -> "KnowledgeObject":
        return cls(
            rid=manifest.rid,
            manifest=manifest,
            event_type=event_type,
            source=source
        )
        
    @classmethod
    def from_bundle(
        cls, 
        bundle: Bundle, 
        event_type: KnowledgeEventType = None, 
        source: KnowledgeSource = KnowledgeSource.Internal
    ) -> "KnowledgeObject":
        return cls(
            rid=bundle.rid,
            manifest=bundle.manifest,
            contents=bundle.contents,
            event_type=event_type,
            source=source
        )
        
    @classmethod
    def from_event(
        cls,
        event: Event,
        source: KnowledgeSource = KnowledgeSource.Internal
    ) -> "KnowledgeObject":
        return cls(
            rid=event.rid,
            manifest=event.manifest,
            contents=event.contents,
            event_type=event.event_type,
            source=source
        )
    
    @property
    def bundle(self):
        if self.manifest is None or self.contents is None:
            return
        
        return Bundle(
            manifest=self.manifest,
            contents=self.contents
        )
    
    @property
    def normalized_event(self):
        if self.normalized_event_type is None:
            raise ValueError("Internal event's normalized event type is None, cannot convert to Event")
        
        elif self.normalized_event_type == EventType.FORGET:
            return Event(
                rid=self.rid,
                event_type=EventType.FORGET
            )
        
        else:    
            return Event(
                rid=self.rid,
                event_type=self.normalized_event_type,
                manifest=self.manifest,
                contents=self.contents
            )


================================================
FILE: src/koi_net/protocol/__init__.py
================================================



================================================
FILE: src/koi_net/protocol/api_models.py
================================================
"""Pydantic models for request and response/payload objects in the KOI-net API."""

from pydantic import BaseModel
from rid_lib import RID, RIDType
from rid_lib.ext import Bundle, Manifest
from .event import Event


# REQUEST MODELS

class PollEvents(BaseModel):
    rid: RID
    limit: int = 0
    
class FetchRids(BaseModel):
    rid_types: list[RIDType] = []
    
class FetchManifests(BaseModel):
    rid_types: list[RIDType] = []
    rids: list[RID] = []
    
class FetchBundles(BaseModel):
    rids: list[RID]
    

# RESPONSE/PAYLOAD MODELS

class RidsPayload(BaseModel):
    rids: list[RID]

class ManifestsPayload(BaseModel):
    manifests: list[Manifest]
    not_found: list[RID] = []
    
class BundlesPayload(BaseModel):
    bundles: list[Bundle]
    not_found: list[RID] = []
    deferred: list[RID] = []
    
class EventsPayload(BaseModel):
    events: list[Event]
    

# TYPES

type RequestModels = EventsPayload | PollEvents | FetchRids | FetchManifests | FetchBundles
type ResponseModels = RidsPayload | ManifestsPayload | BundlesPayload | EventsPayload


================================================
FILE: src/koi_net/protocol/consts.py
================================================
"""API paths for KOI-net protocol."""

BROADCAST_EVENTS_PATH = "/events/broadcast"
POLL_EVENTS_PATH      = "/events/poll"
FETCH_RIDS_PATH       = "/rids/fetch"
FETCH_MANIFESTS_PATH  = "/manifests/fetch"
FETCH_BUNDLES_PATH    = "/bundles/fetch"


================================================
FILE: src/koi_net/protocol/edge.py
================================================
from enum import StrEnum
from pydantic import BaseModel
from rid_lib import RIDType
from rid_lib.types import KoiNetNode


class EdgeStatus(StrEnum):
    PROPOSED = "PROPOSED"
    APPROVED = "APPROVED"
    
class EdgeType(StrEnum):
    WEBHOOK = "WEBHOOK"
    POLL = "POLL"

class EdgeProfile(BaseModel):
    source: KoiNetNode
    target: KoiNetNode
    edge_type: EdgeType
    status: EdgeStatus
    rid_types: list[RIDType]



================================================
FILE: src/koi_net/protocol/event.py
================================================
from enum import StrEnum
from pydantic import BaseModel
from rid_lib import RID
from rid_lib.ext import Manifest, Bundle


class EventType(StrEnum):
    NEW = "NEW"
    UPDATE = "UPDATE"
    FORGET = "FORGET"

class Event(BaseModel):
    rid: RID
    event_type: EventType
    manifest: Manifest | None = None
    contents: dict | None = None
    
    def __repr__(self):
        return f"<Event '{self.rid}' event type: '{self.event_type}'>"
    
    @classmethod
    def from_bundle(cls, event_type: EventType, bundle: Bundle):
        return cls(
            rid=bundle.manifest.rid,
            event_type=event_type,
            manifest=bundle.manifest,
            contents=bundle.contents
        )
        
    @classmethod
    def from_manifest(cls, event_type: EventType, manifest: Manifest):
        return cls(
            rid=manifest.rid,
            event_type=event_type,
            manifest=manifest
        )
        
    @classmethod
    def from_rid(cls, event_type: EventType, rid: RID):
        return cls(
            rid=rid,
            event_type=event_type
        )
        
    @property
    def bundle(self):
        if self.manifest is not None and self.contents is not None:
            return Bundle(
                manifest=self.manifest,
                contents=self.contents
            )


================================================
FILE: src/koi_net/protocol/helpers.py
================================================
from rid_lib.core import RIDType
from rid_lib.ext.bundle import Bundle
from rid_lib.types import KoiNetEdge
from rid_lib.types.koi_net_node import KoiNetNode
from .edge import EdgeProfile, EdgeStatus, EdgeType

def generate_edge_bundle(
    source: KoiNetNode,
    target: KoiNetNode,
    rid_types: list[RIDType],
    edge_type: EdgeType
) -> Bundle:
    edge_rid = KoiNetEdge.generate(source, target)
    edge_profile = EdgeProfile(
        source=source,
        target=target,
        rid_types=rid_types,
        edge_type=edge_type,
        status=EdgeStatus.PROPOSED
    )
    edge_bundle = Bundle.generate(
        edge_rid,
        edge_profile.model_dump()
    )
    return edge_bundle


================================================
FILE: src/koi_net/protocol/node.py
================================================
from enum import StrEnum
from pydantic import BaseModel
from rid_lib import RIDType


class NodeType(StrEnum):
    FULL = "FULL"
    PARTIAL = "PARTIAL"

class NodeProvides(BaseModel):
    event: list[RIDType] = []
    state: list[RIDType] = []

class NodeProfile(BaseModel):
    base_url: str | None = None
    node_type: NodeType
    provides: NodeProvides = NodeProvides()


================================================
FILE: .github/workflows/publish-to-pypi.yml
================================================
name: Publish Python package to PyPI
on:
  push:
    tags:
      - 'v*.*.*'

jobs:
  build:
    name: Build distro
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
      with:
        persistent-credentials: false
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.x"
    - name: Install pypa/build
      run: python3 -m pip install build --user
    - name: Build a binary wheel and a source tarball
      run: python3 -m build
    - name: Store the distribution packages
      uses: actions/upload-artifact@v4
      with:
        name: python-package-distributions
        path: dist/
  
  publish-to-pypi:
    name: Publish Python distribution to PyPI
    if: startsWith(github.ref, 'refs/tags/')
    needs:
    - build
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/koi-net
    permissions:
      id-token: write

    steps:
    - name: Download all the dists
      uses: actions/download-artifact@v4
      with:
        name: python-package-distributions
        path: dist/
    - name: Publish distro to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1

  github-release:
    name: >-
      Sign the Python distribution with Sigstore
      and upload them to GitHub Release
    needs:
    - publish-to-pypi
    runs-on: ubuntu-latest

    permissions:
      contents: write
      id-token: write

    steps:
    - name: Download all the dists
      uses: actions/download-artifact@v4
      with:
        name: python-package-distributions
        path: dist/
    - name: Sign the dists with Sigstore
      uses: sigstore/gh-action-sigstore-python@v3.0.0
      with:
        inputs: >-
          ./dist/*.tar.gz
          ./dist/*.whl
    - name: Upload artifact signatures to GitHub Release
      env:
        GITHUB_TOKEN: ${{ github.token }}
      run: >-
        gh release upload
        "$GITHUB_REF_NAME" dist/**
        --repo "$GITHUB_REPOSITORY"



================================================
REPO: blockscience/rid-lib
================================================

</file_summary>

<directory_structure>
ext/
  __init__.py
  bundle.py
  cache.py
  effector.py
  manifest.py
  utils.py
types/
  __init__.py
  http_s.py
  koi_net_edge.py
  koi_net_node.py
  slack_channel.py
  slack_message.py
  slack_user.py
  slack_workspace.py
__init__.py
consts.py
core.py
README.md
utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="ext/__init__.py">
from .manifest import Manifest
from .bundle import Bundle
from .cache import Cache
from .effector import ActionType, Effector
</file>

<file path="ext/bundle.py">
from typing import TypeVar
from pydantic import BaseModel
from rid_lib.core import RID
from .manifest import Manifest


T = TypeVar("T", bound=BaseModel)

class Bundle(BaseModel):
    """A knowledge bundle composed of a manifest and contents associated with an RIDed object.

    Acts as a container for the data associated with an RID. It is written to and read from the cache.
    """
    manifest: Manifest
    contents: dict
    
    @classmethod
    def generate(cls, rid: RID, contents: dict) -> "Bundle":
        """Generates a bundle from provided RID and contents."""
        return cls(
            manifest=Manifest.generate(rid, contents),
            contents=contents
        )
    
    @property
    def rid(self):
        """This bundle's RID."""
        return self.manifest.rid
    
    def validate_contents(self, model: type[T]) -> T:
        """Attempts to validate contents against a Pydantic model."""
        return model.model_validate(self.contents)
</file>

<file path="ext/cache.py">
import os
import shutil
from rid_lib.core import RID, RIDType
from .bundle import Bundle
from .utils import b64_encode, b64_decode


class Cache:
    def __init__(self, directory_path: str):
        self.directory_path = directory_path
        
    def file_path_to(self, rid: RID) -> str:
        encoded_rid_str = b64_encode(str(rid))
        return f"{self.directory_path}/{encoded_rid_str}.json"

    def write(self, cache_bundle: Bundle) -> Bundle:
        """Writes bundle to cache, returns a Bundle."""
        if not os.path.exists(self.directory_path):
            os.makedirs(self.directory_path)
            
        with open(
            file=self.file_path_to(cache_bundle.manifest.rid), 
            mode="w", 
            encoding="utf-8"
        ) as f:
            f.write(cache_bundle.model_dump_json(indent=2))

        return cache_bundle
    
    def exists(self, rid: RID) -> bool:
        return os.path.exists(
            self.file_path_to(rid)
        )

    def read(self, rid: RID) -> Bundle | None:
        """Reads and returns CacheEntry from RID cache."""
        try:
            with open(
                file=self.file_path_to(rid), 
                mode="r",
                encoding="utf-8"
            ) as f:
                return Bundle.model_validate_json(f.read())
        except FileNotFoundError:
            return None
    
    def list_rids(self, rid_types: list[RIDType] | None = None) -> list[RID]:
        if not os.path.exists(self.directory_path):
            return []
        
        rids = []
        for filename in os.listdir(self.directory_path):
            encoded_rid_str = filename.split(".")[0]
            rid_str = b64_decode(encoded_rid_str)
            rid = RID.from_string(rid_str)
            
            if not rid_types or type(rid) in rid_types:
                rids.append(rid)
            
        return rids
                
    def delete(self, rid: RID) -> None:
        """Deletes cache bundle."""
        try:
            os.remove(self.file_path_to(rid))
        except FileNotFoundError:
            return

    def drop(self) -> None:
        """Deletes all cache bundles."""
        try:
            shutil.rmtree(self.directory_path)
        except FileNotFoundError:
            return
</file>

<file path="ext/effector.py">
from typing import Callable
from enum import StrEnum
from rid_lib import RID
from .bundle import Bundle
from .cache import Cache
from .manifest import Manifest


class ActionType(StrEnum):
    dereference = "dereference"
    
class ProxyHandler:
    def __init__(self, effector):
        self.effector = effector
        
    def __getattr__(self, action_type):
        # shortcut to execute actions, use action type as function name
        def execute(rid: RID, *args, **kwargs):
            return self.effector.execute(action_type, rid, *args, **kwargs)
        return execute

class Effector:
    def __init__(self, cache: Cache | None = None):
        self.cache = cache
        self._action_table = {}
        self.run = ProxyHandler(self)
        
    def register(
        self, 
        action_type: ActionType, 
        rid_type: type[RID] | str | tuple[type[RID] | str]
    ):
        def decorator(func: Callable[[RID], Bundle | dict | None]):
            # accept type or list of types to register
            if isinstance(rid_type, (list, tuple)):
                rid_types = rid_type
            else:
                rid_types = (rid_type,)
            
            # retrieve context from RID objects, or use str directly
            for _rid_type in rid_types:
                if isinstance(_rid_type, type) and issubclass(_rid_type, RID):
                    context = _rid_type.context
                else:
                    context = _rid_type         
            
                self._action_table[(action_type, context)] = func
            
            return func
        return decorator
    
    def execute(self, action_type: str, rid: RID, *args, **kwargs):
        action_pair = (action_type, rid.context)
        if action_pair in self._action_table:
            func = self._action_table[action_pair]
            return func(rid, *args, **kwargs)
        else:
            raise LookupError(f"Failed to execute, no action found for action pair '{action_pair}'")
        
    def register_dereference(self, rid_type: type[RID] | str | tuple[type[RID] | str]):
        return self.register(ActionType.dereference, rid_type)
        
    def deref(
        self, 
        rid: RID, 
        hit_cache=True, # tries to read cache first, writes to cache if there is a miss
        refresh=False   # refreshes cache even if there was a hit
    ) -> Bundle | None:
        if (
            self.cache is not None and 
            hit_cache is True and 
            refresh is False
        ):
            bundle = self.cache.read(rid)
            if (
                bundle is not None and 
                bundle.contents is not None
            ):
                return bundle
        
        raw_data = self.execute(ActionType.dereference, rid)
        
        if raw_data is None: 
            return
        elif isinstance(raw_data, Bundle):
            bundle = raw_data
        else:            
            manifest = Manifest.generate(rid, raw_data)
            bundle = Bundle(manifest, raw_data)
        
        if (
            self.cache is not None and 
            hit_cache is True
        ):
            self.cache.write(bundle)
        
        return bundle
</file>

<file path="ext/manifest.py">
from datetime import datetime, timezone
from pydantic import BaseModel
from rid_lib.core import RID
from .utils import sha256_hash_json


class Manifest(BaseModel):
    """A portable descriptor of a data object associated with an RID.
    
    Composed of an RID, timestamp, and sha256 hash of the data object.
    """
    rid: RID
    timestamp: datetime
    sha256_hash: str
    
    @classmethod
    def generate(cls, rid: RID, data: dict) -> "Manifest":
        """Generates a Manifest using the current time and hashing the provided data."""
        return cls(
            rid=rid,
            timestamp=datetime.now(timezone.utc),
            sha256_hash=sha256_hash_json(data)
        )
</file>

<file path="ext/utils.py">
import json
import hashlib
from base64 import urlsafe_b64encode, urlsafe_b64decode
from dataclasses import asdict, is_dataclass
from datetime import datetime
from pydantic import BaseModel
from rid_lib import RID


def sha256_hash_json(data: dict | BaseModel):
    if isinstance(data, BaseModel):
        data = json.loads(data.model_dump_json())
    json_str = json.dumps(data, separators=(',', ':'), sort_keys=True)
    json_bytes = json_str.encode()
    hash = hashlib.sha256()
    hash.update(json_bytes)
    return hash.hexdigest()

def b64_encode(string: str):
    return urlsafe_b64encode(
        string.encode()).decode().rstrip("=")

def b64_decode(string: str):
    return urlsafe_b64decode(
        (string + "=" * (-len(string) % 4)).encode()).decode()

def json_serialize(obj):
    if isinstance(obj, RID):
        return str(obj)
    elif is_dataclass(obj) and not isinstance(obj, type):
        return json_serialize(asdict(obj))
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, (list, tuple)):
        return [json_serialize(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: json_serialize(value) for key, value in obj.items()}
    else:
        return obj
</file>

<file path="types/__init__.py">
from .http_s import HTTP, HTTPS
from .slack_channel import SlackChannel
from .slack_message import SlackMessage
from .slack_workspace import SlackWorkspace
from .slack_user import SlackUser
from .koi_net_node import KoiNetNode
from .koi_net_edge import KoiNetEdge
</file>

<file path="types/http_s.py">
from urllib.parse import urlsplit, urlunsplit

from rid_lib.core import RID
        
class HTTP(RID):
    scheme = "http"
    
    def __init__(self, authority, path, query, fragment):
        self.authority = authority
        self.path = path
        self.query = query
        self.fragment = fragment
        
    @property
    def reference(self):
        return urlunsplit((
            "",
            self.authority,
            self.path,
            self.query,
            self.fragment
        ))
        
    @classmethod
    def from_reference(cls, reference):
        uri_components = urlsplit(reference, scheme=cls.scheme)
        # excluding scheme component
        return cls(*uri_components[1:])

class HTTPS(HTTP):
    scheme = "https"
</file>

<file path="types/koi_net_edge.py">
import json
import hashlib
from rid_lib.core import ORN
from .koi_net_node import KoiNetNode


class KoiNetEdge(ORN):
    namespace = "koi-net.edge"
    
    def __init__(self, id):
        self.id = id
        
    @classmethod
    def generate(cls, source: KoiNetNode, target: KoiNetNode):
        edge_json = {
            "source": str(source),
            "target": str(target)
        }
        json_bytes = json.dumps(edge_json).encode()
        hash = hashlib.sha256()
        hash.update(json_bytes)
        hash.hexdigest()
        
        return cls(hash.hexdigest())
        
    @property
    def reference(self):
        return self.id
    
    @classmethod
    def from_reference(cls, reference):
        return cls(reference)
</file>

<file path="types/koi_net_node.py">
from uuid import uuid4
from rid_lib.core import ORN


class KoiNetNode(ORN):
    namespace = "koi-net.node"
    
    def __init__(self, name, uuid):
        self.name = name
        self.uuid = uuid
        
    @classmethod
    def generate(cls, name):
        return cls(name, uuid4())
        
    @property
    def reference(self):
        return f"{self.name}+{self.uuid}"
    
    @classmethod
    def from_reference(cls, reference):
        components = reference.split("+")
        if len(components) == 2:
            return cls(*components)
        else:
            raise ValueError("KOI-net Node reference must contain two '+'-separated components: '<name>+<uuid>'")
</file>

<file path="types/slack_channel.py">
from rid_lib.core import ORN
from .slack_workspace import SlackWorkspace


class SlackChannel(ORN):
    namespace = "slack.channel"
    
    def __init__(
            self,
            team_id: str,
            channel_id: str,
        ):
        self.team_id = team_id
        self.channel_id = channel_id
            
    @property
    def reference(self):
        return f"{self.team_id}/{self.channel_id}"
    
    @property
    def workspace(self):
        return SlackWorkspace(
            self.team_id
        )
        
    @classmethod
    def from_reference(cls, reference):
        components = reference.split("/")
        if len(components) == 2:
            return cls(*components)
        else:
            raise ValueError("Slack Channel reference must contain two '/'-separated components: '<team_id>/<channel_id>'")
</file>

<file path="types/slack_message.py">
from rid_lib.core import ORN
from .slack_channel import SlackChannel
from .slack_workspace import SlackWorkspace


class SlackMessage(ORN):
    namespace = "slack.message"
    
    def __init__(
            self,
            team_id: str,
            channel_id: str,
            ts: str,
        ):
        self.team_id = team_id
        self.channel_id = channel_id
        self.ts = ts
            
    @property
    def reference(self):
        return f"{self.team_id}/{self.channel_id}/{self.ts}"
    
    @property
    def workspace(self):
        return SlackWorkspace(
            self.team_id
        )
    
    @property
    def channel(self):
        return SlackChannel(
            self.team_id,
            self.channel_id
        )
        
    @classmethod
    def from_reference(cls, reference):
        components = reference.split("/")
        if len(components) == 3:
            return cls(*components)
        else:
            raise ValueError("Slack Message reference must contain three '/'-separated components: '<team_id>/<channel_id>/<ts>'")
</file>

<file path="types/slack_user.py">
from rid_lib.core import ORN
from .slack_workspace import SlackWorkspace


class SlackUser(ORN):
    namespace = "slack.user"

    def __init__(
            self,
            team_id: str,
            user_id: str,
        ):
        self.team_id = team_id
        self.user_id = user_id
            
    @property
    def reference(self):
        return f"{self.team_id}/{self.user_id}"
    
    @property
    def workspace(self):
        return SlackWorkspace(
            self.team_id
        )
        
    @classmethod
    def from_reference(cls, reference):
        components = reference.split("/")
        if len(components) == 2:
            return cls(*components)
        else:
            raise ValueError("Slack User reference must contain two '/'-separated components: '<team_id>/<user_id>'")
</file>

<file path="types/slack_workspace.py">
from rid_lib.core import ORN


class SlackWorkspace(ORN):
    namespace = "slack.workspace"

    def __init__(
            self,
            team_id: str,
        ):
        self.team_id = team_id
            
    @property
    def reference(self):
        return self.team_id
        
    @classmethod
    def from_reference(cls, reference):
        return cls(reference)
</file>

<file path="__init__.py">
from .core import RID, RIDType
from . import types, ext
</file>

<file path="consts.py">
ORN_SCHEME = "orn"
URN_SCHEME = "urn"
NAMESPACE_SCHEMES = (ORN_SCHEME, URN_SCHEME)
ABSTRACT_TYPES = ("RID", "DefaultType", "ORN", "URN")
</file>

<file path="core.py">
from typing import Any
from abc import ABCMeta, abstractmethod
from pydantic_core import core_schema, CoreSchema
from pydantic import GetCoreSchemaHandler, GetJsonSchemaHandler
from pydantic.json_schema import JsonSchemaValue
from . import utils
from .consts import (
    ABSTRACT_TYPES, 
    NAMESPACE_SCHEMES, 
    ORN_SCHEME, 
    URN_SCHEME
)


class RIDType(ABCMeta):
    scheme: str | None = None
    namespace: str | None = None
    
    # maps RID type strings to their classes
    type_table: dict[str, type["RID"]] = dict() 
    
    def __new__(mcls, name, bases, dct):
        """Runs when RID derived classes are defined."""
            
        cls = super().__new__(mcls, name, bases, dct)
        
        # ignores built in RID types which aren't directly instantiated
        if name in ABSTRACT_TYPES:
            return cls
            
        if not getattr(cls, "scheme", None):
            raise TypeError(f"RID type '{name}' is missing 'scheme' definition")
        
        if not isinstance(cls.scheme, str):
            raise TypeError(f"RID type '{name}' 'scheme' must be of type 'str'")
        
        if cls.scheme in NAMESPACE_SCHEMES:
            if not getattr(cls, "namespace", None): 
                raise TypeError(f"RID type '{name}' is using namespace scheme but missing 'namespace' definition")
            if not isinstance(cls.namespace, str):
                raise TypeError(f"RID type '{name}' is using namespace scheme but 'namespace' is not of type 'str'")
                
        # check for abstract method implementation
        if getattr(cls, "__abstractmethods__", None):
            raise TypeError(f"RID type '{name}' is missing implemenation(s) for abstract method(s) {set(cls.__abstractmethods__)}")
        
        # save RID type to lookup table
        mcls.type_table[str(cls)] = cls
        return cls
    
    @classmethod
    def _new_default_type(mcls, scheme: str, namespace: str | None) -> type["RID"]:
        """Returns a new RID type deriving from DefaultType."""      
        if namespace:
            name = "".join([s.capitalize() for s in namespace.split(".")])
        else:
            name = scheme.capitalize()
        
        bases = (DefaultType,)
        
        if scheme in NAMESPACE_SCHEMES:
            if scheme == ORN_SCHEME:
                bases += (ORN,)
            elif scheme == URN_SCHEME:
                bases += (URN,)
        
        dct = dict(
            scheme=scheme, 
            namespace=namespace
        )
        
        return type(name, bases, dct)
    
    @classmethod
    def from_components(mcls, scheme: str, namespace: str | None = None) -> type["RID"]:
        context = utils.make_context_string(scheme, namespace)
        
        if context in mcls.type_table:
            return mcls.type_table[context]
        else:
            return mcls._new_default_type(scheme, namespace)
    
    @classmethod
    def from_string(mcls, string: str) -> type["RID"]:
        """Returns an RID type class from an RID context string."""
        
        scheme, namespace, _ = utils.parse_rid_string(string, context_only=True)
        return mcls.from_components(scheme, namespace)
     
    def __str__(cls) -> str:
        if cls.__name__ in ABSTRACT_TYPES: 
            return repr(cls)
        return utils.make_context_string(cls.scheme, cls.namespace)
    
    def __repr__(cls) -> str:
        if cls.__name__ in ABSTRACT_TYPES: 
            return type.__repr__(cls)
        return f"<RIDType '{str(cls)}'>"
    
    @classmethod
    def __get_pydantic_core_schema__(
        cls, source: type[Any], handler: GetCoreSchemaHandler
    ) -> CoreSchema:
                
        def not_abstract_type(rid_type: RIDType) -> RIDType:
            if rid_type.__name__ in ABSTRACT_TYPES:
                raise ValueError(f"RIDType must not be abstract type: {ABSTRACT_TYPES}")
            return rid_type
        
        # must be instance of RIDType not in ABSTRACT_TYPES
        from_instance_schema = core_schema.chain_schema([
            core_schema.is_instance_schema(RIDType),
            core_schema.no_info_plain_validator_function(not_abstract_type)
        ])
        
        # must be valid string, validated by RIDType.from_string (and prev schema)
        from_str_schema = core_schema.chain_schema([
            core_schema.str_schema(),
            core_schema.no_info_plain_validator_function(RIDType.from_string),
            from_instance_schema
        ])
        
        return core_schema.json_or_python_schema(
            json_schema=from_str_schema,
            python_schema=core_schema.union_schema([
                from_instance_schema,
                from_str_schema
            ]),
            serialization=core_schema.plain_serializer_function_ser_schema(str)
        )
        
    @classmethod
    def __get_pydantic_json_schema__(
        cls, _core_schema: CoreSchema, handler: GetJsonSchemaHandler
    ) -> JsonSchemaValue:
        json_schema = handler(core_schema.str_schema())
        json_schema.update({"format": "rid-type"})
        return json_schema
        
    # backwards compatibility
    @property
    def context(cls) -> str:
        return str(cls)
        

class RID(metaclass=RIDType):
    scheme: str | None = None
    namespace: str | None = None
    
    @property
    def context(self):
        return str(type(self))
    
    def __str__(self) -> str:
        return self.context + ":" + self.reference
    
    def __repr__(self) -> str:
        return f"<RID '{str(self)}'>"
    
    def __eq__(self, other) -> bool:
        if isinstance(other, self.__class__):
            return str(self) == str(other)
        else:
            return False
    
    def __hash__(self):
        return hash(str(self))
    
    @classmethod
    def __get_pydantic_core_schema__(
        cls, source: type[Any], handler: GetCoreSchemaHandler
    ) -> CoreSchema:
        
        # must be valid string, validated by RID.from_string, and an instance of the correct RID type
        from_str_schema = core_schema.chain_schema([
            core_schema.str_schema(),
            core_schema.no_info_plain_validator_function(RID.from_string),
            core_schema.is_instance_schema(cls)
        ])
        
        return core_schema.json_or_python_schema(
            json_schema=from_str_schema,
            python_schema=core_schema.union_schema([
                core_schema.is_instance_schema(cls),
                from_str_schema
            ]),
            serialization=core_schema.plain_serializer_function_ser_schema(str)
        )
    
    @classmethod
    def __get_pydantic_json_schema__(
        cls, _core_schema: CoreSchema, handler: GetJsonSchemaHandler
    ) -> JsonSchemaValue:
        json_schema = handler(core_schema.str_schema())
        json_schema.update({"format": "rid"})
        return json_schema
    
    @classmethod
    def from_string(cls, string: str) -> "RID":
        scheme, namespace, reference = utils.parse_rid_string(string)
        return RIDType.from_components(scheme, namespace).from_reference(reference)
    
    @abstractmethod
    def __init__(self, *args, **kwargs):
        ...
    
    @classmethod
    @abstractmethod
    def from_reference(cls, reference: str):
        ...
    
    @property
    @abstractmethod
    def reference(self) -> str:
        ...


class ORN(RID):
    scheme = ORN_SCHEME
    
class URN(RID):
    scheme = URN_SCHEME
    
class DefaultType(RID):
    def __init__(self, _reference):
        self._reference = _reference
        
    @classmethod
    def from_reference(cls, reference):
        return cls(reference)
    
    @property
    def reference(self):
        return self._reference
</file>

<file path="README.md">
# RID v3 Protocol

_This specification can be understood as the third iteration of the RID protocol, or RID v3. Previous versions include [RID v1](https://github.com/BlockScience/kms-identity/blob/main/README.md) and [RID v2](https://github.com/BlockScience/rid-lib/blob/v2/README.md)._

### Jump to Sections:

- [RID v3 Protocol](#rid-v3-protocol)
  - [Jump to Sections:](#jump-to-sections)
- [RID Core](#rid-core)
  - [Introduction](#introduction)
  - [Generic Syntax](#generic-syntax)
  - [Object Reference Names (previously RID v2)](#object-reference-names-previously-rid-v2)
  - [Examples](#examples)
  - [Implementation](#implementation)
    - [RID class](#rid-class)
    - [RID types](#rid-types)
    - [Creating your own types](#creating-your-own-types)
    - [Pydantic Compatibility](#pydantic-compatibility)
  - [Installation](#installation)
  - [Usage](#usage)
  - [Development](#development)
- [RID Extensions](#rid-extensions)
  - [Introduction](#introduction-1)
  - [Manifest](#manifest)
  - [Bundle](#bundle)
  - [Cache](#cache)
  - [Effector](#effector)

# RID Core

## Introduction

_Note: throughout this document the terms "resource", "digital object", and "knowledge object" are used roughly interchangeably._

Reference Identifiers (RIDs) identify references to resources primarily for usage within Knowledge Organization Infrastructure (KOI). The RID specification is informed by previous work on representing digital objects (see [Objects as Reference](https://blog.block.science/objects-as-reference-toward-robust-first-principles-of-digital-organization/)) in which objects are identified through a relationship between a reference and a referent. Under this model, RIDs are the _references_, and the resources they refer to are the _referents._ The _means of reference_ describes the relationship between the reference and referent.

```
(reference) -[means of reference]-> (referent)
```

As opposed to Uniform Resource Identifiers (URIs), RIDs are not intended to have universal agreement or a centralized management structure. However, RIDs are compatible with URIs in that _all URIs can be valid RIDs_. [RFC 3986](https://www.rfc-editor.org/info/rfc3986) outlines the basic properties of an URI, adding that "a URI can be further classified as a locator, a name or both." Location and naming can be considered two different means of reference, or methods of linking a reference and referent(s), where:

1. Locators identify resources by _where_ they are, with the referent being defined as the resource retrieved via a defined access method. This type of identifier is less stable, and the resource at the specified location could change or become unavailable over time.
2. Names identify resources by _what_ they are, acting as a more stable, location independent identifier. Resources identified by name are not always intended to be accessed, but some may be resolvable to locators. While the mapping from name to locator may not be constant the broader relationship between reference and referent should be.

## Generic Syntax

The generic syntax to compose an RID roughly mirrors URIS:

```
<context>:<reference>
```

Conceptually, the reference refers to the referent, while the context provides context for how to interpret the reference, or how to discriminate it from another otherwise identical RID. While in many cases the context simply maps to a URI scheme, the context may also include part of the "hierarchical part" (right hand side of a URI following the scheme).

## Object Reference Names (previously RID v2)

The major change from RID v2 to v3 was building compatibility with URIs, and as a result the previous RID v2 style identifiers are now implemented under the (unofficial) `orn:` URI scheme.

Object Reference Names (ORNs) identify references to objects, or resources identified independent of their access method. Given the previous definitions of identifiers, ORNs can be considered "names". They are intended to be used with existing resources which may already have well defined identifiers. An ORN identifies a resource by "dislocating" it from a specific access mechanism, maintaining a reference even if the underlying locator changes or breaks. ORNs are generally formed from one or more context specific identifiers which can be easily accessed for processing in other contexts.

ORNs are composed using the following syntax:

```
orn:<namespace>:<reference>
```

_Note: In previous versions, the namespace was split into `<space>.<form>`. Using a dot to separate a namespace in this way is still encouraged, but is not explicitly defined by this specification._

ORNs also implement a more complex context component: `orn:<namespace>`. The differences between the syntax of ORNs and generic URIs are summarized here:

```
<scheme>:<hierarchical-part>
\______/ \_________________/
    |                |
 context         reference
 ___|_________   ____|____
/             \ /         \
orn:<namespace>:<reference>
```

## Examples

In the current version there are two example implementations of RID types: HTTP/S URLs and Slack objects. The HTTP/S scheme is the most commonly used form of URI and uses the standard RID parsing, where the scheme `http` or `https` is equal to the context, and the hierarchical part is equal to the reference.

```
scheme  authority                  path
 _|_     ____|___  _________________|___________________
/   \   /        \/                                     \
https://github.com/BlockScience/rid-lib/blob/v3/README.md
\___/ \_________________________________________________/
  |                           |
context                   reference
```

The Slack objects are implemented as ORNs, and include workspaces, channels, messages, and users. The Slack message object's namespace is `slack.message` and its reference component is composed of three internal identifiers, the workspace id, channel id, and message id.

```
scheme namespace     team      channel      timestamp
 |   _____|_____   ___|___    ____|___   _______|_______
/ \ /           \ /       \ /         \ /               \
orn:slack.message:TA2E6KPK3/C07BKQX0EVC/1721669683.087619
\_______________/ \_____________________________________/
        |                            |
     context                     reference
```

By representing Slack messages through ORNs, a stable identifier can be assigned to a resource which can be mapped to existing locators for different use cases. For example, a Slack message can be represented as a shareable link which redirects to the Slack app or in browser app:

```
https://blockscienceteam.slack.com/archives/C07BKQX0EVC/p1721669683087619`
```

There's also a "deep link" which can open the Slack app directly (but only to a channel):

```
slack://open?team=TA2E6KPK3&id=C07BKQX0EVC
```

Finally, there's the backend API call to retrieve the JSON data associated with the message:

```
https://slack.com/api/conversations.replies?channel=C07BKQX0EVC&ts=1721669683.087619&limit=1
```

These three different locators have specific use cases, but none of them work well as long term identifiers of a Slack message. None of them contain all of the identifiers needed to uniquely identify the message (the shareable link comes close, but uses the mutable team name instead of the id). Even if a locator can fully describe an object of interested, it is not resilient to changes in access method and is not designed for portability into systems where the context needs to be clearly stated and internal identifiers easily extracted. Instead, we can represent a Slack message as an ORN and resolve it to any of the above locators when necessary.

## Implementation

### RID class

The RID class provides a template for all RID types and access to a global constructor. All RID instances have access to the following properties:

```python
class RID:
	scheme: str

	# defined for ORNs only
	namespace: str | None

	# "orn:<namespace>" for ORNs, otherwise equal to 'scheme'
	context: str

	# the component after namespace component for ORNs, otherwise after the scheme component
	reference: str

	@classmethod
	def from_string(cls, string: str) -> RID: ...

	# only callable from RID type classes, not the RID base class
	@classmethod
	def from_reference(cls, string: str) -> RID: ...
```

Example implementations can be found in [`src/rid_lib/types/`](https://github.com/BlockScience/rid-lib/tree/main/src/rid_lib/types).

### RID types

This library treats both RIDs and RID types as first class objects. Behind the scenes, the `RIDType` base class is the metaclass for all RID type classes (which are created by inheriting from the `RID`, `ORN`, `URN` classes) -- so RID types are the classes, and RIDs are the instances of those classes. You can access the type of an RID using the built-in type function: `type(rid)`. All RIDs with the same context are guaranteed to share the same RID type class. Even if that RID type doesn't have any explicit class implementation, a class will be automatically generated for it.

```python
class RIDType(ABCMeta):
    scheme: str | None = None
    namespace: str | None = None

    # maps RID type strings to their classes
    type_table: dict[str, type["RID"]] = dict()

    @classmethod
    def from_components(mcls, scheme: str, namespace: str | None = None) -> type["RID"]: ...

    @classmethod
    def from_string(mcls, string: str) -> type["RID"]: ...

    # backwards compatibility
    @property
    def context(cls) -> str:
        return str(cls)
```

The correct way to check the type of an RID is to check it's Python type. RID types can also be created using `RIDType.from_string`, which is also guaranteed to return the same class if the context component is the same.

```python
from rid_lib import RID, RIDType
from rid_lib.types import SlackMessage

slack_msg_rid = RID.from_string("orn:slack.message:TA2E6KPK3/C07BKQX0EVC/1721669683.087619")

assert type(slack_msg_rid) == SlackMessage
assert SlackMessage == RIDType.from_string("orn:slack.message")
```

### Creating your own types

In order to create an RID type, follow this minimal implementation:

```python
class MyRIDType(RID): # inherit from `RID` OR `ORN` base classes
	# define scheme for a generic URI type
	scheme = "scheme"
	# OR a namespace for a ORN type
	namespace = "namespace"

	# instantiates a new RID from internal components
	def __init__(self, internal_id):
		self.internal_id = internal_id

	# returns the reference component
	@property
	def reference(self):
		# should dynamically reflect changes to any internal ids
		return self.internal_id

	# instantiates of an RID of this type given a reference
	@classmethod
	def from_reference(cls, reference):
		# in a typical use case, the reference would need to be parsed

		# raise a ValueError if the reference is invalid
		if len(reference) > 10:
			raise ValueError("Internal ID must be less than 10 characters!")

		return cls(reference)
```

### Pydantic Compatibility

Both RIDs and RID types are Pydantic compatible fields, which means they can be used directly within a Pydantic model in very flexible ways:

```python
class Model(BaseModel):
	rid: RID
	slack_rid: SlackMessage | SlackUser | SlackChannel | SlackWorkspace
	rid_types: list[RIDType]
```

## Installation

This package can be installed with pip for use in other projects.

```
pip install rid-lib
```

It can also be built and installed from source by cloning this repo and running this command in the root directory.

```
pip install .
```

## Usage

RIDs are intended to be used as a lightweight, cross platform identifiers to facilitate communication between knowledge processing systems. RID objects can be constructed from any RID string using the general constructor `RID.from_string`. The parser will match the string's context component and call the corresponding `from_reference` constructor. This can also be done directly on any RID type class via `MyRIDType.from_reference`. Finally, each context class provides a default constructor which requires each subcomponent to be indvidiually specified.

```python
from rid_lib import RID
from rid_lib.types import SlackMessage

rid_obj1 = RID.from_string("orn:slack.message:TA2E6KPK3/C07BKQX0EVC/1721669683.087619")
rid_obj2 = SlackMessage.from_reference("TA2E6KPK3/C07BKQX0EVC/1721669683.087619")
rid_obj3 = SlackMessage(team_id="TA2E6KPK3", channel_id="C07BKQX0EVC", ts="1721669683.087619")

assert rid_obj1 == rid_obj2 == rid_obj3

# guaranteed to be defined for all RID objects
print(rid_obj1.scheme, rid_obj1.context, rid_obj1.reference)

# special parameters for the slack.message context
print(rid_obj1.team_id, rid_obj1.channel_id, rid_obj1.ts)
```

If an RID type hasn't been implemented as a class, it can still be parsed by the general constructor if provisional contexts are allowed (enabled by default). In this case a provisional context class is generated on the fly providing the minimal RID type implementation (`reference` property, `from_reference` class method, `__init__` function).

```python
test_obj1 = RID.from_string("test:one")
test_obj2 = RID.from_string("test:one")

assert test_obj1 == test_obj2
```

## Development

Build and install from source with development requirements:

```
pip install .[dev]
```

Run unit tests:

```
pytest --cov=rid_lib
```

To build and upload to PyPI:
(Remember to bump the version number in pyproject.toml first!)

```
python -m build
```

Two new build files should appear in `dist/`, a `.tar.gz` and `.whl` file.

```
python -m twine upload -r pypi dist/*
```

Enter the API key and upload the new package version.

# RID Extensions

## Introduction

In addition to the core implementation of the RID specification, this library also provides extended functionality through objects and patterns that interface with RIDs.

## Manifest

A manifest is a portable descriptor of a data object associated with an RID. It is composed of an RID and metadata about the data object it describes (currently a timestamp and sha256 hash). The name "manifest" comes from a shipping metaphor: a piece of cargo has contents (the stuff inside of it) and a manifest (a paper describing the contents and providing tracking info). In the KOI network ecosystem, a manifest serves a similar role. Manifests can be passed around to inform other nodes of a data objects they may be interested in.

Below are the accessible fields and methods of a Manifest object, all are required.

```python
class Manifest(BaseModel):
	rid: RID
	timestamp: datetime
	sha256_hash: str

	# generates a Manifest using the current datetime and the hash of the provided data
	@classmethod
	def generate(cls, rid: RID, data: dict) -> Manifest: ...
```

## Bundle

A bundle is composed of a manifest and contents. This is the "piece of cargo" in the shipping metaphor described above. It's the construct used to transfer and store the RIDed knowledge objects we are interested in.

```python
class Bundle(BaseModel):
    manifest: Manifest
    contents: dict

	@classmethod
    def generate(cls, rid: RID, contents: dict) -> Bundle: ...
```

_Manifests and bundles are implemented as Pydantic models, meaning they can be initialized with args or kwargs. They can also be serialized with `model_dump()` and `model_dump_json()`, and deserialized with `model_validate()` and `model_validate_json()`._

## Cache

The cache class allows us to set up a cache for reading and writing bundles to the local filesystem. Each bundle is stored as a separate JSON file in the cache directory, where the file name is base 64 encoding of its RID. Below are the accessible fields and methods of a Cache.

```python
class Cache:
    def __init__(self, directory_path: str): ...

    def file_path_to(self, rid: RID) -> str: ...

    def write(self, cache_bundle: Bundle) -> Bundle: ...

    def exists(self, rid: RID) -> bool: ...
    def read(self, rid: RID) -> Bundle | None: ...
    def list_rids(
		self, rid_types: list[RIDType] | None = None
	) -> list[RID]: ...

    def delete(self, rid: RID) -> None: ...
    def drop(self) -> None: ...
```

## Effector

_The effector has not been used or updated in awhile, it may be removed or refactored in the future._

The effector is the most abstract construct out of the rid-lib extensions. It acts as an "end effector", performing actions on/with RIDs. More concretely, it allows you to define and bind functions to a specific action type and RID context. The most obvious use case for this is as a dereferencer (and this use case has added functionality): a dereference function can be defined for different types of RIDs, and the effector will automatically choose the correct one to run based on the context of the RID passed in. Below are the accessible fields and methods of Effector.

```python
class Effector:
	cache: Cache | None

	# alias to 'execute', allows actions to be run by calling:
	# effector.run.<action_type>(rid: RID, *args, **kwargs)
	run: ProxyHandler

    def __init__(self, cache: Cache | None = None): ...

	# decorator used to register actions to the effector:
	# @effector.register(action_type, rid_type)
    def register(
        self,
        action_type: ActionType,
		# rid_type may be singular or multiple strings or RID type classes
        rid_type: Type[RID] | str | tuple[Type[RID] | str]
    ): ...

	# decorator used to register dereference actions to the effector
	# (alias to 'register', sets action_type=ActionType.dereference)
    def register_dereference(
		self,
		rid_type: Type[RID] | str | tuple[Type[RID] | str]
	): ...

	def execute(
		self,
		action_type: str,
		rid: RID,
		*args,
		**kwargs
	): ...

	# special handler for 'dereference' actions, returns a CacheBundle instead of dict, optionally interacts with cache
	# note: different behavior than calling 'dereference' action with 'execute' or 'run'
    def deref(
        self,
        rid: RID,
        hit_cache=True, # tries to read cache first, writes to cache if there is a miss
        refresh=False   # refreshes cache even if there was a hit
    ) -> CacheBundle | None: ...

```

Registering and calling actions with an Effector:

```python
import requests
from rid_lib import RID
from rid_lib.ext import Cache, Effector, ActionType
from rid_lib.types import HTTP, HTTPS

cache = Cache("my_cache")
effector = Effector(cache)

@effector.register_dereference((HTTP, HTTPS))
def dereference_url(url):
	return requests.get(str(url)).json()

my_url = RID.from_string("https://jsonplaceholder.typicode.com/todos/1")

# equivalent actions, returns dict
effector.execute(ActionType.dereference, url)
effector.execute("dereference", url)
effector.run.dereference(url)

# special dereference handler, returns CacheBundle
effector.deref(url)
effector.deref(url, hit_cache=False)
effector.deref(url, refresh=True)
```
</file>

<file path="utils.py">
from .consts import NAMESPACE_SCHEMES


def make_context_string(scheme: str, namespace: str | None):
    if scheme in NAMESPACE_SCHEMES:
        if namespace is None:
            raise TypeError("Cannot create context for namespace scheme '{scheme}' when namespace is None")
        return scheme + ":" + namespace
    else:
        if namespace is not None:
            raise TypeError("Cannot create context for non-namespace scheme '{scheme}' when namespace is not None")
        return scheme

def parse_rid_string(
    string: str, 
    context_only: bool = False
) -> tuple[str, str | None, str | None]:
    """Parses RID (or context) string into scheme, namespace, and reference components."""
    
    scheme = None
    namespace = None
    reference = None
    
    if not isinstance(string, str):
        raise TypeError(f"RID type string '{string}' must be of type 'str'")
    
    i = string.find(":")
    
    if i < 0:
        if not context_only:
            raise TypeError(f"RID string '{string}' should contain a ':'-separated context and reference componeont")
        
        scheme = string
        namespace = None
        
        if scheme in NAMESPACE_SCHEMES:
            raise TypeError(f"RID type string '{string}' is a namespace scheme but is missing a namespace component")
        
    else:        
        scheme = string[:i]
        if scheme in NAMESPACE_SCHEMES:
            j = string.find(":", i+1)
        
            if j < 0:
                if context_only:
                    namespace = string[i+1:]
                else:
                    raise TypeError(f"RID string '{string}' is missing a reference component")
            else:
                if context_only:
                    raise TypeError(f"RID type string '{string}' should contain a maximum of two ':'-separated components")
                else:
                    namespace = string[i+1:j]
                    reference = string[j+1:]
        else:
            if context_only:
                raise TypeError(f"RID type string '{string}' contains a ':'-separated namespace component, but scheme doesn't support namespaces")
            else:
                reference = string[i+1:]
    
    if scheme == "":
        raise TypeError(f"RID type string '{string}' cannot have an empty scheme")
    
    if namespace == "":
        raise TypeError(f"RID type string '{string}' cannot have an empty namespace")
    
    if reference == "":
        raise TypeError(f"RID string '{string}' cannot have an empty reference")
    
    return scheme, namespace, reference
</file>

</files>
</file>

<file path=".ai/code-2.txt">
Directory structure:
└── github/
    ├── README.md
    ├── __init__.py
    ├── github-node.service
    ├── pyproject.toml
    ├── repomix-output.xml
    ├── .env.example
    ├── .gitignore
    └── node/
        ├── __init__.py
        ├── __main__.py
        ├── backfill.py
        ├── config.py
        ├── core.py
        ├── loader.py
        ├── server.py
        ├── types.py
        ├── webhook.py
        └── handlers/
            └── github.py


Files Content:

================================================
FILE: services/github/README.md
================================================



================================================
FILE: services/github/__init__.py
================================================



================================================
FILE: services/github/github-node.service
================================================
[Unit]
Description=KOI-net Github Node Service
After=network.target

[Service]
WorkingDirectory=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/services/github
ExecStart=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/.venv/bin/python3 -m node
Restart=always

[Install]
WantedBy=multi-user.target



================================================
FILE: services/github/pyproject.toml
================================================
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "github"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.3",
    "koi-net",
    "PyGithub",
    "aiohttp",
    "rich" # Added for logging
]

[tool.setuptools]
package-dir = {"" = "."}


================================================
FILE: services/github/repomix-output.xml
================================================
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
node/
  handlers/
    github.py
  __init__.py
  __main__.py
  backfill.py
  config.py
  core.py
  loader.py
  server.py
  types.py
  webhook.py
.gitignore
github-node.service
pyproject.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="node/handlers/github.py">
import logging
from rid_lib.ext import Bundle
# Assuming GithubCommit RID type is accessible
from ..core import node # Import the initialized node instance
from ..types import GithubCommit # Ensure this import is correct

# Imports needed for coordinator_contact handler from refactor.md
from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode # Assuming this RID type is accessible

logger = logging.getLogger(__name__)

@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of the coordinator node (or other nodes providing KoiNetNode events).
    
    On discovering a NEW coordinator, proposes a WEBHOOK edge for bidirectional
    communication and requests a list of other known nodes (sync).
    (Based on refactor.md example)
    """
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return
        
    # Validate that the discovered node actually provides network events
    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if KoiNetNode not in profile.provides.event:
            logger.debug(f"Node {kobj.rid} does not provide KoiNetNode events. Ignoring.")
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    logger.info(f"Identified potential coordinator/peer: {kobj.rid}; proposing WEBHOOK edge")
    try:
        # Propose edge FROM coordinator TO self
        edge_bundle = generate_edge_bundle(
            source=kobj.rid, 
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode] # Specify the type of information this edge carries
        )
        processor.handle(bundle=edge_bundle)
    except Exception as e:
        logger.error(f"Failed to generate or handle WEBHOOK edge bundle for {kobj.rid}: {e}", exc_info=True)

    # Sync network nodes from the discovered node
    logger.info(f"Syncing network nodes from {kobj.rid}")
    try:
        payload = processor.network.request_handler.fetch_rids(kobj.rid, rid_types=[KoiNetNode])
        if not payload or not payload.rids:
             logger.warning(f"Received empty RIDs payload from {kobj.rid} during sync.")
             return
             
        logger.debug(f"Received {len(payload.rids)} RIDs from {kobj.rid}")
        for rid in payload.rids:
            # Don't process self or already known nodes
            if rid == processor.identity.rid or processor.cache.exists(rid):
                continue
            logger.debug(f"Handling discovered RID from sync: {rid}")
            # Handle the RID to fetch its details (profile, etc.)
            processor.handle(rid=rid, source=KnowledgeSource.External)
    except Exception as e:
        logger.error(f"Failed during network sync with {kobj.rid}: {e}", exc_info=True)

@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit(processor: ProcessorInterface, kobj: KnowledgeObject):
    """
    Basic handler for processing GithubCommit bundles.
    Currently just logs information.
    
    Args:
        bundle: The Bundle object containing the GithubCommit RID and contents.
    """
    try:
        # kobj.bundle is guaranteed to exist in Bundle handler phase
        bundle = kobj.bundle
        rid: GithubCommit = bundle.rid
        contents: dict = bundle.contents
        
        logger.info(f"Processing commit: {rid} (Normalized Type: {kobj.normalized_event_type}, Source: {kobj.source})")
        # Log some details from the contents for visibility
        logger.debug(f"  Author: {contents.get('author_name')} <{contents.get('author_email')}>")
        logger.debug(f"  Message: {contents.get('message', '').splitlines()[0][:80]}...") # Log first line up to 80 chars
        logger.debug(f"  URL: {contents.get('html_url')}")

        # Default Bundle handler will write to cache based on normalized_event_type.
        # If you needed to modify contents *before* caching, you would return
        # the modified kobj here: return kobj

        # If you wanted to stop processing this specific KObj (e.g., based on content),
        # you could return STOP_CHAIN here: return STOP_CHAIN

        # Returning None passes the kobj unchanged to the next handler in the chain
        # (likely the default handler which performs the cache write)
        return None

    except Exception as e:
        logger.error(f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True)
        # Decide if error should stop the pipeline for this KObj
        # return STOP_CHAIN
        return None # Continue processing despite handler error

# Add a NEW handler specifically to propose a GithubCommit edge to the coordinator
@node.processor.register_handler(HandlerType.Final, rid_types=[KoiNetNode])
def propose_github_commit_edge_to_coordinator(processor: ProcessorInterface, kobj: KnowledgeObject):
    # This handler runs after a NEW KoiNetNode is fully processed (cached, graph updated, default edges proposed)
    # It's in the Final phase, less intrusive to core pipeline flow.
    # Simplified logic: If we just processed a NEW KoiNetNode and it's the *only* other node in our graph besides ourselves,
    # assume it's the coordinator and propose the GithubCommit edge.
    if kobj.normalized_event_type != EventType.NEW:
        return
        
    known_peers = processor.network.graph.get_neighbors()
    if len(known_peers) == 1 and known_peers[0] == kobj.rid:
        logger.info(f"Assuming {kobj.rid} is the coordinator. Proposing WEBHOOK edge for GithubCommit events.")
        try:
            github_edge_bundle = generate_edge_bundle(
                source=node.identity.rid, # I am the source of these events
                target=kobj.rid,           # The coordinator is the target
                edge_type=EdgeType.WEBHOOK, # Use webhook
                rid_types=[GithubCommit]    # Specify GithubCommit events
            )
            # Queue this new edge bundle for processing
            processor.handle(bundle=github_edge_bundle)
        except Exception as e:
            logger.error(f"Failed to generate/handle GithubCommit edge bundle for {kobj.rid}: {e}", exc_info=True)

logger.info("GithubCommit Bundle handler and KoiNetNode handlers registered.")
</file>

<file path="node/__init__.py">
import logging
from rich.logging import RichHandler

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[RichHandler()]
)
</file>

<file path="node/__main__.py">
import uvicorn
from .config import PORT


uvicorn.run("services.github.node.server:app", port=PORT, log_config=None)
</file>

<file path="node/backfill.py">
import logging
from github import Github, GithubException, RateLimitExceededException
from github.Commit import Commit
from rid_lib.ext import Bundle
# Assuming GithubCommit RID type is accessible
from .types import GithubCommit 
from .core import node
from .config import (
    GITHUB_TOKEN, MONITORED_REPOS,
    LAST_PROCESSED_SHA, update_state_file 
)

logger = logging.getLogger(__name__)

# Initialize GitHub client (authenticated if token provided)
github_client = Github(GITHUB_TOKEN) if GITHUB_TOKEN else Github()
logger.info(f"GitHub client initialized. Authenticated: {bool(GITHUB_TOKEN)}")

def perform_backfill():
    """
    One-time startup backfill: fetch all commits since LAST_PROCESSED_SHA
    for each monitored repo, bundle them as NEW, and persist the latest SHA processed.
    Processes commits oldest-to-newest after fetching.
    """
    logger.info("Starting GitHub backfill process...")
    
    # Load the state dictionary once before the loop
    current_state = LAST_PROCESSED_SHA.copy() # Use a copy to avoid modifying during iteration if needed
    newest_sha_processed_overall_map = {} # Track newest SHA per repo processed in this run

    if not MONITORED_REPOS:
        logger.warning("No repositories configured in MONITORED_REPOS. Backfill skipped.")
        return

    for repo_full_name in MONITORED_REPOS:
        try:
            owner, repo_name_only = repo_full_name.split('/')
            # Get the last processed SHA for *this specific repository*
            last_sha_for_repo = current_state.get(repo_full_name)
            logger.info(f"Backfilling repository: {repo_full_name} since SHA: {last_sha_for_repo or 'beginning'}")

            gh_repo = github_client.get_repo(repo_full_name)
            commits_to_process_buffer: list[Commit] = []
            
            # Iterate commits newest-first until we find the last processed one
            paginated_commits = gh_repo.get_commits()
            logger.debug(f"Fetching commits for {repo_full_name}...")
            commit_count = 0
            for commit in paginated_commits:
                commit_count += 1
                # Check against the specific SHA for this repo
                if last_sha_for_repo and commit.sha == last_sha_for_repo:
                    logger.info(f"Found last processed SHA {last_sha_for_repo} in {repo_full_name}. Stopping fetch for this repo.")
                    break
                commits_to_process_buffer.append(commit)
                # Safety break for potentially huge repos without a known SHA
                # Adjust limit as needed
                if commit_count % 100 == 0:
                    logger.debug(f"Fetched {commit_count} commits for {repo_full_name} so far...")
                # if commit_count > 1000: 
                #    logger.warning(f"Reached fetch limit (1000) for {repo_full_name}. Consider adjusting.")
                #    break 
            
            logger.info(f"Found {len(commits_to_process_buffer)} new commits in {repo_full_name} to backfill.")

            # Process commits oldest → newest
            newest_sha_processed_in_repo = None
            for commit in reversed(commits_to_process_buffer):
                try:
                    rid = GithubCommit(owner=owner, repo=repo_name_only, sha=commit.sha)
                    # Extract commit details carefully, handling potential missing attributes
                    author = commit.commit.author
                    committer = commit.commit.committer
                    
                    contents = {
                        "sha": commit.sha,
                        "message": commit.commit.message,
                        "author_name": author.name if author else None,
                        "author_email": author.email if author else None,
                        "author_date": author.date.isoformat() if author and author.date else None,
                        "committer_name": committer.name if committer else None,
                        "committer_email": committer.email if committer else None,
                        "committer_date": committer.date.isoformat() if committer and committer.date else None,
                        "html_url": commit.html_url,
                        "parents": [p.sha for p in commit.parents] # List of parent SHAs
                    }
                    bundle = Bundle.generate(rid=rid, contents=contents)
                    logger.debug(f"Bundling backfill commit {rid}")
                    node.processor.handle(bundle=bundle) 
                    
                    # Track the newest SHA processed in this run for this repo
                    newest_sha_processed_in_repo = commit.sha 
                    
                except Exception as e:
                    logger.error(f"Error processing commit {commit.sha} in {repo_full_name}: {e}", exc_info=True)

            # Store the newest SHA processed for this repo during this run
            if newest_sha_processed_in_repo:
                newest_sha_processed_overall_map[repo_full_name] = newest_sha_processed_in_repo
                logger.debug(f"Newest SHA processed for {repo_full_name} in this run: {newest_sha_processed_in_repo}")

        except RateLimitExceededException:
            logger.error(f"GitHub API rate limit exceeded while backfilling {repo_full_name}. Aborting backfill. Try again later or use a GITHUB_TOKEN.")
            # Depending on requirements, could wait and retry, but for now, we stop.
            return # Stop the entire backfill process
        except GithubException as e:
            logger.error(f"GitHub API error for repository {repo_full_name}: {e}. Skipping this repo.")
            continue # Skip to the next repository
        except Exception as e:
            logger.error(f"Unexpected error backfilling repository {repo_full_name}: {e}", exc_info=True)
            continue # Skip to the next repository

    # After processing all repos, persist the newest SHAs found (if changed)
    updated_count = 0
    for repo_full_name, newest_sha in newest_sha_processed_overall_map.items():
        if newest_sha != current_state.get(repo_full_name):
            update_state_file(repo_full_name, newest_sha) # Call function from config.py
            updated_count += 1
        else:
            logger.debug(f"No state update needed for {repo_full_name}, newest SHA {newest_sha} is same as stored.")
            
    if updated_count > 0:
        logger.info(f"Backfill complete. Updated state for {updated_count} repositories.")
    else:
        logger.info(f"Backfill complete. No new commits found or state changes required across monitored repositories.")

if __name__ == "__main__":
    # Example of how to run backfill directly for testing
    # Requires node to be started if handle() depends on active components
    # In practice, this is called by server.py during startup
    logging.basicConfig(level=logging.INFO)
    logger.info("Running backfill directly for testing...")
    # node.start() # Might be needed depending on node.processor.handle implementation
    perform_backfill()
    # node.stop()
</file>

<file path="node/config.py">
import logging
import json
from pydantic_settings import BaseSettings
from pydantic import Field
from typing import Optional, List

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Settings(BaseSettings):
    # --- KOI-net Configuration ---
    URL: str = Field(..., description="Public URL where this Github node can be reached by other KOI-net nodes.")
    FIRST_CONTACT: str = Field(..., description="URL of the coordinator node or another known KOI-net node.")

    # --- Github Sensor Server Configuration ---
    HOST: str = Field(default="127.0.0.1", description="Host the FastAPI server should listen on.")
    PORT: int = Field(default=8001, description="Port the FastAPI server should listen on.")

    # --- Github Configuration ---
    GITHUB_TOKEN: Optional[str] = Field(default=None, description="Optional Github Personal Access Token.")
    # Load MONITORED_REPOS from env var as a string
    MONITORED_REPOS_STR: str = Field(alias="MONITORED_REPOS", default="", description="Comma-separated list of repositories (owner/repo).")
    GITHUB_WEBHOOK_SECRET: str = Field(..., description="Secret used to verify webhook signatures.")

    # --- State File ---
    STATE_FILE_PATH: str = Field(default="state.json", description="Path to the JSON file storing the last processed commit SHA.")

    # --- Logging Configuration ---
    LOG_LEVEL: str = Field(default="INFO", description="Logging level (e.g., DEBUG, INFO, WARNING).")

    @property
    def MONITORED_REPOS(self) -> List[str]:
        """Parses the comma-separated MONITORED_REPOS_STR into a list of 'owner/repo' strings."""
        if not self.MONITORED_REPOS_STR:
            return []
        # Split by comma, strip whitespace, and filter out empty strings
        repos = [repo.strip() for repo in self.MONITORED_REPOS_STR.split(',') if repo.strip()]
        logger.debug(f"Parsed monitored repos: {repos}")
        return repos

    class Config:
        env_file = 'services/github/.env'
        env_file_encoding = 'utf-8'
        extra = 'ignore'

# Instantiate settings early to catch config errors on import
try:
    settings = Settings()
    # Apply log level from settings
    logging.getLogger().setLevel(settings.LOG_LEVEL.upper())
    logger.info("Configuration loaded successfully.")
    logger.info(f"Settings: {settings.model_dump(exclude={'GITHUB_TOKEN', 'GITHUB_WEBHOOK_SECRET'})}") # Exclude token and secret from debug logs
except Exception as e:
    logger.exception(f"Failed to load configuration from .env file: {e}")
    # Optionally re-raise or exit if config is critical
    raise

# --- Exported Variables ---
# Make settings easily accessible throughout the application
URL = settings.URL
FIRST_CONTACT = settings.FIRST_CONTACT
HOST = settings.HOST
PORT = settings.PORT
GITHUB_TOKEN = settings.GITHUB_TOKEN
GITHUB_WEBHOOK_SECRET = settings.GITHUB_WEBHOOK_SECRET
MONITORED_REPOS = settings.MONITORED_REPOS # Access the parsed list via the property
STATE_FILE_PATH = settings.STATE_FILE_PATH
LOG_LEVEL = settings.LOG_LEVEL

# --- State Management (Loading initial state & update function) ---
LAST_PROCESSED_SHA = {} # Dictionary mapping repo_name -> last_sha

def load_state():
    """Loads the last processed SHA state from the JSON file."""
    global LAST_PROCESSED_SHA
    try:
        with open(STATE_FILE_PATH, 'r') as f:
            LAST_PROCESSED_SHA = json.load(f)
        logger.info(f"Loaded state from '{STATE_FILE_PATH}': {list(LAST_PROCESSED_SHA.keys())}")
    except FileNotFoundError:
        logger.warning(f"State file '{STATE_FILE_PATH}' not found. Starting with empty state.")
        LAST_PROCESSED_SHA = {}
    except json.JSONDecodeError:
        logger.error(f"Error decoding JSON from state file '{STATE_FILE_PATH}'. Starting with empty state.")
        LAST_PROCESSED_SHA = {}
    except Exception as e:
        logger.error(f"Unexpected error loading state file '{STATE_FILE_PATH}': {e}", exc_info=True)
        LAST_PROCESSED_SHA = {}

def update_state_file(repo_name: str, last_sha: str):
    """Updates the state file with the latest processed SHA for a repo."""
    global LAST_PROCESSED_SHA
    LAST_PROCESSED_SHA[repo_name] = last_sha
    try:
        # Create directory if it doesn't exist (though it should usually be just the filename)
        # import os
        # os.makedirs(os.path.dirname(STATE_FILE_PATH), exist_ok=True) # Uncomment if path can include dirs
        with open(STATE_FILE_PATH, 'w') as f:
            json.dump(LAST_PROCESSED_SHA, f, indent=4)
        logger.debug(f"Updated state file '{STATE_FILE_PATH}' for {repo_name} with SHA: {last_sha}")
    except IOError as e:
        logger.error(f"Failed to write state file '{STATE_FILE_PATH}': {e}")
    except Exception as e:
        logger.error(f"Unexpected error writing state file '{STATE_FILE_PATH}': {e}", exc_info=True)

# Load initial state when config module is imported
load_state()
</file>

<file path="node/core.py">
import os
import shutil
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

from .config import HOST, PORT, FIRST_CONTACT
from .types import GithubCommit

logger = logging.getLogger(__name__)

name = "sensor"

identity_dir = f".koi/{name}"
cache_dir = f".koi/{name}/rid_cache_{name}"
# Remove existing directories if they exist
shutil.rmtree(identity_dir, ignore_errors=True)
shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the directories
os.makedirs(identity_dir, exist_ok=True)
os.makedirs(cache_dir, exist_ok=True)

# Initialize the KOI-net Node Interface for the GitHub Sensor
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        base_url=f"http://{HOST}:{PORT}/koi-net",
        node_type=NodeType.FULL, # Assuming it provides data and potentially processes
        provides=NodeProvides(
            event=[GithubCommit], # Provides GithubCommit events
            state=[GithubCommit]  # Can serve state for GithubCommit RIDs
        )
    ),
    use_kobj_processor_thread=True, # Use a background thread for processing
    first_contact=FIRST_CONTACT,   # Coordinator node to connect to initially
    identity_file_path=f"{identity_dir}/{name}_identity.json", # Use the variable
    event_queues_file_path=f"{identity_dir}/{name}_event_queues.json", # Use the variable
    cache_directory_path=cache_dir # Use the variable
)

logger.info(f"Initialized NodeInterface: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {FIRST_CONTACT}")
</file>

<file path="node/loader.py">
from .core import node
from .handlers import github

def register_handlers():
    print("Registering GITHUB handlers...")
    _ = github
</file>

<file path="node/server.py">
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, Request, Body, Header, HTTPException
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH
)
from .core import node
from .webhook import router as github_router
from .backfill import perform_backfill
from .loader import register_handlers

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup, backfill, and shutdown."""
    logger.info("Starting FastAPI application lifespan...")
    # Start the KOI-net node
    try:
        register_handlers()
        node.start()
        logger.info("KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        # Depending on requirements, you might want to prevent the app from starting
        raise RuntimeError("Failed to initialize KOI-net node") from e

    # Run initial backfill in the background
    logger.info("Scheduling initial GitHub backfill...")
    # Run backfill in a separate thread/task to avoid blocking startup
    # Note: perform_backfill itself is synchronous, so asyncio.to_thread is suitable.
    # If perform_backfill becomes async, just use asyncio.create_task directly.
    backfill_task = asyncio.to_thread(perform_backfill)
    # If you need to wait for backfill completion before yielding, await here.
    # For now, let it run in the background.

    try:
        yield # Application runs here
    finally:
        # Cleanup: Cancel pending tasks, stop the node
        logger.info("Shutting down FastAPI application...")
        # Attempt to gracefully cancel the backfill if it's still running
        # This might require more sophisticated task management if backfill is long-running
        # if backfill_task and not backfill_task.done():
        #     try:
        #         backfill_task.cancel()
        #         await backfill_task
        #     except asyncio.CancelledError:
        #         logger.info("Backfill task cancelled.")
        #     except Exception as e:
        #         logger.error(f"Error cancelling backfill task: {e}", exc_info=True)
        
        try:
            node.stop()
            logger.info("KOI-net node stopped successfully.")
        except Exception as e:
            logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
        logger.info("FastAPI application shutdown complete.")

# Create FastAPI app instance
app = FastAPI(
    title="KOI-net GitHub Sensor Node",
    description="Listens for GitHub webhooks and performs backfill to ingest commit data.",
    version="0.1.0",
    lifespan=lifespan # Register the lifespan context manager
)

# Define KOI-net API router
koi_net_router = APIRouter(prefix="/koi-net")

@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    logger.info(f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)")
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    return {} # Broadcast endpoint typically returns empty success

@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)

@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    # The default response_handler reads from cache, fulfilling the provides=[GithubCommit] state
    return node.network.response_handler.fetch_rids(req)

@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}")
    manifests_payload = node.network.response_handler.fetch_manifests(req)
    # Add any custom logic here if you need to fetch manifests not in cache
    return manifests_payload # The default handler already includes not_found

@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    bundles_payload = node.network.response_handler.fetch_bundles(req)
    # Add any custom logic here if you need to fetch bundles not in cache
    return bundles_payload # The default handler already includes not_found and deferred

# Include routers
app.include_router(koi_net_router) # KOI-net API endpoints
app.include_router(github_router) # GitHub webhook endpoint

logger.info("FastAPI application configured with webhook and KOI-net routers.")

# Note: This file defines the 'app'. It will be run by uvicorn via __main__.py
</file>

<file path="node/types.py">
from rid_lib.core import ORN

class GithubCommit(ORN):
    """
    Resource Identifier (RID) for a specific GitHub commit.
    
    Format: orn:github.commit:<owner>/<repo>/<sha>
    Example: orn:github.commit:microsoft/vscode/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
    """
    namespace = "github.commit"

    def __init__(self, owner: str, repo: str, sha: str):
        """
        Initialize a GitHub commit RID.
        
        Args:
            owner: The repository owner (user or organization)
            repo: The repository name
            sha: The commit SHA (full 40-character or shortened)
        """
        if not owner or not repo or not sha:
            raise ValueError("Owner, repo, and SHA cannot be empty")
        
        if "/" in owner or "/" in repo:
            raise ValueError("Owner and repo cannot contain '/' character")
            
        self.owner = owner
        self.repo = repo
        self.sha = sha

    @property
    def reference(self) -> str:
        """Returns the reference part of the RID: '<owner>/<repo>/<sha>'."""
        return f"{self.owner}/{self.repo}/{self.sha}"
    
    @property
    def repository_full_name(self) -> str:
        """Returns the full repository name: '<owner>/<repo>'."""
        return f"{self.owner}/{self.repo}"
    
    @property
    def html_url(self) -> str:
        """Returns the HTML URL to view this commit on GitHub."""
        return f"https://github.com/{self.owner}/{self.repo}/commit/{self.sha}"
    
    @property
    def api_url(self) -> str:
        """Returns the GitHub API URL for this commit."""
        return f"https://api.github.com/repos/{self.owner}/{self.repo}/commits/{self.sha}"

    @classmethod
    def from_reference(cls, reference: str) -> "GithubCommit":
        """
        Creates a GithubCommit instance from its reference string.
        
        Args:
            reference: String in format '<owner>/<repo>/<sha>'
            
        Returns:
            GithubCommit instance
            
        Raises:
            ValueError: If the reference format is invalid
        """
        try:
            parts = reference.split("/", maxsplit=2)
            if len(parts) != 3:
                raise ValueError("Reference must contain exactly two '/' separators")
                
            owner, repo, sha = parts
            
            if not owner or not repo or not sha:
                raise ValueError("Owner, repo, and SHA parts cannot be empty")
                
            # Basic SHA length check
            if len(sha) < 7:  # Minimum length for a short SHA
                raise ValueError(f"SHA part seems too short: {sha}")
                
            return cls(owner=owner, repo=repo, sha=sha)
            
        except ValueError as e:
            raise ValueError(f"Invalid reference format for GithubCommit. Expected '<owner>/<repo>/<sha>', got '{reference}'. Error: {e}") from e
        except Exception as e:
            raise TypeError(f"Unexpected error parsing GithubCommit reference '{reference}': {e}") from e
</file>

<file path="node/webhook.py">
import logging
import hmac
import hashlib
from fastapi import APIRouter, Request, Header, HTTPException, Body
from rid_lib.ext import Bundle
# Assuming GithubCommit RID type is accessible
from .types import GithubCommit
from .core import node
from .config import (
    GITHUB_WEBHOOK_SECRET, MONITORED_REPOS,
    update_state_file, LAST_PROCESSED_SHA 
)

logger = logging.getLogger(__name__)

router = APIRouter()

async def verify_signature(request: Request, x_hub_signature_256: str = Header(None)):
    """Verify the GitHub webhook signature."""
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning("Webhook verification skipped: GITHUB_WEBHOOK_SECRET not set.")
        return # Skip verification if secret is not configured

    # This check is removed because FastAPI ensures the header is present
    # if x_hub_signature_256 is None:
    #     logger.error("Webhook verification failed: Missing X-Hub-Signature-256 header")
    #     raise HTTPException(status_code=400, detail="Missing X-Hub-Signature-256 header")

    body = await request.body()
    hash_object = hmac.new(GITHUB_WEBHOOK_SECRET.encode('utf-8'), msg=body, digestmod=hashlib.sha256)
    expected_signature = "sha256=" + hash_object.hexdigest()

    if not hmac.compare_digest(expected_signature, x_hub_signature_256):
        logger.error(f"Webhook verification failed: Invalid signature. Expected: {expected_signature}, Got: {x_hub_signature_256}")
        raise HTTPException(status_code=403, detail="Invalid signature")

    logger.debug("Webhook signature verified successfully.")


@router.post("/github/webhook", status_code=202) # Use 202 Accepted as we process async
async def github_webhook(
    request: Request,
    x_github_event: str = Header(...), # Required header
    x_hub_signature_256: str = Header(...), # Required for verification
    payload: dict = Body(...)
):
    """Handle incoming GitHub webhook events (specifically 'push')."""
    logger.info(f"Received GitHub webhook event: {x_github_event}")

    try: # Main try block for the entire function
        # --- Signature Verification --- 
        await verify_signature(request, x_hub_signature_256)

        # --- Event Handling --- 
        if x_github_event == "ping":
            logger.info("Received 'ping' event from GitHub. Responding OK.")
            return {"message": "Pong!"}

        if x_github_event != "push":
            logger.debug(f"Ignoring non-'push' event: {x_github_event}")
            return {"message": f"Ignoring event type: {x_github_event}"} 

        # --- Process 'push' Event --- 
        repo_info = payload.get("repository", {})
        repo_full_name = repo_info.get("full_name")
        repo_owner = repo_info.get("owner", {}).get("login") or repo_info.get("owner", {}).get("name")
        repo_name = repo_info.get("name")
        commits = payload.get("commits", [])
        head_commit = payload.get("head_commit", {})

        if not repo_full_name or not repo_owner or not repo_name:
            logger.error(f"Webhook payload missing repository details: {payload.get('repository')}")
            raise HTTPException(status_code=400, detail="Missing repository information in payload")
            
        # Check if the repository is monitored
        if repo_full_name not in MONITORED_REPOS:
            logger.debug(f"Ignoring push event for non-monitored repository: {repo_full_name}")
            return {"message": f"Repository {repo_full_name} not monitored"}

        if not commits and not head_commit:
             logger.warning(f"'push' event for {repo_full_name} received without 'commits' or 'head_commit' data. Possibly a branch deletion or tag push? Payload head: {payload.get('ref', '')}")
             return {"message": "No commit data found in push event"}

        # Determine the commit(s) to process and the SHA to potentially update state with
        commits_to_process = []
        sha_to_update_state = None
        
        head_commit_id = head_commit.get('id')
        if head_commit_id:
            commits_to_process = [head_commit] # Use head_commit as the primary source
            sha_to_update_state = head_commit_id # This is the SHA representing the push tip
            logger.debug(f"Processing head_commit: {head_commit_id}")
        elif commits:
            commits_to_process = commits # Fallback to commits list
            # If using commits list, the last commit's SHA is the best candidate for state update
            if commits:
                sha_to_update_state = commits[-1].get('id') 
            logger.debug(f"Processing commits list (count: {len(commits)}). Potential state update SHA: {sha_to_update_state}")
        else:
             # This case should ideally not be reached due to the check above, but included for completeness
             logger.warning(f"No processable commit data found in push event for {repo_full_name}. Skipping.")
             return {"message": "No processable commit data"}

        processed_new_commit = False
        for commit in commits_to_process:
            commit_sha = commit.get("id")
            if not commit_sha:
                logger.warning("Skipping commit in payload with missing 'id'.")
                continue
            
            # Avoid reprocessing the last known SHA for this repo
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if last_known_sha_for_repo and commit_sha == last_known_sha_for_repo:
                logger.debug(f"Skipping commit {commit_sha} for {repo_full_name} as it matches last known SHA {last_known_sha_for_repo}.")
                continue

            # Inner try-except for processing individual commits within the push
            try:
                # Construct RID
                rid = GithubCommit(owner=repo_owner, repo=repo_name, sha=commit_sha)
                
                # Extract details - ensure keys exist
                author = commit.get("author", {})
                committer = commit.get("committer", {})
                
                contents = {
                    "sha": commit_sha,
                    "message": commit.get("message"),
                    "author_name": author.get("name"),
                    "author_email": author.get("email"),
                    "author_date": commit.get("timestamp"), # GitHub often uses 'timestamp'
                    "committer_name": committer.get("name"),
                    "committer_email": committer.get("email"),
                    "committer_date": committer.get("timestamp"),
                    "html_url": commit.get("url"), # Use 'url' from webhook payload
                    "parents": commit.get("parents", []) # Typically a list of SHAs in webhook
                }
                
                bundle = Bundle.generate(rid=rid, contents=contents)
                logger.debug(f"Bundling webhook commit {rid}")
                node.processor.handle(bundle=bundle)
                
                processed_new_commit = True # Mark that we processed at least one new commit
            
            except Exception as e:
                 logger.error(f"Error processing webhook commit {commit_sha} for {repo_full_name}: {e}", exc_info=True)
                 # Decide whether to continue processing other commits in the push or stop
                 continue # Continue with next commit in the webhook push
        
        # Update state file only if we processed a new commit and have a valid SHA representing the push tip
        if processed_new_commit and sha_to_update_state:
            # Check again if the sha_to_update is different from the stored one before writing
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if sha_to_update_state != last_known_sha_for_repo:
                logger.info(f"Webhook processing complete for {repo_full_name}. Updating state to SHA: {sha_to_update_state}")
                update_state_file(repo_full_name, sha_to_update_state) 
            else:
                 logger.info(f"Webhook processing complete for {repo_full_name}. State SHA {sha_to_update_state} already stored.")
        elif processed_new_commit:
             logger.warning(f"Webhook processing complete for {repo_full_name}. Processed new commit(s) but could not determine SHA for state update.")
        else:
             logger.info(f"Webhook processing complete for {repo_full_name}. No new commits processed or state updated.")

        return {"message": "Webhook processed successfully"}

    # Exception handlers are now correctly indented relative to the main 'try' block
    except HTTPException as he:
        # Re-raise HTTP exceptions to return proper status codes
        logger.warning(f"HTTP Exception during webhook processing: {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"Unexpected error handling webhook event {x_github_event}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal server error handling webhook")
</file>

<file path=".gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
</file>

<file path="github-node.service">
[Unit]
Description=KOI-net Github Node Service
After=network.target

[Service]
WorkingDirectory=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/services/github
ExecStart=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/.venv/bin/python3 -m node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "github"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.3",
    "koi-net",
    "PyGithub",
    "aiohttp",
    "rich" # Added for logging
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

</files>



================================================
FILE: services/github/.env.example
================================================

# KOI-net GitHub Node Configuration

# Public URL where this GitHub node can be reached by other KOI-net nodes
URL="http://127.0.0.1:8001"

# URL of the coordinator node or another known KOI-net node
FIRST_CONTACT="http://127.0.0.1:8000/koi-net"

# Host the FastAPI server should listen on (0.0.0.0 allows external connections)
HOST="0.0.0.0"

# Port the FastAPI server should listen on
PORT="8001"

# GitHub Personal Access Token (replace with your own token)
GITHUB_TOKEN="your_github_token_here"

# Secret used to verify webhook signatures (generate a secure random string)
GITHUB_WEBHOOK_SECRET="your_webhook_secret_here"

# Comma-separated list of repositories to monitor (format: owner/repo)
MONITORED_REPOS="owner/repo1,owner/repo2"

# Path to the JSON file storing the last processed commit SHA
STATE_FILE_PATH="state.json"

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL="INFO"


================================================
FILE: services/github/.gitignore
================================================
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store


================================================
FILE: services/github/node/__init__.py
================================================
import logging
from rich.logging import RichHandler

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[RichHandler()]
)




================================================
FILE: services/github/node/__main__.py
================================================
import uvicorn
import logging
from .config import HOST, PORT

logger = logging.getLogger(__name__)

logger.info(f"GitHub sensor node starting on {HOST}:{PORT}")
uvicorn.run("services.github.node.server:app", host=HOST, port=PORT, log_config=None, reload=True)


================================================
FILE: services/github/node/backfill.py
================================================
import logging
from github import Github, GithubException, RateLimitExceededException
from github.Commit import Commit
from rid_lib.ext import Bundle
# Assuming GithubCommit RID type is accessible
from .types import GithubCommit 
from .core import node
from .config import (
    GITHUB_TOKEN, MONITORED_REPOS,
    LAST_PROCESSED_SHA, update_state_file 
)

logger = logging.getLogger(__name__)

# Initialize GitHub client (authenticated if token provided)
github_client = Github(GITHUB_TOKEN) if GITHUB_TOKEN else Github()
logger.info(f"GitHub client initialized. Authenticated: {bool(GITHUB_TOKEN)}")

def perform_backfill():
    """
    One-time startup backfill: fetch all commits since LAST_PROCESSED_SHA
    for each monitored repo, bundle them as NEW, and persist the latest SHA processed.
    Processes commits oldest-to-newest after fetching.
    """
    logger.info("Starting GitHub backfill process...")
    
    # Load the state dictionary once before the loop
    current_state = LAST_PROCESSED_SHA.copy() # Use a copy to avoid modifying during iteration if needed
    newest_sha_processed_overall_map = {} # Track newest SHA per repo processed in this run

    if not MONITORED_REPOS:
        logger.warning("No repositories configured in MONITORED_REPOS. Backfill skipped.")
        return

    for repo_full_name in MONITORED_REPOS:
        try:
            owner, repo_name_only = repo_full_name.split('/')
            # Get the last processed SHA for *this specific repository*
            last_sha_for_repo = current_state.get(repo_full_name)
            logger.info(f"Backfilling repository: {repo_full_name} since SHA: {last_sha_for_repo or 'beginning'}")

            gh_repo = github_client.get_repo(repo_full_name)
            commits_to_process_buffer: list[Commit] = []
            
            # Iterate commits newest-first until we find the last processed one
            paginated_commits = gh_repo.get_commits()
            logger.debug(f"Fetching commits for {repo_full_name}...")
            commit_count = 0
            for commit in paginated_commits:
                commit_count += 1
                # Check against the specific SHA for this repo
                if last_sha_for_repo and commit.sha == last_sha_for_repo:
                    logger.info(f"Found last processed SHA {last_sha_for_repo} in {repo_full_name}. Stopping fetch for this repo.")
                    break
                commits_to_process_buffer.append(commit)
                # Safety break for potentially huge repos without a known SHA
                # Adjust limit as needed
                if commit_count % 100 == 0:
                    logger.debug(f"Fetched {commit_count} commits for {repo_full_name} so far...")
                # if commit_count > 1000: 
                #    logger.warning(f"Reached fetch limit (1000) for {repo_full_name}. Consider adjusting.")
                #    break 
            
            logger.info(f"Found {len(commits_to_process_buffer)} new commits in {repo_full_name} to backfill.")

            # Process commits oldest → newest
            newest_sha_processed_in_repo = None
            for commit in reversed(commits_to_process_buffer):
                try:
                    rid = GithubCommit(owner=owner, repo=repo_name_only, sha=commit.sha)
                    # Extract commit details carefully, handling potential missing attributes
                    author = commit.commit.author
                    committer = commit.commit.committer
                    
                    contents = {
                        "sha": commit.sha,
                        "message": commit.commit.message,
                        "author_name": author.name if author else None,
                        "author_email": author.email if author else None,
                        "author_date": author.date.isoformat() if author and author.date else None,
                        "committer_name": committer.name if committer else None,
                        "committer_email": committer.email if committer else None,
                        "committer_date": committer.date.isoformat() if committer and committer.date else None,
                        "html_url": commit.html_url,
                        "parents": [p.sha for p in commit.parents] # List of parent SHAs
                    }
                    bundle = Bundle.generate(rid=rid, contents=contents)
                    logger.debug(f"Bundling backfill commit {rid}")
                    node.processor.handle(bundle=bundle) 
                    
                    # Track the newest SHA processed in this run for this repo
                    newest_sha_processed_in_repo = commit.sha 
                    
                except Exception as e:
                    logger.error(f"Error processing commit {commit.sha} in {repo_full_name}: {e}", exc_info=True)

            # Store the newest SHA processed for this repo during this run
            if newest_sha_processed_in_repo:
                newest_sha_processed_overall_map[repo_full_name] = newest_sha_processed_in_repo
                logger.debug(f"Newest SHA processed for {repo_full_name} in this run: {newest_sha_processed_in_repo}")

        except RateLimitExceededException:
            logger.error(f"GitHub API rate limit exceeded while backfilling {repo_full_name}. Aborting backfill. Try again later or use a GITHUB_TOKEN.")
            # Depending on requirements, could wait and retry, but for now, we stop.
            return # Stop the entire backfill process
        except GithubException as e:
            logger.error(f"GitHub API error for repository {repo_full_name}: {e}. Skipping this repo.")
            continue # Skip to the next repository
        except Exception as e:
            logger.error(f"Unexpected error backfilling repository {repo_full_name}: {e}", exc_info=True)
            continue # Skip to the next repository

    # After processing all repos, persist the newest SHAs found (if changed)
    updated_count = 0
    for repo_full_name, newest_sha in newest_sha_processed_overall_map.items():
        if newest_sha != current_state.get(repo_full_name):
            update_state_file(repo_full_name, newest_sha) # Call function from config.py
            updated_count += 1
        else:
            logger.debug(f"No state update needed for {repo_full_name}, newest SHA {newest_sha} is same as stored.")
            
    if updated_count > 0:
        logger.info(f"Backfill complete. Updated state for {updated_count} repositories.")
    else:
        logger.info(f"Backfill complete. No new commits found or state changes required across monitored repositories.")

if __name__ == "__main__":
    # Example of how to run backfill directly for testing
    # Requires node to be started if handle() depends on active components
    # In practice, this is called by server.py during startup
    logging.basicConfig(level=logging.INFO)
    logger.info("Running backfill directly for testing...")
    # node.start() # Might be needed depending on node.processor.handle implementation
    perform_backfill()
    # node.stop() 



================================================
FILE: services/github/node/config.py
================================================
import logging
import json
from pydantic_settings import BaseSettings
from pydantic import Field
from typing import Optional, List

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Settings(BaseSettings):
    # --- KOI-net Configuration ---
    URL: str = Field(..., description="Public URL where this Github node can be reached by other KOI-net nodes.")
    FIRST_CONTACT: str = Field(..., description="URL of the coordinator node or another known KOI-net node.")

    # --- Github Sensor Server Configuration ---
    HOST: str = Field(default="127.0.0.1", description="Host the FastAPI server should listen on.")
    PORT: int = Field(default=8001, description="Port the FastAPI server should listen on.")

    # --- Github Configuration ---
    GITHUB_TOKEN: Optional[str] = Field(default=None, description="Optional Github Personal Access Token.")
    # Load MONITORED_REPOS from env var as a string
    MONITORED_REPOS_STR: str = Field(alias="MONITORED_REPOS", default="", description="Comma-separated list of repositories (owner/repo).")
    GITHUB_WEBHOOK_SECRET: str = Field(..., description="Secret used to verify webhook signatures.")

    # --- State File ---
    STATE_FILE_PATH: str = Field(default="state.json", description="Path to the JSON file storing the last processed commit SHA.")

    # --- Logging Configuration ---
    LOG_LEVEL: str = Field(default="INFO", description="Logging level (e.g., DEBUG, INFO, WARNING).")

    @property
    def MONITORED_REPOS(self) -> List[str]:
        """Parses the comma-separated MONITORED_REPOS_STR into a list of 'owner/repo' strings."""
        if not self.MONITORED_REPOS_STR:
            return []
        # Split by comma, strip whitespace, and filter out empty strings
        repos = [repo.strip() for repo in self.MONITORED_REPOS_STR.split(',') if repo.strip()]
        logger.debug(f"Parsed monitored repos: {repos}")
        return repos

    class Config:
        env_file = 'services/github/.env'
        env_file_encoding = 'utf-8'
        extra = 'ignore'

# Instantiate settings early to catch config errors on import
try:
    settings = Settings()
    # Apply log level from settings
    logging.getLogger().setLevel(settings.LOG_LEVEL.upper())
    logger.info("Configuration loaded successfully.")
    logger.info(f"Settings: {settings.model_dump(exclude={'GITHUB_TOKEN', 'GITHUB_WEBHOOK_SECRET'})}") # Exclude token and secret from debug logs
except Exception as e:
    logger.exception(f"Failed to load configuration from .env file: {e}")
    # Optionally re-raise or exit if config is critical
    raise

# --- Exported Variables ---
# Make settings easily accessible throughout the application
URL = settings.URL
FIRST_CONTACT = settings.FIRST_CONTACT
HOST = settings.HOST
PORT = settings.PORT
GITHUB_TOKEN = settings.GITHUB_TOKEN
GITHUB_WEBHOOK_SECRET = settings.GITHUB_WEBHOOK_SECRET
MONITORED_REPOS = settings.MONITORED_REPOS # Access the parsed list via the property
STATE_FILE_PATH = settings.STATE_FILE_PATH
LOG_LEVEL = settings.LOG_LEVEL

# --- State Management (Loading initial state & update function) ---
LAST_PROCESSED_SHA = {} # Dictionary mapping repo_name -> last_sha

def load_state():
    """Loads the last processed SHA state from the JSON file."""
    global LAST_PROCESSED_SHA
    try:
        with open(STATE_FILE_PATH, 'r') as f:
            LAST_PROCESSED_SHA = json.load(f)
        logger.info(f"Loaded state from '{STATE_FILE_PATH}': {list(LAST_PROCESSED_SHA.keys())}")
    except FileNotFoundError:
        logger.warning(f"State file '{STATE_FILE_PATH}' not found. Starting with empty state.")
        LAST_PROCESSED_SHA = {}
    except json.JSONDecodeError:
        logger.error(f"Error decoding JSON from state file '{STATE_FILE_PATH}'. Starting with empty state.")
        LAST_PROCESSED_SHA = {}
    except Exception as e:
        logger.error(f"Unexpected error loading state file '{STATE_FILE_PATH}': {e}", exc_info=True)
        LAST_PROCESSED_SHA = {}

def update_state_file(repo_name: str, last_sha: str):
    """Updates the state file with the latest processed SHA for a repo."""
    global LAST_PROCESSED_SHA
    LAST_PROCESSED_SHA[repo_name] = last_sha
    try:
        # Create directory if it doesn't exist (though it should usually be just the filename)
        # import os
        # os.makedirs(os.path.dirname(STATE_FILE_PATH), exist_ok=True) # Uncomment if path can include dirs
        with open(STATE_FILE_PATH, 'w') as f:
            json.dump(LAST_PROCESSED_SHA, f, indent=4)
        logger.debug(f"Updated state file '{STATE_FILE_PATH}' for {repo_name} with SHA: {last_sha}")
    except IOError as e:
        logger.error(f"Failed to write state file '{STATE_FILE_PATH}': {e}")
    except Exception as e:
        logger.error(f"Unexpected error writing state file '{STATE_FILE_PATH}': {e}", exc_info=True)

# Load initial state when config module is imported
load_state()



================================================
FILE: services/github/node/core.py
================================================
import os
import shutil
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

from .config import HOST, PORT, FIRST_CONTACT
from .types import GithubCommit

logger = logging.getLogger(__name__)

name = "sensor"

identity_dir = f".koi/{name}"
cache_dir = f".koi/{name}/rid_cache_{name}"
# Remove existing directories if they exist
shutil.rmtree(identity_dir, ignore_errors=True)
shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the directories
os.makedirs(identity_dir, exist_ok=True)
os.makedirs(cache_dir, exist_ok=True)

# Initialize the KOI-net Node Interface for the GitHub Sensor
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        base_url=f"http://{HOST}:{PORT}/koi-net",
        node_type=NodeType.FULL, # Assuming it provides data and potentially processes
        provides=NodeProvides(
            event=[GithubCommit], # Provides GithubCommit events
            state=[GithubCommit]  # Can serve state for GithubCommit RIDs
        )
    ),
    use_kobj_processor_thread=True, # Use a background thread for processing
    first_contact=FIRST_CONTACT,   # Coordinator node to connect to initially
    identity_file_path=f"{identity_dir}/{name}_identity.json", # Use the variable
    event_queues_file_path=f"{identity_dir}/{name}_event_queues.json", # Use the variable
    cache_directory_path=cache_dir # Use the variable
)

logger.info(f"Initialized NodeInterface: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {FIRST_CONTACT}")



================================================
FILE: services/github/node/loader.py
================================================
from .core import node
from .handlers import github

def register_handlers():
    print("Registering GITHUB handlers...")
    _ = github


================================================
FILE: services/github/node/server.py
================================================
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, Request, Body, Header, HTTPException
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH
)
from .core import node
from .webhook import router as github_router
from .backfill import perform_backfill
from .loader import register_handlers

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup, backfill, and shutdown."""
    logger.info("Starting FastAPI application lifespan...")
    # Start the KOI-net node
    try:
        register_handlers()
        node.start()
        logger.info("KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        # Depending on requirements, you might want to prevent the app from starting
        raise RuntimeError("Failed to initialize KOI-net node") from e

    # Run initial backfill in the background
    logger.info("Scheduling initial GitHub backfill...")
    # Run backfill in a separate thread/task to avoid blocking startup
    # Note: perform_backfill itself is synchronous, so asyncio.to_thread is suitable.
    # If perform_backfill becomes async, just use asyncio.create_task directly.
    backfill_task = asyncio.to_thread(perform_backfill)
    # If you need to wait for backfill completion before yielding, await here.
    # For now, let it run in the background.

    try:
        yield # Application runs here
    finally:
        # Cleanup: Cancel pending tasks, stop the node
        logger.info("Shutting down FastAPI application...")
        # Attempt to gracefully cancel the backfill if it's still running
        # This might require more sophisticated task management if backfill is long-running
        # if backfill_task and not backfill_task.done():
        #     try:
        #         backfill_task.cancel()
        #         await backfill_task
        #     except asyncio.CancelledError:
        #         logger.info("Backfill task cancelled.")
        #     except Exception as e:
        #         logger.error(f"Error cancelling backfill task: {e}", exc_info=True)
        
        try:
            node.stop()
            logger.info("KOI-net node stopped successfully.")
        except Exception as e:
            logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
        logger.info("FastAPI application shutdown complete.")

# Create FastAPI app instance
app = FastAPI(
    title="KOI-net GitHub Sensor Node",
    description="Listens for GitHub webhooks and performs backfill to ingest commit data.",
    version="0.1.0",
    lifespan=lifespan # Register the lifespan context manager
)

# Define KOI-net API router
koi_net_router = APIRouter(prefix="/koi-net")

@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    logger.info(f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)")
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    return {} # Broadcast endpoint typically returns empty success

@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)

@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    # The default response_handler reads from cache, fulfilling the provides=[GithubCommit] state
    return node.network.response_handler.fetch_rids(req)

@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}")
    manifests_payload = node.network.response_handler.fetch_manifests(req)
    # Add any custom logic here if you need to fetch manifests not in cache
    return manifests_payload # The default handler already includes not_found

@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    bundles_payload = node.network.response_handler.fetch_bundles(req)
    # Add any custom logic here if you need to fetch bundles not in cache
    return bundles_payload # The default handler already includes not_found and deferred

# Include routers
app.include_router(koi_net_router) # KOI-net API endpoints
app.include_router(github_router) # GitHub webhook endpoint

logger.info("FastAPI application configured with webhook and KOI-net routers.")



================================================
FILE: services/github/node/types.py
================================================
from rid_lib.core import ORN

class GithubCommit(ORN):
    """
    Resource Identifier (RID) for a specific GitHub commit.
    
    Format: orn:github.commit:<owner>/<repo>/<sha>
    Example: orn:github.commit:microsoft/vscode/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
    """
    namespace = "github.commit"

    def __init__(self, owner: str, repo: str, sha: str):
        """
        Initialize a GitHub commit RID.
        
        Args:
            owner: The repository owner (user or organization)
            repo: The repository name
            sha: The commit SHA (full 40-character or shortened)
        """
        if not owner or not repo or not sha:
            raise ValueError("Owner, repo, and SHA cannot be empty")
        
        if "/" in owner or "/" in repo:
            raise ValueError("Owner and repo cannot contain '/' character")
            
        self.owner = owner
        self.repo = repo
        self.sha = sha

    @property
    def reference(self) -> str:
        """Returns the reference part of the RID: '<owner>/<repo>/<sha>'."""
        return f"{self.owner}/{self.repo}/{self.sha}"
    
    @property
    def repository_full_name(self) -> str:
        """Returns the full repository name: '<owner>/<repo>'."""
        return f"{self.owner}/{self.repo}"
    
    @property
    def html_url(self) -> str:
        """Returns the HTML URL to view this commit on GitHub."""
        return f"https://github.com/{self.owner}/{self.repo}/commit/{self.sha}"
    
    @property
    def api_url(self) -> str:
        """Returns the GitHub API URL for this commit."""
        return f"https://api.github.com/repos/{self.owner}/{self.repo}/commits/{self.sha}"

    @classmethod
    def from_reference(cls, reference: str) -> "GithubCommit":
        """
        Creates a GithubCommit instance from its reference string.
        
        Args:
            reference: String in format '<owner>/<repo>/<sha>'
            
        Returns:
            GithubCommit instance
            
        Raises:
            ValueError: If the reference format is invalid
        """
        try:
            parts = reference.split("/", maxsplit=2)
            if len(parts) != 3:
                raise ValueError("Reference must contain exactly two '/' separators")
                
            owner, repo, sha = parts
            
            if not owner or not repo or not sha:
                raise ValueError("Owner, repo, and SHA parts cannot be empty")
                
            # Basic SHA length check
            if len(sha) < 7:  # Minimum length for a short SHA
                raise ValueError(f"SHA part seems too short: {sha}")
                
            return cls(owner=owner, repo=repo, sha=sha)
            
        except ValueError as e:
            raise ValueError(f"Invalid reference format for GithubCommit. Expected '<owner>/<repo>/<sha>', got '{reference}'. Error: {e}") from e
        except Exception as e:
            raise TypeError(f"Unexpected error parsing GithubCommit reference '{reference}': {e}") from e



================================================
FILE: services/github/node/webhook.py
================================================
import logging
import hmac
import hashlib
import json
from fastapi import APIRouter, Request, Header, HTTPException, Body
from rid_lib.ext import Bundle
from .types import GithubCommit
from .core import node
from .config import (
    GITHUB_WEBHOOK_SECRET, MONITORED_REPOS,
    update_state_file, LAST_PROCESSED_SHA 
)

logger = logging.getLogger(__name__)

router = APIRouter()

async def verify_signature(request: Request, x_hub_signature_256: str = Header(None)):
    """Verify the GitHub webhook signature."""
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning("Webhook verification skipped: GITHUB_WEBHOOK_SECRET not set.")
        return # Skip verification if secret is not configured

    # This check is removed because FastAPI ensures the header is present
    # if x_hub_signature_256 is None:
    #     logger.error("Webhook verification failed: Missing X-Hub-Signature-256 header")
    #     raise HTTPException(status_code=400, detail="Missing X-Hub-Signature-256 header")

    body = await request.body()
    hash_object = hmac.new(GITHUB_WEBHOOK_SECRET.encode('utf-8'), msg=body, digestmod=hashlib.sha256)
    expected_signature = "sha256=" + hash_object.hexdigest()

    if not hmac.compare_digest(expected_signature, x_hub_signature_256):
        logger.error(f"Webhook verification failed: Invalid signature. Expected: {expected_signature}, Got: {x_hub_signature_256}")
        raise HTTPException(status_code=403, detail="Invalid signature")

    logger.debug("Webhook signature verified successfully.")


@router.post("/github/webhook", status_code=202)  # Use 202 Accepted as we process async
async def github_webhook(
    request: Request,
    x_github_event: str = Header(...),  # Required header
    x_hub_signature_256: str = Header(...)  # Required for verification
):
    """Handle incoming GitHub webhook events (specifically 'push')."""
    logger.info(f"Received GitHub webhook event: {x_github_event}")

    try:
        # --- Signature Verification (Optional) ---
        # await verify_signature(request, x_hub_signature_256)

        # --- Parse JSON Payload ---
        raw_body = await request.body()
        try:
            payload = json.loads(raw_body)
        except json.JSONDecodeError:
            logger.error("Invalid JSON in GitHub webhook payload")
            raise HTTPException(status_code=400, detail="Invalid JSON")

        # --- Event Handling ---
        if x_github_event == "ping":
            logger.info("Received 'ping' event from GitHub. Responding OK.")
            return {"message": "Pong!"}

        if x_github_event != "push":
            logger.debug(f"Ignoring non-'push' event: {x_github_event}")
            return {"message": f"Ignoring event type: {x_github_event}"}

        logger.info(f"Processing 'push' event: {payload}")

        # --- Process 'push' Event ---
        repo_info = payload.get("repository", {})
        repo_full_name = repo_info.get("full_name")
        repo_owner = repo_info.get("owner", {}).get("login") or repo_info.get("owner", {}).get("name")
        repo_name = repo_info.get("name")
        commits = payload.get("commits", [])
        head_commit = payload.get("head_commit", {})

        if not repo_full_name or not repo_owner or not repo_name:
            logger.error(f"Webhook payload missing repository details: {repo_info}")
            raise HTTPException(status_code=400, detail="Missing repository information in payload")
            
        # Check if the repository is monitored
        if repo_full_name not in MONITORED_REPOS:
            logger.debug(f"Ignoring push event for non-monitored repository: {repo_full_name}")
            return {"message": f"Repository {repo_full_name} not monitored"}

        if not commits and not head_commit:
             logger.warning(f"'push' event for {repo_full_name} received without 'commits' or 'head_commit' data. Possibly a branch deletion or tag push? Payload head: {payload.get('ref', '')}")
             return {"message": "No commit data found in push event"}

        # Determine the commit(s) to process and the SHA to potentially update state with
        commits_to_process = []
        sha_to_update_state = None
        
        head_commit_id = head_commit.get('id')
        if head_commit_id:
            commits_to_process = [head_commit] # Use head_commit as the primary source
            sha_to_update_state = head_commit_id # This is the SHA representing the push tip
            logger.debug(f"Processing head_commit: {head_commit_id}")
        elif commits:
            commits_to_process = commits # Fallback to commits list
            # If using commits list, the last commit's SHA is the best candidate for state update
            if commits:
                sha_to_update_state = commits[-1].get('id') 
            logger.debug(f"Processing commits list (count: {len(commits)}). Potential state update SHA: {sha_to_update_state}")
        else:
             # This case should ideally not be reached due to the check above, but included for completeness
             logger.warning(f"No processable commit data found in push event for {repo_full_name}. Skipping.")
             return {"message": "No processable commit data"}

        processed_new_commit = False
        for commit in commits_to_process:
            commit_sha = commit.get("id")
            if not commit_sha:
                logger.warning("Skipping commit in payload with missing 'id'.")
                continue
            
            # Avoid reprocessing the last known SHA for this repo
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if last_known_sha_for_repo and commit_sha == last_known_sha_for_repo:
                logger.debug(f"Skipping commit {commit_sha} for {repo_full_name} as it matches last known SHA {last_known_sha_for_repo}.")
                continue

            # Inner try-except for processing individual commits within the push
            try:
                # Construct RID
                rid = GithubCommit(owner=repo_owner, repo=repo_name, sha=commit_sha)
                
                # Extract details - ensure keys exist
                author = commit.get("author", {})
                committer = commit.get("committer", {})
                
                contents = {
                    "sha": commit_sha,
                    "message": commit.get("message"),
                    "author_name": author.get("name"),
                    "author_email": author.get("email"),
                    "author_date": commit.get("timestamp"), # GitHub often uses 'timestamp'
                    "committer_name": committer.get("name"),
                    "committer_email": committer.get("email"),
                    "committer_date": committer.get("timestamp"),
                    "html_url": commit.get("url"), # Use 'url' from webhook payload
                    "parents": commit.get("parents", []) # Typically a list of SHAs in webhook
                }
                
                bundle = Bundle.generate(rid=rid, contents=contents)
                logger.debug(f"Bundling webhook commit {rid}")
                node.processor.handle(bundle=bundle)
                
                processed_new_commit = True # Mark that we processed at least one new commit
            
            except Exception as e:
                 logger.error(f"Error processing webhook commit {commit_sha} for {repo_full_name}: {e}", exc_info=True)
                 # Decide whether to continue processing other commits in the push or stop
                 continue # Continue with next commit in the webhook push
        
        # Update state file only if we processed a new commit and have a valid SHA representing the push tip
        if processed_new_commit and sha_to_update_state:
            # Check again if the sha_to_update is different from the stored one before writing
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if sha_to_update_state != last_known_sha_for_repo:
                logger.info(f"Webhook processing complete for {repo_full_name}. Updating state to SHA: {sha_to_update_state}")
                update_state_file(repo_full_name, sha_to_update_state) 
            else:
                 logger.info(f"Webhook processing complete for {repo_full_name}. State SHA {sha_to_update_state} already stored.")
        elif processed_new_commit:
             logger.warning(f"Webhook processing complete for {repo_full_name}. Processed new commit(s) but could not determine SHA for state update.")
        else:
             logger.info(f"Webhook processing complete for {repo_full_name}. No new commits processed or state updated.")

        return {"message": "Webhook processed successfully"}

    # Exception handlers are now correctly indented relative to the main 'try' block
    except HTTPException as he:
        # Re-raise HTTP exceptions to return proper status codes
        logger.warning(f"HTTP Exception during webhook processing: {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"Unexpected error handling webhook event {x_github_event}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal server error handling webhook")



================================================
FILE: services/github/node/handlers/github.py
================================================
import logging
from rid_lib.ext import Bundle
# Assuming GithubCommit RID type is accessible
from ..core import node # Import the initialized node instance
from ..types import GithubCommit # Ensure this import is correct

# Imports needed for coordinator_contact handler from refactor.md
from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode # Assuming this RID type is accessible

logger = logging.getLogger(__name__)

@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of the coordinator node (or other nodes providing KoiNetNode events).
    
    On discovering a NEW coordinator, proposes a WEBHOOK edge for bidirectional
    communication and requests a list of other known nodes (sync).
    (Based on refactor.md example)
    """
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return
        
    # Validate that the discovered node actually provides network events
    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if KoiNetNode not in profile.provides.event:
            logger.debug(f"Node {kobj.rid} does not provide KoiNetNode events. Ignoring.")
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    logger.info(f"Identified potential coordinator/peer: {kobj.rid}; proposing WEBHOOK edge")
    try:
        # Propose edge FROM coordinator TO self
        edge_bundle = generate_edge_bundle(
            source=kobj.rid, 
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode] # Specify the type of information this edge carries
        )
        processor.handle(bundle=edge_bundle)
    except Exception as e:
        logger.error(f"Failed to generate or handle WEBHOOK edge bundle for {kobj.rid}: {e}", exc_info=True)

    # Sync network nodes from the discovered node
    logger.info(f"Syncing network nodes from {kobj.rid}")
    try:
        payload = processor.network.request_handler.fetch_rids(kobj.rid, rid_types=[KoiNetNode])
        if not payload or not payload.rids:
             logger.warning(f"Received empty RIDs payload from {kobj.rid} during sync.")
             return
             
        logger.debug(f"Received {len(payload.rids)} RIDs from {kobj.rid}")
        for rid in payload.rids:
            # Don't process self or already known nodes
            if rid == processor.identity.rid or processor.cache.exists(rid):
                continue
            logger.debug(f"Handling discovered RID from sync: {rid}")
            # Handle the RID to fetch its details (profile, etc.)
            processor.handle(rid=rid, source=KnowledgeSource.External)
    except Exception as e:
        logger.error(f"Failed during network sync with {kobj.rid}: {e}", exc_info=True)

@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit(processor: ProcessorInterface, kobj: KnowledgeObject):
    """
    Basic handler for processing GithubCommit bundles.
    Currently just logs information.
    
    Args:
        bundle: The Bundle object containing the GithubCommit RID and contents.
    """
    try:
        # kobj.bundle is guaranteed to exist in Bundle handler phase
        bundle = kobj.bundle
        rid: GithubCommit = bundle.rid
        contents: dict = bundle.contents
        
        logger.info(f"Processing commit: {rid} (Normalized Type: {kobj.normalized_event_type}, Source: {kobj.source})")
        # Log some details from the contents for visibility
        logger.debug(f"  Author: {contents.get('author_name')} <{contents.get('author_email')}>")
        logger.debug(f"  Message: {contents.get('message', '').splitlines()[0][:80]}...") # Log first line up to 80 chars
        logger.debug(f"  URL: {contents.get('html_url')}")

        # Default Bundle handler will write to cache based on normalized_event_type.
        # If you needed to modify contents *before* caching, you would return
        # the modified kobj here: return kobj

        # If you wanted to stop processing this specific KObj (e.g., based on content),
        # you could return STOP_CHAIN here: return STOP_CHAIN

        # Returning None passes the kobj unchanged to the next handler in the chain
        # (likely the default handler which performs the cache write)
        return None

    except Exception as e:
        logger.error(f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True)
        # Decide if error should stop the pipeline for this KObj
        # return STOP_CHAIN
        return None # Continue processing despite handler error

# Add a NEW handler specifically to propose a GithubCommit edge to the coordinator
@node.processor.register_handler(HandlerType.Final, rid_types=[KoiNetNode])
def propose_github_commit_edge_to_coordinator(processor: ProcessorInterface, kobj: KnowledgeObject):
    # This handler runs after a NEW KoiNetNode is fully processed (cached, graph updated, default edges proposed)
    # It's in the Final phase, less intrusive to core pipeline flow.
    # Simplified logic: If we just processed a NEW KoiNetNode and it's the *only* other node in our graph besides ourselves,
    # assume it's the coordinator and propose the GithubCommit edge.
    if kobj.normalized_event_type != EventType.NEW:
        return
        
    known_peers = processor.network.graph.get_neighbors()
    if len(known_peers) == 1 and known_peers[0] == kobj.rid:
        logger.info(f"Assuming {kobj.rid} is the coordinator. Proposing WEBHOOK edge for GithubCommit events.")
        try:
            github_edge_bundle = generate_edge_bundle(
                source=node.identity.rid, # I am the source of these events
                target=kobj.rid,           # The coordinator is the target
                edge_type=EdgeType.WEBHOOK, # Use webhook
                rid_types=[GithubCommit]    # Specify GithubCommit events
            )
            # Queue this new edge bundle for processing
            processor.handle(bundle=github_edge_bundle)
        except Exception as e:
            logger.error(f"Failed to generate/handle GithubCommit edge bundle for {kobj.rid}: {e}", exc_info=True)

logger.info("GithubCommit Bundle handler and KoiNetNode handlers registered.")
</file>

<file path=".ai/code-3.txt">
Directory structure:
└── blockscience-koi-net-hackmd-sensor-node/
    ├── README.md
    ├── koi-net-hackmd-sensor-node.service
    ├── LICENSE
    ├── requirements.txt
    ├── rid_types.py
    └── hackmd_sensor_node/
        ├── __init__.py
        ├── __main__.py
        ├── backfill.py
        ├── config.py
        ├── core.py
        ├── hackmd_api.py
        ├── handlers.py
        └── server.py


Files Content:

================================================
FILE: README.md
================================================
# koi-net-hackmd-sensor-node
 HackMD sensor node implementation for BlockScience's KOI-net 



================================================
FILE: koi-net-hackmd-sensor-node.service
================================================
[Unit]
Description=KOI-net HackMD Sensor Node Service
After=network.target

[Service]
WorkingDirectory=/home/dev/koi-net-hackmd-sensor-node
ExecStart=/home/dev/koi-net-hackmd-sensor-node/venv/bin/python3 -m hackmd_sensor_node
Restart=always

[Install]
WantedBy=multi-user.target


================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 BlockScience

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: requirements.txt
================================================
koi-net>=1.0.0b7
rid-lib>=3.2.3
rich
fastapi
uvicorn
python-dotenv
requests


================================================
FILE: rid_types.py
================================================
from rid_lib.core import ORN

class HackMDNote(ORN):
    namespace = "hackmd.note"
    
    def __init__(self, note_id: str):
        self.note_id = note_id
        
    @property
    def reference(self):
        return self.note_id
    
    @classmethod
    def from_reference(cls, reference):
        return cls(reference)    


================================================
FILE: hackmd_sensor_node/__init__.py
================================================
import logging
from rich.logging import RichHandler

logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

rich_handler = RichHandler()
rich_handler.setLevel(logging.INFO)
rich_handler.setFormatter(logging.Formatter(
    "%(name)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
))

file_handler = logging.FileHandler("node-log.txt")
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(logging.Formatter(
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
))

# Add both
logger.addHandler(rich_handler)
logger.addHandler(file_handler)


================================================
FILE: hackmd_sensor_node/__main__.py
================================================
import uvicorn
from .config import HOST, PORT

uvicorn.run("hackmd_sensor_node.server:app", host=HOST, port=PORT, log_config=None)


================================================
FILE: hackmd_sensor_node/backfill.py
================================================
import logging
import asyncio
from rid_lib.ext import Bundle
from rid_types import HackMDNote
from . import hackmd_api
from .core import node

logger = logging.getLogger(__name__)

async def backfill(team_path="blockscience"):
    notes = await hackmd_api.async_request(f"/teams/{team_path}/notes")
    
    logger.debug(f"Found {len(notes)} in team")

    for note in notes:
        note_rid = HackMDNote(note["id"])
        
        note_bundle = Bundle.generate(
            rid=note_rid,
            contents=note
        )
        
        node.processor.handle(bundle=note_bundle)
        
if __name__ == "__main__":
    node.start()
    asyncio.run(
        backfill()
    )
    node.stop()


================================================
FILE: hackmd_sensor_node/config.py
================================================
import os
from dotenv import load_dotenv

load_dotenv()

HOST = "127.0.0.1"
PORT = 8002
URL = f"http://{HOST}:{PORT}/koi-net"

FIRST_CONTACT = "http://127.0.0.1:8000/koi-net"

HACKMD_API_TOKEN = os.environ["HACKMD_API_TOKEN"]


================================================
FILE: hackmd_sensor_node/core.py
================================================
import logging
from rid_types import HackMDNote
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides
from koi_net.processor.default_handlers import (
    basic_rid_handler,
    edge_negotiation_handler,
    basic_network_output_filter
)
from .config import URL, FIRST_CONTACT

logger = logging.getLogger(__name__)


node = NodeInterface(
    name="hackmd-sensor",
    profile=NodeProfile(
        base_url=URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[HackMDNote],
            state=[HackMDNote]
        )
    ),
    use_kobj_processor_thread=True,
    first_contact=FIRST_CONTACT,
    handlers=[
        basic_rid_handler,
        edge_negotiation_handler,
        basic_network_output_filter
    ]
)

from . import handlers


================================================
FILE: hackmd_sensor_node/hackmd_api.py
================================================
import asyncio
import httpx
from .config import HACKMD_API_TOKEN

api_base_url = "https://api.hackmd.io/v1"

def request(path, method="GET"):
    resp = httpx.request(
        method=method,
        url=api_base_url+path,
        headers={
            "Authorization": "Bearer " + HACKMD_API_TOKEN
        }
    )

    if resp.status_code == 200:
        return resp.json()
    
    else:
        print(resp.status_code, resp.text)
        return

async def async_request(path, method="GET"):
    timeout = 60
    
    while True: 
        async with httpx.AsyncClient() as client:
            
            resp = await client.request(
                method=method,
                url=api_base_url+path,
                headers={
                    "Authorization": "Bearer " + HACKMD_API_TOKEN
                }
            )
        

        if resp.status_code == 200:
            return resp.json()

        elif resp.status_code == 429:
            print(resp.status_code, resp.text, f"retrying in {timeout} seconds")
            await asyncio.sleep(timeout)
            timeout *= 2
        else:
            print(resp.status_code, resp.text)
            return


================================================
FILE: hackmd_sensor_node/handlers.py
================================================
import logging
from multiprocessing import process
from koi_net.processor.handler import HandlerType, STOP_CHAIN
from koi_net.processor.knowledge_object import KnowledgeSource, KnowledgeObject
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.ext import Bundle
from rid_lib.types import KoiNetNode

from rid_types import HackMDNote
from .core import node
from . import hackmd_api

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    # when I found out about a new node
    if kobj.normalized_event_type != EventType.NEW: 
        return
    
    node_profile = kobj.bundle.validate_contents(NodeProfile)
    
    # looking for event provider of nodes
    if KoiNetNode not in node_profile.provides.event:
        return
    
    logger.debug("Identified a coordinator!")
    logger.debug("Proposing new edge")
    
    # queued for processing
    processor.handle(bundle=generate_edge_bundle(
        source=kobj.rid,
        target=node.identity.rid,
        edge_type=EdgeType.WEBHOOK,
        rid_types=[KoiNetNode]
    ))
    
    logger.debug("Catching up on network state")
    
    rid_payload = processor.network.request_handler.fetch_rids(kobj.rid, rid_types=[KoiNetNode])
        
    rids = [
        rid for rid in rid_payload.rids 
        if rid != processor.identity.rid and 
        not processor.cache.exists(rid)
    ]
    
    bundle_payload = processor.network.request_handler.fetch_bundles(kobj.rid, rids=rids)
    
    for bundle in bundle_payload.bundles:
        # marked as external since we are handling RIDs from another node
        # will fetch remotely instead of checking local cache
        processor.handle(bundle=bundle, source=KnowledgeSource.External)
    logger.debug("Done")


@node.processor.register_handler(HandlerType.Manifest)
def custom_manifest_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    if type(kobj.rid) == HackMDNote:
        logger.debug("Skipping HackMD note manifest handling")
        return
    
    prev_bundle = processor.cache.read(kobj.rid)

    if prev_bundle:
        if kobj.manifest.sha256_hash == prev_bundle.manifest.sha256_hash:
            logger.debug("Hash of incoming manifest is same as existing knowledge, ignoring")
            return STOP_CHAIN
        if kobj.manifest.timestamp <= prev_bundle.manifest.timestamp:
            logger.debug("Timestamp of incoming manifest is the same or older than existing knowledge, ignoring")
            return STOP_CHAIN
        
        logger.debug("RID previously known to me, labeling as 'UPDATE'")
        kobj.normalized_event_type = EventType.UPDATE

    else:
        logger.debug("RID previously unknown to me, labeling as 'NEW'")
        kobj.normalized_event_type = EventType.NEW
        
    return kobj
    
    
@node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
def custom_hackmd_bundle_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    assert type(kobj.rid) == HackMDNote
    
    prev_bundle = processor.cache.read(kobj.rid)
    
    if prev_bundle:
        prevChangedAt = prev_bundle.contents["lastChangedAt"]
        currChangedAt = kobj.contents["lastChangedAt"]
        logger.debug(f"Changed at {prevChangedAt} -> {currChangedAt}")
        if currChangedAt > prevChangedAt:
            logger.debug("Incoming note has been changed more recently!")
            kobj.normalized_event_type = EventType.UPDATE
            
        else:
            logger.debug("Incoming note is not newer")
            return STOP_CHAIN
        
    else:
        logger.debug("Incoming note is previously unknown to me")
        kobj.normalized_event_type = EventType.NEW
        
    logger.debug("Retrieving full note...")
    data = hackmd_api.request(f"/notes/{kobj.rid.note_id}")
    
    if not data:
        logger.debug("Failed.")
        return STOP_CHAIN
    
    logger.debug("Done.")
    
    full_note_bundle = Bundle.generate(
        rid=kobj.rid,
        contents=data
    )
    
    kobj.manifest = full_note_bundle.manifest
    kobj.contents = full_note_bundle.contents
    
    return kobj


================================================
FILE: hackmd_sensor_node/server.py
================================================
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH
)
from .core import node
from .backfill import backfill


logger = logging.getLogger(__name__)


async def backfill_loop():
    while True:
        await backfill()
        await asyncio.sleep(600)

@asynccontextmanager
async def lifespan(app: FastAPI):    
    node.start()
    asyncio.create_task(
        backfill_loop()
    )
    
    yield
    node.stop()

app = FastAPI(
    lifespan=lifespan, 
    title="KOI-net Protocol API",
    version="1.0.0"
)


koi_net_router = APIRouter(
    prefix="/koi-net"
)

@koi_net_router.post(BROADCAST_EVENTS_PATH)
def broadcast_events(req: EventsPayload):
    logger.info(f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)")
    for event in req.events:
        logger.info(f"{event!r}")
        node.processor.handle(event=event, source=KnowledgeSource.External)
    

@koi_net_router.post(POLL_EVENTS_PATH)
def poll_events(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)

@koi_net_router.post(FETCH_RIDS_PATH)
def fetch_rids(req: FetchRids) -> RidsPayload:
    return node.network.response_handler.fetch_rids(req)

@koi_net_router.post(FETCH_MANIFESTS_PATH)
def fetch_manifests(req: FetchManifests) -> ManifestsPayload:
    return node.network.response_handler.fetch_manifests(req)

@koi_net_router.post(FETCH_BUNDLES_PATH)
def fetch_bundles(req: FetchBundles) -> BundlesPayload:
    return node.network.response_handler.fetch_bundles(req)


app.include_router(koi_net_router)
</file>

<file path=".ai/koi-howto.md">
# KOI‑net – Complete Repository Reference

**Repository:** `blockscience/koi-net`

---

## 1. Purpose & Scope

KOI‑net is the reference Python implementation of the **KOI‑net protocol**—a lightweight, RID‑based messaging layer that lets autonomous "knowledge nodes" discover each other, exchange state (RIDs, Manifests, Bundles) and coordinate updates via event broadcasts. It builds directly on [`rid‑lib`](https://github.com/BlockScience/rid-lib) and provides:

- Typed RIDs for nodes (`KoiNetNode`) & edges (`KoiNetEdge`)
- Five JSON endpoints for event & state transfer
- A pluggable knowledge‑processing pipeline with decorator‑driven handlers
- Local cache & P2P fetch helpers

---

## 2. High‑Level Architecture

```text
src/koi_net/
├── core.py          ⟶ NodeInterface façade (cache, network, processor)
├── config.py        ⟶ YAML / .env driven runtime config
├── identity.py      ⟶ Generates & stores node RID / profile / bundle
├── network/         ⟶ P2P comms (graph view + HTTP helpers)
│   ├── graph.py     ⟶ NetworkX DG wrapper (nodes, edges, profiles)
│   ├── interface.py ⟶ Event queues & high‑level orchestration
│   ├── request_handler.py  ⟶ HTTP POST client (FastAPI‑agnostic)
│   └── response_handler.py ⟶ Generate payloads for incoming requests
├── processor/       ⟶ Knowledge pipeline (RID→Manifest→Bundle→Network→Final)
│   ├── interface.py ⟶ Queue + worker thread + pipeline runner
│   ├── handler.py   ⟶ `KnowledgeHandler` decorator + `STOP_CHAIN` logic
│   ├── knowledge_object.py ⟶ Normalised container used in pipeline
│   └── default_handlers.py ⟶ 4 default handlers (RID, Manifest, Bundle, Network)
├── protocol/        ⟶ Pure data layer (pydantic models & constants)
│   ├── api_models.py  ⟶ Request / response schemas
│   ├── consts.py      ⟶ API paths
│   ├── node.py        ⟶ `NodeProfile`, `NodeType`, `NodeProvides`
│   ├── edge.py        ⟶ `EdgeProfile`, `EdgeType`, `EdgeStatus`
│   ├── event.py       ⟶ `Event`, `EventType`
│   └── helpers.py     ⟶ `generate_edge_bundle(...)`
└── __init__.py       ⟶ Re‑exports `NodeInterface`
```

### Supporting files

- **`koi-net-protocol-openapi.json`** – generated OpenAPI 3.1 spec for FastAPI servers.
- **Examples folder** – runnable reference nodes (`basic_coordinator_node.py`, partial/full templates).
- **`pyproject.toml` + `requirements.txt`** – package metadata & deps.
- **GitHub workflow** – auto‑publish to PyPI + Sigstore signing.

---

## 3. Installation Matrix

| Use‑case           | Command                         |
| ------------------ | ------------------------------- |
| Library only       | `pip install koi-net`           |
| Examples (FastAPI) | `pip install koi-net[examples]` |
| Dev / release      | `pip install -e .[dev]`         |

Python ≥ 3.10 is required; `rid‑lib >= 3.2.1`, `networkx`, `httpx`, `pydantic` are core deps.

---

## 4. Core Concepts & Data Types

| Concept               | RID Type / Model  | Stored Where                  | Notes                                                     |
| --------------------- | ----------------- | ----------------------------- | --------------------------------------------------------- |
| **Node**              | `KoiNetNode`      | Local cache ➝ bundle(profile) | Contains `NodeProfile` (base‑URL, provides list, type)    |
| **Edge**              | `KoiNetEdge`      | Bundle(`EdgeProfile`)         | Directed; status `PROPOSED/APPROVED`; type `WEBHOOK/POLL` |
| **Event**             | pydantic `Event`  | Transient (queues)            | Types: `NEW`, `UPDATE`, `FORGET` (“FUN”)                  |
| **Manifest / Bundle** | `rid-lib` objects | Cache directories             | Hash + timestamp integrity                                |

## http://0.0.0.0:8080

## 5. Configuration (`koi_net.config`)

```yaml
server:
  host: 127.0.0.1
  port: 8000
  path: /koi-net
koi_net:
  node_name: mynode
  node_profile:
    node_type: FULL|PARTIAL
    provides:
      event: [] # RID types broadcasted
      state: [] # RID types served via fetch*
  cache_directory_path: .rid_cache
  event_queues_path: event_queues.json
  first_contact: http://seed-node/koi-net
```

`Config.load_from_yaml()` auto‑fills missing fields & persists defaults.

---

## 6. Node Lifecycle (`NodeInterface`)

1. **Init** – wires `Cache`, `NodeIdentity`, `NetworkInterface`, `ProcessorInterface`.
2. **start()**

   - spin worker thread (if enabled)
   - load persisted event queues
   - regen in‑memory graph
   - enqueue own bundle (`NEW`)
   - optional handshake with `first_contact`.

3. **stop()** – flush pipeline, persist queues.

---

## 7. Processor Pipeline (`processor.interface`)

```
RID ⇒ Manifest ⇒ Bundle ⇒ [cache write/delete] ⇒ Network ⇒ Final
```

Each stage triggers a **handler chain** (ordered list). Handlers are `KnowledgeHandler` objects created via decorator:

```python
@node.processor.register_handler(HandlerType.Bundle)
def my_logic(proc, kobj):
    ...
```

Return contract:

- `None` – pass unchanged kobj to next handler.
- Modified `KnowledgeObject` – continue with new kobj.
- `STOP_CHAIN` – abort remaining handlers + downstream stages.

### Default handlers (summary)

| Stage    | Function                      | Behaviour                                                                 |
| -------- | ----------------------------- | ------------------------------------------------------------------------- |
| RID      | `basic_rid_handler`           | Ignore external events claiming to alter _self_; allow `FORGET` if cached |
| Manifest | `basic_manifest_handler`      | De‑dup identical hash or older timestamp; label `NEW/UPDATE`              |
| Bundle   | `edge_negotiation_handler`    | Auto‑approve/propose edges, fallback to `FORGET` on invalid request       |
| Network  | `basic_network_output_filter` | Route events only to subscribers + affected peers                         |

---

## 8. Networking Layer

### Event Queues (`NetworkInterface`)

- **poll_event_queue** – outbound → neighbours that _poll_.
- **webhook_event_queue** – outbound → neighbours that accept webhooks.

> Flush → `request_handler.broadcast_events(...)` (webhook) or returned via `/events/poll`.

### Request/Response Handlers

All five protocol calls are thin wrappers around `httpx.post`, typed with pydantic models:

```python
payload = node.network.request_handler.fetch_bundles(node_rid, rids=[some_rid])
```

---

## 9. Protocol Endpoints (FastAPI‑style)

| Path                | Method | Purpose                      | Model                               |
| ------------------- | ------ | ---------------------------- | ----------------------------------- |
| `/events/broadcast` | POST   | Push events to **full** node | `EventsPayload`                     |
| `/events/poll`      | POST   | Pull events from node        | `PollEvents → EventsPayload`        |
| `/bundles/fetch`    | POST   | Bulk bundles                 | `FetchBundles → BundlesPayload`     |
| `/manifests/fetch`  | POST   | Bulk manifests               | `FetchManifests → ManifestsPayload` |
| `/rids/fetch`       | POST   | List cached RIDs             | `FetchRids → RidsPayload`           |

OpenAPI spec (`koi-net-protocol-openapi.json`) is auto‑generated from `basic_coordinator_node.py`.

---

## 10. Example Scripts

- **`examples/basic_coordinator_node.py`** (full node + custom handshake handler)
  – Rich‑logged FastAPI server on :8000; auto‑approves webhook edges.
- **`examples/basic_partial_node.py`** – minimal polling node.
- **`examples/full_node_template.py` / `partial_node_template.py`** – copy‑paste boilerplates.

---

## 11. Extending KOI‑net

1. **Custom RID types** – add in `rid-lib`; advertise via `NodeProfile.provides`.
2. **New pipeline logic** – register additional handlers or disable defaults by supplying your own `handlers=[...]` list to `NodeInterface`.
3. **Alternate transport** – swap out `RequestHandler.make_request` to use another client (e.g. `aiohttp`).
4. **Edge strategies** – override `edge_negotiation_handler` to implement trust/rate‑limit logic.

---

## 12. Development & Release

```bash
# clone & dev install
git clone https://github.com/BlockScience/koi-net
cd koi-net && python -m venv venv && source venv/bin/activate
pip install -e .[dev]

# run tests (none yet) / generate docs
python -m build            # wheel + sdist
# tags vX.Y.Z trigger GitHub Action → PyPI publish + Sigstore signing
```

---

## 13. Key Classes Quick‑Ref

| Class                | File                            | Core Methods & Properties                                                       |
| -------------------- | ------------------------------- | ------------------------------------------------------------------------------- |
| `NodeInterface`      | `core.py`                       | `start()`, `stop()`, `.cache`, `.network`, `.processor`                         |
| `NetworkInterface`   | `network/interface.py`          | `push_event_to`, `poll_neighbors`, `fetch_remote_bundle`, `flush_webhook_queue` |
| `ProcessorInterface` | `processor/interface.py`        | `handle`, `flush_kobj_queue`, `register_handler`, `call_handler_chain`          |
| `KnowledgeObject`    | `processor/knowledge_object.py` | `.from_*` ctors, `.bundle`, `.normalized_event`                                 |
| `KnowledgeHandler`   | `processor/handler.py`          | `.create(...)` decorator constants `STOP_CHAIN`                                 |
| `EdgeProfile`        | `protocol/edge.py`              | `source`, `target`, `edge_type`, `status`, `rid_types`                          |

---

## 14. Glossary

- **RID** – Reference Identifier (strongly‑typed URI from `rid-lib`).
- **Bundle** – Manifest + optional contents (cached state).
- **Event** – FUN message (`NEW`, `UPDATE`, `FORGET`) carrying RID context.
- **Full node** – Runs API, receives webhooks, serves state.
- **Partial node** – Client‑only; polls peers but cannot serve state.
- **Edge negotiation** – Two‑step proposal (`PROPOSED` ➝ `APPROVED`) creating directional relation & broadcast method (WEBHOOK/POLL).
</file>

<file path=".ai/Processor.md">
# Processor Design Specifications

This document outlines the design for Processor A (Repo Indexer) and Processor B (Note Indexer) as described in the PRD.

## ProcessorA – Repo Indexer

### 1. Name & Purpose

**ProcessorA (Repo Indexer):** Subscribes to GitHub sensor manifests, optionally dereferences commit/PR bundles, and maintains a local searchable index via a simple API endpoint.

### 2. KOI-Net Integration

- **NodeInterface:** Initializes a `koi_net.NodeInterface` instance, likely configured as `NodeType.FULL`.
- **Endpoints Exposed:**
  - Implements standard KOI endpoints (`/events/*`, `/edges/*`, `/bundles/*`, `/rids/*`, `/manifests/*`) via `NodeInterface`.
  - Exposes a custom FastAPI endpoint: `GET /search` for querying the local index.
  - Exposes a `/health` endpoint.
- **Edge Management:**
  - On startup, proposes an `EdgeType.WEBHOOK` edge to the Coordinator specified in configuration (`first_contact`).
  - Listens for `KoiNetNode` events to discover the `GitHubSensorNode`.
  - Upon discovering the sensor, proposes a direct `EdgeType.WEBHOOK` edge to it, subscribing to `CommitManifestRID` and potentially `PullRequestManifestRID` events.
- **Event Consumption:** Receives manifests for `CommitManifestRID` (and potentially `PullRequestManifestRID`) via the `/events/broadcast` endpoint from the connected `GitHubSensorNode`.
- **Bundle Fetching:** May optionally use `node.network.request_handler.fetch_bundles` or trigger internal handling via `node.processor.handle(rid=...)` to dereference specific commit/PR RIDs based on manifest content or search needs.

### 3. RID-Lib Integration

- **RID Consumption:** Primarily consumes `CommitManifestRID` (and potentially `PullRequestManifestRID`) via `koi_net.Event` objects.
- **Manifest Handling:** Uses `kobj.manifest` within handlers to inspect metadata (e.g., commit message, author) before deciding whether to dereference.
- **Bundle Handling:** When dereferencing, uses `kobj.bundle.validate_contents()` or accesses `kobj.contents` (dictionary) within bundle handlers to process commit data (SHA, message, author, files changed, etc.).
- **Cache Interaction:** Relies on the underlying `NodeInterface` cache (`CACHE_DIR`) to store fetched bundles. The processor logic itself might implement an additional layer for its searchable index (e.g., in-memory dict, simple file, SQLite).
- **RID Emission:** **Does not emit any new RIDs** as per the PRD simplification note.

### 4. Inputs & Outputs

- **Inputs:**
  - `koi_net.Event` objects containing manifests for:
    - `github_sensor_node.types.GithubCommit` (via `CommitManifestRID`)
    - _(Potentially)_ A corresponding `PullRequestManifestRID` (definition needed if used).
  - HTTP GET requests to its custom `/search` endpoint.
- **Outputs:**
  - HTTP JSON responses from its custom `/search` endpoint (structure TBD, e.g., `{"results": [{"rid": "...", "match_context": "..."}]}`).
  - Standard KOI protocol responses from its `NodeInterface` endpoints.
  - **No new `koi_net.Event` objects or RIDs are broadcast.**

### 5. Internal Flow

1.  Initialize `NodeInterface` with profile specifying consumption of `GithubCommit` RIDs.
2.  Register event handlers (`@node.processor.register_handler`) for:
    - `HandlerType.Network` for `KoiNetNode` (to discover Coordinator and Sensor).
    - `HandlerType.Manifest` for `CommitManifestRID` (and potentially `PullRequestManifestRID`).
    - `HandlerType.Bundle` for `CommitManifestRID` (and potentially `PullRequestManifestRID`) if dereferencing.
3.  Start the `NodeInterface` (`node.start()`). This automatically attempts connection to the `first_contact` Coordinator.
4.  **Coordinator Handshake:** Handler proposes edge back to Coordinator upon discovery.
5.  **Sensor Discovery & Handshake:** Handler discovers `GitHubSensorNode` (filtering by profile/provided RIDs), proposes edge, and accepts incoming edge proposal from the sensor.
6.  **Manifest Processing:** The manifest handler receives `CommitManifestRID` events.
    - Filters events based on relevance (e.g., specific branches - though not specified in PRD).
    - Decides whether to dereference the bundle based on manifest data (e.g., keywords in message) or if indexing requires full content.
    - If dereferencing, calls `node.processor.handle(rid=manifest.rid, source=KnowledgeSource.External)` to trigger bundle fetch and processing by the bundle handler.
7.  **Bundle Processing (if dereferenced):** The bundle handler receives the commit contents.
    - Extracts relevant data (SHA, message, author, timestamp, etc.).
    - Updates the internal searchable index (implementation specific - e.g., add keywords/SHA to an in-memory dictionary or a simple file).
8.  **Search API:** A separate FastAPI route (`/search`) handles incoming GET requests.
    - Parses the query parameter (`q`).
    - Searches the internal index based on the query (e.g., lookup SHA, search keywords).
    - Returns matching results as JSON.

### 6. Configuration

- **YAML (`config/processor-a.yaml`):**
  - `runtime.base_url`: (String, Required) Public URL of this processor node (e.g., `http://processor-a:8011/koi-net`).
  - `runtime.host`: (String, Default: "0.0.0.0") Host to bind the FastAPI server.
  - `runtime.port`: (Integer, Default: 8011) Port to bind the FastAPI server.
  - `runtime.log_level`: (String, Default: "INFO") Logging level.
  - `runtime.cache_dir`: (String, Required) Path to the shared RID cache volume (e.g., `/data/cache`).
  - `edges.coordinator_url`: (String, Required) URL of the Coordinator node (e.g., `http://coordinator:8080/koi-net`).
  - _(Optional)_ `processor_a.github_sensor_rid`: (String, Optional) Specific RID of the GitHub sensor to connect to (if discovery is bypassed).
- **Environment (`config/global.env`):**
  - No specific ENV variables required by Processor A itself identified yet, but inherits shared settings if needed.

### 7. Sample Snippets

```python
# processor_a/core.py (Example Node Setup)
import os
import logging
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides
# Assuming GithubCommit is defined elsewhere accessible to the processor
from github_sensor_node.types import GithubCommit # May need to be in shared lib
# Import config values
from .config import BASE_URL, COORDINATOR_URL, CACHE_DIR

logger = logging.getLogger(__name__)
name = "processor-a"
identity_dir = f".koi/{name}"
os.makedirs(identity_dir, exist_ok=True)

node = NodeInterface(
    name=name,
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides( # Processor A provides no new RIDs
             event=[], state=[]
        )
        # Consumes defined implicitly by handlers or explicitly if needed
    ),
    use_kobj_processor_thread=True,
    first_contact=COORDINATOR_URL,
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,
)

# ---

# processor_a/handlers.py (Example Manifest Handler)
from .core import node
from koi_net.processor import ProcessorInterface, HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from github_sensor_node.types import GithubCommit # Assumed shared/accessible

logger = logging.getLogger(__name__)

# In-memory index example
search_index = {} # {sha: commit_message, keyword: [sha1, sha2]}

@node.processor.register_handler(HandlerType.Manifest, rid_types=[GithubCommit])
def handle_commit_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    manifest = kobj.manifest
    rid: GithubCommit = manifest.rid
    logger.info(f"Received manifest for commit: {rid.reference}")

    # Example: Always dereference for indexing message content
    logger.debug(f"Requesting bundle for {rid} for indexing.")
    processor.handle(rid=rid, source=KnowledgeSource.External) # Trigger bundle fetch

# ---

# processor_a/handlers.py (Example Bundle Handler)
@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
     if not kobj.contents:
         logger.warning(f"Bundle for {kobj.rid} has no contents.")
         return

     rid: GithubCommit = kobj.rid
     contents = kobj.contents
     logger.info(f"Processing bundle for commit: {rid.sha[:7]}")

     # Update simple index (example)
     message = contents.get("message", "")
     search_index[rid.sha] = message # Index by SHA
     # Simple keyword indexing
     for keyword in message.lower().split():
         if len(keyword) > 3: # Basic filtering
             if keyword not in search_index:
                 search_index[keyword] = []
             if rid.sha not in search_index[keyword]:
                  search_index[keyword].append(rid.sha)

# ---

# processor_a/server.py (Example Search Endpoint)
from fastapi import FastAPI, HTTPException
from .core import node # Assuming node and app setup like template
from .handlers import search_index # Import the example index

# app = FastAPI(...) # Assuming FastAPI app setup elsewhere

@app.get("/search")
async def search_commits(q: str):
    logger.info(f"Received search request: q='{q}'")
    results = []
    query = q.lower()

    # Simple search logic (example)
    if len(query) >= 7 and query in search_index: # Check if it's a SHA
        results.append({"rid": f"orn:github.commit:{query}", "match_context": search_index.get(query, "")})
    elif query in search_index and isinstance(search_index[query], list): # Check keywords
         for sha in search_index[query]:
             # Need owner/repo - this index example is too simple
             # Need a better index like {sha: {"rid": rid_obj, "message": msg}}
             # For now, just return SHA
             results.append({"sha": sha, "match_context": "Keyword match"})
    else:
         # More sophisticated search could go here
         pass

    if not results:
        logger.info(f"No results found for query: '{q}'")
        # Return 404 or empty list? Let's use empty list.
        # raise HTTPException(status_code=404, detail="No matching commits found")
    return {"query": q, "results": results}
```

---

## ProcessorB – Note Indexer

### 1. Name & Purpose

**ProcessorB (Note Indexer):** Subscribes to HackMD sensor manifests, dereferences note bundles, and maintains a local searchable index of Markdown content via a simple API endpoint.

### 2. KOI-Net Integration

- **NodeInterface:** Initializes a `koi_net.NodeInterface` instance, likely configured as `NodeType.FULL`.
- **Endpoints Exposed:**
  - Implements standard KOI endpoints via `NodeInterface`.
  - Exposes a custom FastAPI endpoint: `GET /search` for querying the local index.
  - Exposes a `/health` endpoint.
- **Edge Management:**
  - On startup, proposes an `EdgeType.WEBHOOK` edge to the Coordinator.
  - Listens for `KoiNetNode` events to discover the `HackMDSensorNode`.
  - Upon discovering the sensor, proposes a direct `EdgeType.WEBHOOK` edge to it, subscribing to `HackMDNote` events.
  - _(Optional/Future based on PRD conflict):_ Discovers `ProcessorA` and negotiates a direct `EdgeType.KNOWLEDGE` edge to potentially exchange derived RIDs (like `RiskScoreRID` -> `ActionItemRID`).
- **Event Consumption:** Receives manifests for `HackMDNote` via `/events/broadcast` from the connected `HackMDSensorNode`.
- **Bundle Fetching:** Always uses `node.processor.handle(rid=...)` or `node.network.request_handler.fetch_bundles` upon receiving a manifest to dereference the full note content for indexing.

### 3. RID-Lib Integration

- **RID Consumption:** Consumes `HackMDNote` RIDs (defined in `hackmd_sensor_node/rid_types.py`) via `koi_net.Event` objects.
- **Manifest Handling:** Uses `kobj.manifest` primarily to trigger bundle dereferencing. May check timestamp against local state if implementing update logic vs. re-indexing all the time.
- **Bundle Handling:** Uses `kobj.bundle.validate_contents()` or accesses `kobj.contents` to get the full Markdown content, title, tags, and metadata (`lastChangedAt`, etc.).
- **Cache Interaction:** Relies on the `NodeInterface` cache (`CACHE_DIR`) for bundle storage. Implements its own searchable index (e.g., in-memory dict mapping tags/title words to note IDs).
- **RID Emission:** **Does not emit any new RIDs** as per the PRD simplification note. _(Note: This contradicts the PRD Table 4 mention of emitting `ActionItemRID` linked to `RiskScoreRID`. The simplification note takes precedence for this initial design)._

### 4. Inputs & Outputs

- **Inputs:**
  - `koi_net.Event` objects containing manifests for `hackmd_sensor_node.rid_types.HackMDNote`.
  - HTTP GET requests to its custom `/search` endpoint.
- **Outputs:**
  - HTTP JSON responses from its custom `/search` endpoint (structure TBD, e.g., `{"results": [{"rid": "...", "title": "...", "tags": [...]}]}`).
  - Standard KOI protocol responses from its `NodeInterface` endpoints.
  - **No new `koi_net.Event` objects or RIDs are broadcast.**

### 5. Internal Flow

1.  Initialize `NodeInterface` with profile specifying consumption of `HackMDNote` RIDs.
2.  Register event handlers (`@node.processor.register_handler`) for:
    - `HandlerType.Network` for `KoiNetNode` (to discover Coordinator and Sensor, potentially Processor A).
    - `HandlerType.Manifest` for `HackMDNote`.
    - `HandlerType.Bundle` for `HackMDNote`.
3.  Start the `NodeInterface` (`node.start()`).
4.  **Coordinator Handshake:** Handler proposes edge back to Coordinator.
5.  **Sensor Discovery & Handshake:** Handler discovers `HackMDSensorNode`, proposes edge, and accepts incoming edge proposal.
6.  **Manifest Processing:** The manifest handler receives `HackMDNote` events.
    - Immediately triggers bundle dereferencing via `node.processor.handle(rid=manifest.rid, source=KnowledgeSource.External)`.
7.  **Bundle Processing:** The bundle handler receives the full note contents.
    - Extracts `title`, `tags`, `content`, `lastChangedAt` from `kobj.contents`.
    - _(Optional)_ Compare `lastChangedAt` with locally stored timestamp for this note ID to avoid re-processing unchanged notes.
    - Parses Markdown content (implementation specific - e.g., simple regex for tags/titles, or more complex parsing for action items if that feature were included).
    - Updates the internal searchable index (e.g., add note ID to lists associated with tags, index words in the title).
    - _(Optional/Future)_ If knowledge edge with Processor A exists, potentially query Processor A based on parsed content.
8.  **Search API:** A separate FastAPI route (`/search`) handles incoming GET requests.
    - Parses the query parameter (`q`).
    - Searches the internal index based on the query (e.g., lookup notes by tag, search title keywords).
    - Returns matching results (e.g., list of note RIDs and titles) as JSON.

### 6. Configuration

- **YAML (`config/processor-b.yaml`):**
  - `runtime.base_url`: (String, Required) Public URL of this processor node (e.g., `http://processor-b:8012/koi-net`).
  - `runtime.host`: (String, Default: "0.0.0.0") Host to bind the FastAPI server.
  - `runtime.port`: (Integer, Default: 8012) Port to bind the FastAPI server.
  - `runtime.log_level`: (String, Default: "INFO") Logging level.
  - `runtime.cache_dir`: (String, Required) Path to the shared RID cache volume (e.g., `/data/cache`).
  - `edges.coordinator_url`: (String, Required) URL of the Coordinator node (e.g., `http://coordinator:8080/koi-net`).
  - _(Optional)_ `processor_b.hackmd_sensor_rid`: (String, Optional) Specific RID of the HackMD sensor to connect to.
- **Environment (`config/global.env`):**
  - No specific ENV variables required by Processor B itself identified yet.

### 7. Sample Snippets

```python
# processor_b/core.py (Example Node Setup)
import os
import logging
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides
# Import the RID type
from hackmd_sensor_node.rid_types import HackMDNote
# Import config values
from .config import BASE_URL, COORDINATOR_URL, CACHE_DIR

logger = logging.getLogger(__name__)
name = "processor-b"
identity_dir = f".koi/{name}"
os.makedirs(identity_dir, exist_ok=True)

node = NodeInterface(
    name=name,
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides( # Processor B provides no new RIDs per simplification
             event=[], state=[]
        )
    ),
    use_kobj_processor_thread=True,
    first_contact=COORDINATOR_URL,
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,
)

# ---

# processor_b/handlers.py (Example Manifest Handler)
from .core import node
from koi_net.processor import ProcessorInterface, HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from hackmd_sensor_node.rid_types import HackMDNote

logger = logging.getLogger(__name__)

# In-memory index example
# { "tag": [rid1, rid2], "title_word": [rid1, rid3], ... }
# { rid_str: {"title": title, "tags": tags, "lastChangedAt": ts}}
search_index = {}
note_metadata = {}

@node.processor.register_handler(HandlerType.Manifest, rid_types=[HackMDNote])
def handle_note_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    manifest = kobj.manifest
    rid: HackMDNote = manifest.rid
    logger.info(f"Received manifest for HackMD note: {rid.reference}")

    # Always dereference notes for indexing
    logger.debug(f"Requesting bundle for {rid} for indexing.")
    processor.handle(rid=rid, source=KnowledgeSource.External) # Trigger bundle fetch

# ---

# processor_b/handlers.py (Example Bundle Handler)
# (Requires markdown parsing library, e.g., markdown-it-py or similar)
# import re # Example for simple parsing

@node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
def handle_note_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
     if not kobj.contents:
         logger.warning(f"Bundle for {kobj.rid} has no contents.")
         return

     rid: HackMDNote = kobj.rid
     contents = kobj.contents
     rid_str = str(rid) # Use string representation for dict keys

     logger.info(f"Processing bundle for note: {rid.note_id} - '{contents.get('title', 'N/A')}'")

     # Check timestamp to avoid re-indexing if not changed
     last_changed = contents.get("lastChangedAt")
     if rid_str in note_metadata and last_changed == note_metadata[rid_str].get("lastChangedAt"):
         logger.debug(f"Note {rid.note_id} has not changed since last index. Skipping.")
         return

     # --- Update Metadata Cache ---
     note_metadata[rid_str] = {
         "title": contents.get("title", ""),
         "tags": contents.get("tags", []),
         "lastChangedAt": last_changed
     }

     # --- Update Search Index (Example: Tags and Title words) ---
     # Clear old index entries for this note first (important for updates)
     for key, rid_list in list(search_index.items()):
         if rid_str in rid_list:
             search_index[key].remove(rid_str)
             if not search_index[key]: # Remove key if list becomes empty
                 del search_index[key]

     # Index by tags
     tags = contents.get("tags", [])
     for tag in tags:
         tag_key = tag.lower()
         if tag_key not in search_index:
             search_index[tag_key] = []
         if rid_str not in search_index[tag_key]:
              search_index[tag_key].append(rid_str)

     # Index by title words
     title = contents.get("title", "")
     for word in title.lower().split():
         if len(word) > 2: # Basic filtering
             if word not in search_index:
                 search_index[word] = []
             if rid_str not in search_index[word]:
                 search_index[word].append(rid_str)

     # Index by note ID itself
     search_index[rid.note_id] = [rid_str]


     # Note: Markdown content parsing is omitted for brevity
     # md_content = contents.get("content", "")
     # Parse md_content here...

     logger.debug(f"Updated index for note {rid.note_id}")

# ---

# processor_b/server.py (Example Search Endpoint)
from fastapi import FastAPI, HTTPException
from .core import node # Assuming node and app setup like template
from .handlers import search_index, note_metadata # Import example index/metadata

# app = FastAPI(...) # Assuming FastAPI app setup elsewhere

@app.get("/search")
async def search_notes(q: str):
    logger.info(f"Received note search request: q='{q}'")
    results_rids = set()
    query = q.lower()

    # Simple search logic (example: match query as tag or title word or note_id)
    if query in search_index:
        results_rids.update(search_index[query])

    # Format results
    results = []
    for rid_str in results_rids:
        meta = note_metadata.get(rid_str, {})
        results.append({
            "rid": rid_str,
            "title": meta.get("title", "N/A"),
            "tags": meta.get("tags", [])
        })

    if not results:
        logger.info(f"No results found for query: '{q}'")

    return {"query": q, "results": results}



```
</file>

<file path="config/docker/github-sensor.yaml">
edges:
  coordinator_url: http://coordinator:8080/koi-net
runtime:
  base_url: http://github-sensor:8001/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8001
  state_file: /data/cache/github_state.json
sensor:
  kind: github
  mode: webhook
  poll_interval: 60
  repos:
    - BlockScience/koi
webhook:
  secret_env_var: GITHUB_WEBHOOK_SECRET
</file>

<file path="config/docker/global.env.example">
GITHUB_TOKEN=<github-token>
HACKMD_TOKEN=<hackmd-token>
GITHUB_WEBHOOK_SECRET=<github-webhook-secret>
</file>

<file path="config/docker/hackmd-sensor.yaml">
api:
  token_env_var: HACKMD_TOKEN
edges:
  coordinator_url: http://coordinator:8080/koi-net
runtime:
  base_url: http://hackmd-sensor:8002/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8002
sensor:
  kind: hackmd
  poll_interval: 300
  target_note_ids:
  - C1xso4C8SH-ZzDaloTq4Uw
  team_path: blockscience
</file>

<file path="config/docker/processor-a.yaml">
# Processor A Config (Docker)
runtime:
  base_url: http://processor-a:8011/koi-net
  cache_dir: /data/cache # Shared cache volume
  host: 0.0.0.0
  log_level: INFO
  port: 8011

edges:
  coordinator_url: http://coordinator:8080/koi-net
# Optional: Specify exact sensor if discovery is problematic
# processor_a:
#   github_sensor_rid: "orn:koi.node:<uuid_of_github_sensor>"
</file>

<file path="config/docker/processor-b.yaml">
# Processor B Config (Docker)
runtime:
  base_url: http://processor-b:8012/koi-net
  cache_dir: /data/cache # Shared cache volume
  host: 0.0.0.0
  log_level: INFO
  port: 8012

edges:
  coordinator_url: http://coordinator:8080/koi-net
# Optional: Specify exact sensor if discovery is problematic
# processor_b:
#   hackmd_sensor_rid: "orn:koi.node:<uuid_of_hackmd_sensor>"
</file>

<file path="config/local/github-sensor.yaml">
edges:
  coordinator_url: http://0.0.0.0:8080/koi-net
runtime:
  base_url: http://0.0.0.0:8001/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8001
  state_file: /data/cache/github_state.json
sensor:
  kind: github
  mode: webhook
  poll_interval: 60
  repos:
    - blockscience/target-repo-1
    - blockscience/target-repo-2
webhook:
  secret_env_var: GITHUB_WEBHOOK_SECRET
</file>

<file path="config/local/global.env.example">
GITHUB_TOKEN=<github-token>
HACKMD_TOKEN=<hackmd-token>
GITHUB_WEBHOOK_SECRET=<github-webhook-secret>
</file>

<file path="config/local/hackmd-sensor.yaml">
api:
  token_env_var: HACKMD_TOKEN
edges:
  coordinator_url: http://0.0.0.0:8080/koi-net
runtime:
  base_url: http://0.0.0.0:8002/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8002
sensor:
  kind: hackmd
  poll_interval: 300
  target_note_ids:
    - C1xso4C8SH-ZzDaloTq4Uw
  team_path: blockscience
</file>

<file path="config/local/processor-a.yaml">
# Processor A Config (Local)
runtime:
  base_url: http://127.0.0.1:8011/koi-net # Use localhost for local dev
  cache_dir: ./.koi/processor-a/cache # Local cache path
  host: 127.0.0.1 # Bind to localhost locally
  log_level: DEBUG # More verbose locally
  port: 8011

edges:
  coordinator_url: http://127.0.0.1:8080/koi-net # Coordinator on localhost

# processor_a:
#   github_sensor_rid: "..." # Optional override for local testing
</file>

<file path="config/local/processor-b.yaml">
# Processor B Config (Local)
runtime:
  base_url: http://127.0.0.1:8012/koi-net # Use localhost for local dev
  cache_dir: ./.koi/processor-b/cache # Local cache path
  host: 127.0.0.1 # Bind to localhost locally
  log_level: DEBUG # More verbose locally
  port: 8012

edges:
  coordinator_url: http://127.0.0.1:8080/koi-net # Coordinator on localhost

# processor_b:
#   hackmd_sensor_rid: "..." # Optional override for local testing
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path
from .config_loader import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/coordinator")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "coordinator-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
# logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
# logging.getLogger("anyio").setLevel(logging.WARNING)

logger.info(
    f"Logging configured (Level: {LOG_LEVEL}). Console via Rich, File: {log_file_path}"
)
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/__main__.py">
import uvicorn

# Remove import from old config
# from .config import PORT

# Port is now handled by Docker CMD and config_loader if needed elsewhere
uvicorn.run(
    "coordinator_node.server:app",
    host="0.0.0.0",
    port=8080,
    log_config=None,
    reload=True,
)
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/server.py">
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node


logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    node.start()
    yield
    node.stop()


app = FastAPI(
    lifespan=lifespan,
    root_path="/koi-net",
    title="KOI-net Protocol API",
    version="1.0.0",
)


@app.get("/health")
def health_check():
    """Basic health check endpoint."""
    # You could potentially add more checks here later, like checking
    # the status of the `node` object if it provides such a method.
    logger.debug("Health check endpoint hit")
    return {"status": "healthy"}


@app.post(BROADCAST_EVENTS_PATH)
def broadcast_events(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)


@app.post(POLL_EVENTS_PATH)
def poll_events(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@app.post(FETCH_RIDS_PATH)
def fetch_rids(req: FetchRids) -> RidsPayload:
    return node.network.response_handler.fetch_rids(req)


@app.post(FETCH_MANIFESTS_PATH)
def fetch_manifests(req: FetchManifests) -> ManifestsPayload:
    return node.network.response_handler.fetch_manifests(req)


@app.post(FETCH_BUNDLES_PATH)
def fetch_bundles(req: FetchBundles) -> BundlesPayload:
    return node.network.response_handler.fetch_bundles(req)
</file>

<file path="nodes/koi-net-coordinator-node/.gitignore">
rid_cache
identity.json
events_queue.json
venv
.env
*.json
__pycache__
</file>

<file path="nodes/koi-net-coordinator-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy pyproject and perform editable installation
COPY pyproject.toml /app/

# 3) install curl for healthcheck
RUN apt-get update && apt-get install -y curl

# 4) install dependencies using UV and pyproject.toml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 5) copy in code
COPY . /app/

# 6) expose port and set environment
EXPOSE 8080

# 7) add healthcheck using the main app's endpoint
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8080/koi-net/health || exit 1

# 8) start the main service using shell form for ENV var substitution
CMD ["uvicorn", "coordinator_node.server:app", "--host", "0.0.0.0", "--port", "8080"]
</file>

<file path="nodes/koi-net-coordinator-node/koi-net-coordinator-node.service">
[Unit]
Description=KOI-net Coordinator Node Service
After=network.target

[Service]
WorkingDirectory=/home/dev/koi-net-coordinator-node
ExecStart=/home/dev/koi-net-coordinator-node/venv/bin/python3 -m coordinator_node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="nodes/koi-net-coordinator-node/LICENSE">
MIT License

Copyright (c) 2025 BlockScience

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="nodes/koi-net-coordinator-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "coordinator-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.1",
    "koi-net==1.0.0b12",
    "rich",
    "ruamel.yaml",
    "python-dotenv"
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="nodes/koi-net-coordinator-node/README.md">
# koi-net-coordinator-node
Coordinator node implementation for BlockScience's KOI-net
</file>

<file path="nodes/koi-net-coordinator-node/requirements.txt">
koi-net==1.0.0b12
rid-lib>=3.2.2
fastapi
uvicorn
rich
ruamel.yaml
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/handlers/github.py">
import logging
from ..core import node
from ..types import GithubCommit

from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of the coordinator node (or other nodes providing KoiNetNode events).

    On discovering a NEW coordinator, proposes a WEBHOOK edge for bidirectional
    communication and requests a list of other known nodes (sync).
    (Based on refactor.md example)
    """
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if KoiNetNode not in profile.provides.event:
            logger.debug(
                f"Node {kobj.rid} does not provide KoiNetNode events. Ignoring."
            )
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    logger.info(
        f"Identified potential coordinator/peer: {kobj.rid}; proposing WEBHOOK edge"
    )
    try:

        edge_bundle = generate_edge_bundle(
            source=kobj.rid,
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode],
        )
        processor.handle(bundle=edge_bundle)
    except Exception as e:
        logger.error(
            f"Failed to generate or handle WEBHOOK edge bundle for {kobj.rid}: {e}",
            exc_info=True,
        )

    logger.info(f"Syncing network nodes from {kobj.rid}")
    try:
        payload = processor.network.request_handler.fetch_rids(
            kobj.rid, rid_types=[KoiNetNode]
        )
        if not payload or not payload.rids:
            logger.warning(f"Received empty RIDs payload from {kobj.rid} during sync.")
            return

        logger.debug(f"Received {len(payload.rids)} RIDs from {kobj.rid}")
        for rid in payload.rids:

            if rid == processor.identity.rid or processor.cache.exists(rid):
                continue
            logger.debug(f"Handling discovered RID from sync: {rid}")

            processor.handle(rid=rid, source=KnowledgeSource.External)
    except Exception as e:
        logger.error(f"Failed during network sync with {kobj.rid}: {e}", exc_info=True)


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit(processor: ProcessorInterface, kobj: KnowledgeObject):
    """
    Basic handler for processing GithubCommit bundles.
    Currently just logs information.

    Args:
        bundle: The Bundle object containing the GithubCommit RID and contents.
    """
    try:

        bundle = kobj.bundle
        rid: GithubCommit = bundle.rid
        contents: dict = bundle.contents

        logger.info(
            f"Processing commit: {rid} (Normalized Type: {kobj.normalized_event_type}, Source: {kobj.source})"
        )

        logger.debug(
            f"  Author: {contents.get('author_name')} <{contents.get('author_email')}>"
        )
        logger.debug(
            f"  Message: {contents.get('message', '').splitlines()[0][:80]}..."
        )
        logger.debug(f"  URL: {contents.get('html_url')}")

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Example handler demonstrating processing before cache write."""
    logger.debug(f"Handling GithubCommit bundle PRE-CACHE for {kobj.rid}")

    try:

        if kobj.contents and "message" in kobj.contents:
            logger.info(
                f"Processing commit {kobj.rid.sha[:7]}: {kobj.contents['message'].splitlines()[0]}"
            )

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


logger.info("GithubCommit Bundle handler registered.")
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path

# Import LOG_LEVEL from refactored config
from .config import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/github")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "github-sensor-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
logging.getLogger("github").setLevel(logging.WARNING)  # PyGithub can be verbose
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/__main__.py">
import uvicorn
import logging
from .config import HOST, PORT

logger = logging.getLogger(__name__)

logger.info(f"GitHub sensor node starting on {HOST}:{PORT}")
uvicorn.run(
    "github_sensor_node.server:app",
    host=HOST,
    port=PORT,
    log_config=None,
    reload=True,
)
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/backfill.py">
import logging
from github import Github, GithubException, RateLimitExceededException
from github.Commit import Commit
from rid_lib.ext import Bundle

# Assuming GithubCommit RID type is accessible
from .types import GithubCommit
from .core import node

# Import necessary config values
from .config import GITHUB_TOKEN, MONITORED_REPOS, LAST_PROCESSED_SHA, update_state_file

logger = logging.getLogger(__name__)

# Initialize GitHub client (authenticated if token provided)
# Use the GITHUB_TOKEN loaded from config
github_client = Github(GITHUB_TOKEN) if GITHUB_TOKEN else Github()
logger.info(f"GitHub client initialized. Authenticated: {bool(GITHUB_TOKEN)}")


def perform_backfill():
    """
    One-time startup backfill: fetch all commits since LAST_PROCESSED_SHA
    for each monitored repo, bundle them as NEW, and persist the latest SHA processed.
    Processes commits oldest-to-newest after fetching.
    """
    logger.info("Starting GitHub backfill process...")

    # Load the state dictionary once before the loop
    current_state = (
        LAST_PROCESSED_SHA.copy()
    )  # Use a copy to avoid modifying during iteration if needed
    newest_sha_processed_overall_map = (
        {}
    )  # Track newest SHA per repo processed in this run

    if not MONITORED_REPOS:
        logger.warning(
            "No repositories configured in MONITORED_REPOS. Backfill skipped."
        )
        return

    for repo_full_name in MONITORED_REPOS:
        try:
            owner, repo_name_only = repo_full_name.split("/")
            # Get the last processed SHA for *this specific repository*
            last_sha_for_repo = current_state.get(repo_full_name)
            logger.info(
                f"Backfilling repository: {repo_full_name} since SHA: {last_sha_for_repo or 'beginning'}"
            )

            gh_repo = github_client.get_repo(repo_full_name)
            commits_to_process_buffer: list[Commit] = []

            # Iterate commits newest-first until we find the last processed one
            paginated_commits = gh_repo.get_commits()
            logger.debug(f"Fetching commits for {repo_full_name}...")
            commit_count = 0
            for commit in paginated_commits:
                commit_count += 1
                # Check against the specific SHA for this repo
                if last_sha_for_repo and commit.sha == last_sha_for_repo:
                    logger.info(
                        f"Found last processed SHA {last_sha_for_repo} in {repo_full_name}. Stopping fetch for this repo."
                    )
                    break
                commits_to_process_buffer.append(commit)
                # Safety break for potentially huge repos without a known SHA
                # Adjust limit as needed
                if commit_count % 100 == 0:
                    logger.debug(
                        f"Fetched {commit_count} commits for {repo_full_name} so far..."
                    )
                # if commit_count > 1000:
                #    logger.warning(f"Reached fetch limit (1000) for {repo_full_name}. Consider adjusting.")
                #    break

            logger.info(
                f"Found {len(commits_to_process_buffer)} new commits in {repo_full_name} to backfill."
            )

            # Process commits oldest → newest
            newest_sha_processed_in_repo = None
            for commit in reversed(commits_to_process_buffer):
                try:
                    rid = GithubCommit(owner=owner, repo=repo_name_only, sha=commit.sha)
                    # Extract commit details carefully, handling potential missing attributes
                    author = commit.commit.author
                    committer = commit.commit.committer

                    contents = {
                        "sha": commit.sha,
                        "message": commit.commit.message,
                        "author_name": author.name if author else None,
                        "author_email": author.email if author else None,
                        "author_date": (
                            author.date.isoformat() if author and author.date else None
                        ),
                        "committer_name": committer.name if committer else None,
                        "committer_email": committer.email if committer else None,
                        "committer_date": (
                            committer.date.isoformat()
                            if committer and committer.date
                            else None
                        ),
                        "html_url": commit.html_url,
                        "parents": [
                            p.sha for p in commit.parents
                        ],  # List of parent SHAs
                    }
                    bundle = Bundle.generate(rid=rid, contents=contents)
                    # CORRECT USAGE: 'handle' makes the bundle available locally
                    # in the sensor's cache/event queue for consumers to poll/fetch.
                    # It does NOT push the application-specific GithubCommit bundle directly.
                    logger.debug(
                        f"Making backfill commit bundle {rid} available locally via sensor API."
                    )
                    node.processor.handle(bundle=bundle)

                    # Track the newest SHA processed in this run for this repo
                    newest_sha_processed_in_repo = commit.sha

                except Exception as e:
                    logger.error(
                        f"Error processing commit {commit.sha} in {repo_full_name}: {e}",
                        exc_info=True,
                    )

            # Store the newest SHA processed for this repo during this run
            if newest_sha_processed_in_repo:
                newest_sha_processed_overall_map[repo_full_name] = (
                    newest_sha_processed_in_repo
                )
                logger.debug(
                    f"Newest SHA processed for {repo_full_name} in this run: {newest_sha_processed_in_repo}"
                )

        except RateLimitExceededException:
            logger.error(
                f"GitHub API rate limit exceeded while backfilling {repo_full_name}. Aborting backfill. Try again later or use a GITHUB_TOKEN."
            )
            # Depending on requirements, could wait and retry, but for now, we stop.
            return  # Stop the entire backfill process
        except GithubException as e:
            logger.error(
                f"GitHub API error for repository {repo_full_name}: {e}. Skipping this repo."
            )
            continue  # Skip to the next repository
        except Exception as e:
            logger.error(
                f"Unexpected error backfilling repository {repo_full_name}: {e}",
                exc_info=True,
            )
            continue  # Skip to the next repository

    # After processing all repos, persist the newest SHAs found (if changed)
    updated_count = 0
    for repo_full_name, newest_sha in newest_sha_processed_overall_map.items():
        if newest_sha != current_state.get(repo_full_name):
            update_state_file(
                repo_full_name, newest_sha
            )  # Call function from config.py
            updated_count += 1
        else:
            logger.debug(
                f"No state update needed for {repo_full_name}, newest SHA {newest_sha} is same as stored."
            )

    if updated_count > 0:
        logger.info(
            f"Backfill complete. Updated state for {updated_count} repositories."
        )
    else:
        logger.info(
            f"Backfill complete. No new commits found or state changes required across monitored repositories."
        )


if __name__ == "__main__":
    # Example of how to run backfill directly for testing
    # Requires node to be started if handle() depends on active components
    # In practice, this is called by server.py during startup
    logging.basicConfig(level=logging.INFO)
    logger.info("Running backfill directly for testing...")
    # node.start() # Might be needed depending on node.processor.handle implementation
    perform_backfill()
    # node.stop()
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/loader.py">
from .core import node
from .handlers import github


def register_handlers():
    print("Registering GITHUB handlers...")
    _ = github
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/server.py">
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, Request, Body, Header, HTTPException
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node
from .webhook import router as github_router
from .backfill import perform_backfill
from .loader import register_handlers

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup, backfill, and shutdown."""
    logger.info("Starting FastAPI application lifespan...")
    # Start the KOI-net node
    try:
        register_handlers()
        node.start()
        logger.info("KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        raise RuntimeError("Failed to initialize KOI-net node") from e

    logger.info("Scheduling initial GitHub backfill...")
    backfill_task = asyncio.to_thread(perform_backfill)

    try:
        yield  # Application runs here
    finally:
        logger.info("Shutting down FastAPI application...")
        # Attempt to gracefully cancel the backfill if it's still running
        # This might require more sophisticated task management if backfill is long-running
        # if backfill_task and not backfill_task.done():
        #     try:
        #         backfill_task.cancel()
        #         await backfill_task
        #     except asyncio.CancelledError:
        #         logger.info("Backfill task cancelled.")
        #     except Exception as e:
        #         logger.error(f"Error cancelling backfill task: {e}", exc_info=True)

        try:
            node.stop()
            logger.info("KOI-net node stopped successfully.")
        except Exception as e:
            logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
        logger.info("FastAPI application shutdown complete.")


app = FastAPI(
    title="KOI-net GitHub Sensor Node",
    description="Listens for GitHub webhooks and performs backfill to ingest commit data.",
    version="0.1.0",
    lifespan=lifespan,
)

koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    return {}


@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    return node.network.response_handler.fetch_rids(req)


@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    manifests_payload = node.network.response_handler.fetch_manifests(req)
    return manifests_payload  # The default handler already includes not_found


@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    bundles_payload = node.network.response_handler.fetch_bundles(req)
    return bundles_payload


@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed, e.g., node.is_running()
    return {"status": "healthy"}


app.include_router(koi_net_router)  # KOI-net API endpoints
app.include_router(github_router)  # GitHub webhook endpoint

logger.info("FastAPI application configured with webhook and KOI-net routers.")
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/types.py">
from rid_lib.core import ORN


class GithubCommit(ORN):
    """
    Resource Identifier (RID) for a specific GitHub commit.

    Format: orn:github.commit:<owner>/<repo>/<sha>
    Example: orn:github.commit:microsoft/vscode/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
    """

    namespace = "github.commit"

    def __init__(self, owner: str, repo: str, sha: str):
        """
        Initialize a GitHub commit RID.

        Args:
            owner: The repository owner (user or organization)
            repo: The repository name
            sha: The commit SHA (full 40-character or shortened)
        """
        if not owner or not repo or not sha:
            raise ValueError("Owner, repo, and SHA cannot be empty")

        if "/" in owner or "/" in repo:
            raise ValueError("Owner and repo cannot contain '/' character")

        self.owner = owner
        self.repo = repo
        self.sha = sha

    @property
    def reference(self) -> str:
        """Returns the reference part of the RID: '<owner>/<repo>/<sha>'."""
        return f"{self.owner}/{self.repo}/{self.sha}"

    @property
    def repository_full_name(self) -> str:
        """Returns the full repository name: '<owner>/<repo>'."""
        return f"{self.owner}/{self.repo}"

    @property
    def html_url(self) -> str:
        """Returns the HTML URL to view this commit on GitHub."""
        return f"https://github.com/{self.owner}/{self.repo}/commit/{self.sha}"

    @property
    def api_url(self) -> str:
        """Returns the GitHub API URL for this commit."""
        return (
            f"https://api.github.com/repos/{self.owner}/{self.repo}/commits/{self.sha}"
        )

    @classmethod
    def from_reference(cls, reference: str) -> "GithubCommit":
        """
        Creates a GithubCommit instance from its reference string.

        Args:
            reference: String in format '<owner>/<repo>/<sha>'

        Returns:
            GithubCommit instance

        Raises:
            ValueError: If the reference format is invalid
        """
        try:
            parts = reference.split("/", maxsplit=2)
            if len(parts) != 3:
                raise ValueError("Reference must contain exactly two '/' separators")

            owner, repo, sha = parts

            if not owner or not repo or not sha:
                raise ValueError("Owner, repo, and SHA parts cannot be empty")

            # Basic SHA length check
            if len(sha) < 7:  # Minimum length for a short SHA
                raise ValueError(f"SHA part seems too short: {sha}")

            return cls(owner=owner, repo=repo, sha=sha)

        except ValueError as e:
            raise ValueError(
                f"Invalid reference format for GithubCommit. Expected '<owner>/<repo>/<sha>', got '{reference}'. Error: {e}"
            ) from e
        except Exception as e:
            raise TypeError(
                f"Unexpected error parsing GithubCommit reference '{reference}': {e}"
            ) from e
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/webhook.py">
import logging
import hmac
import hashlib
import json
from fastapi import APIRouter, Request, Header, HTTPException, Body
from rid_lib.ext import Bundle
from .types import GithubCommit
from .core import node
from .config import (
    GITHUB_WEBHOOK_SECRET,
    MONITORED_REPOS,
    update_state_file,
    LAST_PROCESSED_SHA,
)

logger = logging.getLogger(__name__)

router = APIRouter()


async def verify_signature(request: Request, x_hub_signature_256: str = Header(None)):
    """Verify the GitHub webhook signature."""
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning("Webhook verification skipped: GITHUB_WEBHOOK_SECRET not set.")
        return

    body = await request.body()
    hash_object = hmac.new(
        GITHUB_WEBHOOK_SECRET.encode("utf-8"), msg=body, digestmod=hashlib.sha256
    )
    expected_signature = "sha256=" + hash_object.hexdigest()

    if not hmac.compare_digest(expected_signature, x_hub_signature_256):
        logger.error(
            f"Webhook verification failed: Invalid signature. Expected: {expected_signature}, Got: {x_hub_signature_256}"
        )
        raise HTTPException(status_code=403, detail="Invalid signature")

    logger.debug("Webhook signature verified successfully.")


@router.post("/github/webhook", status_code=202)  # Use 202 Accepted as we process async
async def github_webhook(
    request: Request,
    x_github_event: str = Header(...),  # Required header
    x_hub_signature_256: str = Header(...),  # Required for verification
):
    """Handle incoming GitHub webhook events (specifically 'push')."""
    logger.info(f"Received GitHub webhook event: {x_github_event}")

    try:
        # --- Signature Verification (Optional) ---
        # await verify_signature(request, x_hub_signature_256)

        # --- Parse JSON Payload ---
        raw_body = await request.body()
        try:
            payload = json.loads(raw_body)
        except json.JSONDecodeError:
            logger.error("Invalid JSON in GitHub webhook payload")
            raise HTTPException(status_code=400, detail="Invalid JSON")

        # --- Event Handling ---
        if x_github_event == "ping":
            logger.info("Received 'ping' event from GitHub. Responding OK.")
            return {"message": "Pong!"}

        if x_github_event != "push":
            logger.debug(f"Ignoring non-'push' event: {x_github_event}")
            return {"message": f"Ignoring event type: {x_github_event}"}

        logger.info(f"Processing 'push' event: {payload}")

        # --- Process 'push' Event ---
        repo_info = payload.get("repository", {})
        repo_full_name = repo_info.get("full_name")
        repo_owner = repo_info.get("owner", {}).get("login") or repo_info.get(
            "owner", {}
        ).get("name")
        repo_name = repo_info.get("name")
        commits = payload.get("commits", [])
        head_commit = payload.get("head_commit", {})

        if not repo_full_name or not repo_owner or not repo_name:
            logger.error(f"Webhook payload missing repository details: {repo_info}")
            raise HTTPException(
                status_code=400, detail="Missing repository information in payload"
            )

        # Check if the repository is monitored
        if repo_full_name not in MONITORED_REPOS:
            logger.debug(
                f"Ignoring push event for non-monitored repository: {repo_full_name}"
            )
            return {"message": f"Repository {repo_full_name} not monitored"}

        if not commits and not head_commit:
            logger.warning(
                f"'push' event for {repo_full_name} received without 'commits' or 'head_commit' data. Possibly a branch deletion or tag push? Payload head: {payload.get('ref', '')}"
            )
            return {"message": "No commit data found in push event"}

        # Determine the commit(s) to process and the SHA to potentially update state with
        commits_to_process = []
        sha_to_update_state = None

        head_commit_id = head_commit.get("id")
        if head_commit_id:
            commits_to_process = [head_commit]  # Use head_commit as the primary source
            sha_to_update_state = (
                head_commit_id  # This is the SHA representing the push tip
            )
            logger.debug(f"Processing head_commit: {head_commit_id}")
        elif commits:
            commits_to_process = commits  # Fallback to commits list
            # If using commits list, the last commit's SHA is the best candidate for state update
            if commits:
                sha_to_update_state = commits[-1].get("id")
            logger.debug(
                f"Processing commits list (count: {len(commits)}). Potential state update SHA: {sha_to_update_state}"
            )
        else:
            # This case should ideally not be reached due to the check above, but included for completeness
            logger.warning(
                f"No processable commit data found in push event for {repo_full_name}. Skipping."
            )
            return {"message": "No processable commit data"}

        processed_new_commit = False
        for commit in commits_to_process:
            commit_sha = commit.get("id")
            if not commit_sha:
                logger.warning("Skipping commit in payload with missing 'id'.")
                continue

            # Avoid reprocessing the last known SHA for this repo
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if last_known_sha_for_repo and commit_sha == last_known_sha_for_repo:
                logger.debug(
                    f"Skipping commit {commit_sha} for {repo_full_name} as it matches last known SHA {last_known_sha_for_repo}."
                )
                continue

            # Inner try-except for processing individual commits within the push
            try:
                # Construct RID
                rid = GithubCommit(owner=repo_owner, repo=repo_name, sha=commit_sha)

                # Extract details - ensure keys exist
                author = commit.get("author", {})
                committer = commit.get("committer", {})

                contents = {
                    "sha": commit_sha,
                    "message": commit.get("message"),
                    "author_name": author.get("name"),
                    "author_email": author.get("email"),
                    "author_date": commit.get(
                        "timestamp"
                    ),  # GitHub often uses 'timestamp'
                    "committer_name": committer.get("name"),
                    "committer_email": committer.get("email"),
                    "committer_date": committer.get("timestamp"),
                    "html_url": commit.get("url"),  # Use 'url' from webhook payload
                    "parents": commit.get(
                        "parents", []
                    ),  # Typically a list of SHAs in webhook
                }

                bundle = Bundle.generate(rid=rid, contents=contents)
                # CORRECT USAGE: 'handle' makes the bundle available locally
                # in the sensor's cache/event queue for consumers to poll/fetch.
                # It does NOT push the application-specific GithubCommit bundle directly.
                logger.debug(
                    f"Making webhook commit bundle {rid} available locally via sensor API."
                )
                node.processor.handle(bundle=bundle)

                processed_new_commit = (
                    True  # Mark that we processed at least one new commit
                )

            except Exception as e:
                logger.error(
                    f"Error processing webhook commit {commit_sha} for {repo_full_name}: {e}",
                    exc_info=True,
                )
                # Decide whether to continue processing other commits in the push or stop
                continue  # Continue with next commit in the webhook push

        # Update state file only if we processed a new commit and have a valid SHA representing the push tip
        if processed_new_commit and sha_to_update_state:
            # Check again if the sha_to_update is different from the stored one before writing
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if sha_to_update_state != last_known_sha_for_repo:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. Updating state to SHA: {sha_to_update_state}"
                )
                update_state_file(repo_full_name, sha_to_update_state)
            else:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. State SHA {sha_to_update_state} already stored."
                )
        elif processed_new_commit:
            logger.warning(
                f"Webhook processing complete for {repo_full_name}. Processed new commit(s) but could not determine SHA for state update."
            )
        else:
            logger.info(
                f"Webhook processing complete for {repo_full_name}. No new commits processed or state updated."
            )

        return {"message": "Webhook processed successfully"}

    # Exception handlers are now correctly indented relative to the main 'try' block
    except HTTPException as he:
        # Re-raise HTTP exceptions to return proper status codes
        logger.warning(f"HTTP Exception during webhook processing: {he.detail}")
        raise he
    except Exception as e:
        logger.error(
            f"Unexpected error handling webhook event {x_github_event}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Internal server error handling webhook"
        )
</file>

<file path="nodes/koi-net-github-sensor-node/.env.example">
# KOI-net GitHub Node Configuration

# Public URL where this GitHub node can be reached by other KOI-net nodes
URL="http://127.0.0.1:8001"

# URL of the coordinator node or another known KOI-net node
FIRST_CONTACT="http://127.0.0.1:8000/koi-net"

# Host the FastAPI server should listen on (0.0.0.0 allows external connections)
HOST="0.0.0.0"

# Port the FastAPI server should listen on
PORT="8001"

# GitHub Personal Access Token (replace with your own token)
GITHUB_TOKEN="your_github_token_here"

# Secret used to verify webhook signatures (generate a secure random string)
GITHUB_WEBHOOK_SECRET="your_webhook_secret_here"

# Comma-separated list of repositories to monitor (format: owner/repo)
MONITORED_REPOS="owner/repo1,owner/repo2"

# Path to the JSON file storing the last processed commit SHA
STATE_FILE_PATH="state.json"

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL="INFO"
</file>

<file path="nodes/koi-net-github-sensor-node/.gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
</file>

<file path="nodes/koi-net-github-sensor-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy pyproject and perform editable installation
COPY pyproject.toml /app/

# 3) install curl for healthcheck
RUN apt-get update && apt-get install -y curl

# 4) install dependencies using UV and pyproject.toml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 5) copy in code
COPY . /app/

# 6) expose port and set environment
EXPOSE 8001

# 7) add healthcheck using the main app's endpoint
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8001/koi-net/health || exit 1

# 8) start the main service using shell form for ENV var substitution
CMD ["uvicorn", "github_sensor_node.server:app", "--host", "0.0.0.0", "--port", "8001"]
</file>

<file path="nodes/koi-net-github-sensor-node/github-node.service">
[Unit]
Description=KOI-net Github Node Service
After=network.target

[Service]
WorkingDirectory=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/services/github
ExecStart=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/.venv/bin/python3 -m node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="nodes/koi-net-github-sensor-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "github-sensor-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.3",
    "koi-net==1.0.0b12",
    "PyGithub",
    "aiohttp",
    "rich", # Added for logging
    "ruamel.yaml", # Added for YAML config loading
    "python-dotenv" # Added for loading .env in local runs
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="nodes/koi-net-github-sensor-node/repomix-output.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
github_sensor_node/
  handlers/
    github.py
  __init__.py
  __main__.py
  backfill.py
  config.py
  core.py
  loader.py
  server.py
  types.py
  webhook.py
.env.example
.gitignore
Dockerfile
github-node.service
pyproject.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="github_sensor_node/handlers/github.py">
import logging
from ..core import node
from ..types import GithubCommit

from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of the coordinator node (or other nodes providing KoiNetNode events).

    On discovering a NEW coordinator, proposes a WEBHOOK edge for bidirectional
    communication and requests a list of other known nodes (sync).
    (Based on refactor.md example)
    """
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if KoiNetNode not in profile.provides.event:
            logger.debug(
                f"Node {kobj.rid} does not provide KoiNetNode events. Ignoring."
            )
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    logger.info(
        f"Identified potential coordinator/peer: {kobj.rid}; proposing WEBHOOK edge"
    )
    try:

        edge_bundle = generate_edge_bundle(
            source=kobj.rid,
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode],
        )
        processor.handle(bundle=edge_bundle)
    except Exception as e:
        logger.error(
            f"Failed to generate or handle WEBHOOK edge bundle for {kobj.rid}: {e}",
            exc_info=True,
        )

    logger.info(f"Syncing network nodes from {kobj.rid}")
    try:
        payload = processor.network.request_handler.fetch_rids(
            kobj.rid, rid_types=[KoiNetNode]
        )
        if not payload or not payload.rids:
            logger.warning(f"Received empty RIDs payload from {kobj.rid} during sync.")
            return

        logger.debug(f"Received {len(payload.rids)} RIDs from {kobj.rid}")
        for rid in payload.rids:

            if rid == processor.identity.rid or processor.cache.exists(rid):
                continue
            logger.debug(f"Handling discovered RID from sync: {rid}")

            processor.handle(rid=rid, source=KnowledgeSource.External)
    except Exception as e:
        logger.error(f"Failed during network sync with {kobj.rid}: {e}", exc_info=True)


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit(processor: ProcessorInterface, kobj: KnowledgeObject):
    """
    Basic handler for processing GithubCommit bundles.
    Currently just logs information.

    Args:
        bundle: The Bundle object containing the GithubCommit RID and contents.
    """
    try:

        bundle = kobj.bundle
        rid: GithubCommit = bundle.rid
        contents: dict = bundle.contents

        logger.info(
            f"Processing commit: {rid} (Normalized Type: {kobj.normalized_event_type}, Source: {kobj.source})"
        )

        logger.debug(
            f"  Author: {contents.get('author_name')} <{contents.get('author_email')}>"
        )
        logger.debug(
            f"  Message: {contents.get('message', '').splitlines()[0][:80]}..."
        )
        logger.debug(f"  URL: {contents.get('html_url')}")

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Example handler demonstrating processing before cache write."""
    logger.debug(f"Handling GithubCommit bundle PRE-CACHE for {kobj.rid}")

    try:

        if kobj.contents and "message" in kobj.contents:
            logger.info(
                f"Processing commit {kobj.rid.sha[:7]}: {kobj.contents['message'].splitlines()[0]}"
            )

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


logger.info("GithubCommit Bundle handler registered.")
</file>

<file path="github_sensor_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path

# Import LOG_LEVEL from refactored config
from .config import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/github")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "github-sensor-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
logging.getLogger("github").setLevel(logging.WARNING)  # PyGithub can be verbose
</file>

<file path="github_sensor_node/__main__.py">
import uvicorn
import logging
from .config import HOST, PORT

logger = logging.getLogger(__name__)

logger.info(f"GitHub sensor node starting on {HOST}:{PORT}")
uvicorn.run(
    "github_sensor_node.server:app",
    host=HOST,
    port=PORT,
    log_config=None,
    reload=True,
)
</file>

<file path="github_sensor_node/backfill.py">
import logging
from github import Github, GithubException, RateLimitExceededException
from github.Commit import Commit
from rid_lib.ext import Bundle

# Assuming GithubCommit RID type is accessible
from .types import GithubCommit
from .core import node

# Import necessary config values
from .config import GITHUB_TOKEN, MONITORED_REPOS, LAST_PROCESSED_SHA, update_state_file

logger = logging.getLogger(__name__)

# Initialize GitHub client (authenticated if token provided)
# Use the GITHUB_TOKEN loaded from config
github_client = Github(GITHUB_TOKEN) if GITHUB_TOKEN else Github()
logger.info(f"GitHub client initialized. Authenticated: {bool(GITHUB_TOKEN)}")


def perform_backfill():
    """
    One-time startup backfill: fetch all commits since LAST_PROCESSED_SHA
    for each monitored repo, bundle them as NEW, and persist the latest SHA processed.
    Processes commits oldest-to-newest after fetching.
    """
    logger.info("Starting GitHub backfill process...")

    # Load the state dictionary once before the loop
    current_state = (
        LAST_PROCESSED_SHA.copy()
    )  # Use a copy to avoid modifying during iteration if needed
    newest_sha_processed_overall_map = (
        {}
    )  # Track newest SHA per repo processed in this run

    if not MONITORED_REPOS:
        logger.warning(
            "No repositories configured in MONITORED_REPOS. Backfill skipped."
        )
        return

    for repo_full_name in MONITORED_REPOS:
        try:
            owner, repo_name_only = repo_full_name.split("/")
            # Get the last processed SHA for *this specific repository*
            last_sha_for_repo = current_state.get(repo_full_name)
            logger.info(
                f"Backfilling repository: {repo_full_name} since SHA: {last_sha_for_repo or 'beginning'}"
            )

            gh_repo = github_client.get_repo(repo_full_name)
            commits_to_process_buffer: list[Commit] = []

            # Iterate commits newest-first until we find the last processed one
            paginated_commits = gh_repo.get_commits()
            logger.debug(f"Fetching commits for {repo_full_name}...")
            commit_count = 0
            for commit in paginated_commits:
                commit_count += 1
                # Check against the specific SHA for this repo
                if last_sha_for_repo and commit.sha == last_sha_for_repo:
                    logger.info(
                        f"Found last processed SHA {last_sha_for_repo} in {repo_full_name}. Stopping fetch for this repo."
                    )
                    break
                commits_to_process_buffer.append(commit)
                # Safety break for potentially huge repos without a known SHA
                # Adjust limit as needed
                if commit_count % 100 == 0:
                    logger.debug(
                        f"Fetched {commit_count} commits for {repo_full_name} so far..."
                    )
                # if commit_count > 1000:
                #    logger.warning(f"Reached fetch limit (1000) for {repo_full_name}. Consider adjusting.")
                #    break

            logger.info(
                f"Found {len(commits_to_process_buffer)} new commits in {repo_full_name} to backfill."
            )

            # Process commits oldest → newest
            newest_sha_processed_in_repo = None
            for commit in reversed(commits_to_process_buffer):
                try:
                    rid = GithubCommit(owner=owner, repo=repo_name_only, sha=commit.sha)
                    # Extract commit details carefully, handling potential missing attributes
                    author = commit.commit.author
                    committer = commit.commit.committer

                    contents = {
                        "sha": commit.sha,
                        "message": commit.commit.message,
                        "author_name": author.name if author else None,
                        "author_email": author.email if author else None,
                        "author_date": (
                            author.date.isoformat() if author and author.date else None
                        ),
                        "committer_name": committer.name if committer else None,
                        "committer_email": committer.email if committer else None,
                        "committer_date": (
                            committer.date.isoformat()
                            if committer and committer.date
                            else None
                        ),
                        "html_url": commit.html_url,
                        "parents": [
                            p.sha for p in commit.parents
                        ],  # List of parent SHAs
                    }
                    bundle = Bundle.generate(rid=rid, contents=contents)
                    # CORRECT USAGE: 'handle' makes the bundle available locally
                    # in the sensor's cache/event queue for consumers to poll/fetch.
                    # It does NOT push the application-specific GithubCommit bundle directly.
                    logger.debug(
                        f"Making backfill commit bundle {rid} available locally via sensor API."
                    )
                    node.processor.handle(bundle=bundle)

                    # Track the newest SHA processed in this run for this repo
                    newest_sha_processed_in_repo = commit.sha

                except Exception as e:
                    logger.error(
                        f"Error processing commit {commit.sha} in {repo_full_name}: {e}",
                        exc_info=True,
                    )

            # Store the newest SHA processed for this repo during this run
            if newest_sha_processed_in_repo:
                newest_sha_processed_overall_map[repo_full_name] = (
                    newest_sha_processed_in_repo
                )
                logger.debug(
                    f"Newest SHA processed for {repo_full_name} in this run: {newest_sha_processed_in_repo}"
                )

        except RateLimitExceededException:
            logger.error(
                f"GitHub API rate limit exceeded while backfilling {repo_full_name}. Aborting backfill. Try again later or use a GITHUB_TOKEN."
            )
            # Depending on requirements, could wait and retry, but for now, we stop.
            return  # Stop the entire backfill process
        except GithubException as e:
            logger.error(
                f"GitHub API error for repository {repo_full_name}: {e}. Skipping this repo."
            )
            continue  # Skip to the next repository
        except Exception as e:
            logger.error(
                f"Unexpected error backfilling repository {repo_full_name}: {e}",
                exc_info=True,
            )
            continue  # Skip to the next repository

    # After processing all repos, persist the newest SHAs found (if changed)
    updated_count = 0
    for repo_full_name, newest_sha in newest_sha_processed_overall_map.items():
        if newest_sha != current_state.get(repo_full_name):
            update_state_file(
                repo_full_name, newest_sha
            )  # Call function from config.py
            updated_count += 1
        else:
            logger.debug(
                f"No state update needed for {repo_full_name}, newest SHA {newest_sha} is same as stored."
            )

    if updated_count > 0:
        logger.info(
            f"Backfill complete. Updated state for {updated_count} repositories."
        )
    else:
        logger.info(
            f"Backfill complete. No new commits found or state changes required across monitored repositories."
        )


if __name__ == "__main__":
    # Example of how to run backfill directly for testing
    # Requires node to be started if handle() depends on active components
    # In practice, this is called by server.py during startup
    logging.basicConfig(level=logging.INFO)
    logger.info("Running backfill directly for testing...")
    # node.start() # Might be needed depending on node.processor.handle implementation
    perform_backfill()
    # node.stop()
</file>

<file path="github_sensor_node/config.py">
import logging
import json
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Load Environment Variables First ---
# Define potential paths for global.env
DOCKER_ENV_PATH = Path("/app/config/global.env")  # Path inside docker
LOCAL_ENV_PATH = Path(__file__).parent.parent.parent.parent / "config" / "global.env"

if DOCKER_ENV_PATH.is_file():
    load_dotenv(dotenv_path=DOCKER_ENV_PATH, override=True)
    logger.debug(f"Loaded environment variables from Docker path: {DOCKER_ENV_PATH}")
elif LOCAL_ENV_PATH.is_file():
    load_dotenv(dotenv_path=LOCAL_ENV_PATH, override=True)
    logger.debug(f"Loaded environment variables from Local path: {LOCAL_ENV_PATH}")
else:
    logger.warning(
        f"global.env file not found at Docker path {DOCKER_ENV_PATH} or local path {LOCAL_ENV_PATH}. Secrets might be missing."
    )
# --- End Environment Variable Loading ---


# Define potential configuration file paths for github-sensor.yaml
DOCKER_CONFIG_PATH = Path("/app/config/github-sensor.yaml")
LOCAL_CONFIG_PATH = (
    Path(__file__).parent.parent.parent.parent / "config" / "github-sensor.yaml"
)


def find_config_path() -> Path | None:
    """Finds the valid configuration file path."""
    if DOCKER_CONFIG_PATH.is_file():
        logger.debug(f"Using Docker config path: {DOCKER_CONFIG_PATH}")
        return DOCKER_CONFIG_PATH
    elif LOCAL_CONFIG_PATH.is_file():
        logger.debug(f"Using local config path: {LOCAL_CONFIG_PATH}")
        return LOCAL_CONFIG_PATH
    else:
        logger.error(
            f"Configuration file not found at Docker path {DOCKER_CONFIG_PATH} or local path {LOCAL_CONFIG_PATH}"
        )
        return None


def load_yaml_config() -> Dict[str, Any]:
    """Loads configuration from the YAML file."""
    config_path = find_config_path()
    if not config_path:
        logger.error("No valid YAML configuration file found. Using empty default.")
        return {}

    try:
        yaml = YAML(typ="safe")
        with open(config_path, "r") as f:
            config_data = yaml.load(f)
        logger.info(f"Successfully loaded YAML configuration from {config_path}")
        return config_data
    except Exception as e:
        logger.exception(f"Error loading YAML configuration from {config_path}: {e}")
        return {}


# Load YAML configuration
CONFIG = load_yaml_config()

# --- Determine Run Context ---
is_docker = os.getenv("RUN_CONTEXT") == "docker"

# --- Extract specific configurations with defaults ---
SENSOR_CONFIG: Dict[str, Any] = CONFIG.get("sensor", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
WEBHOOK_CONFIG: Dict[str, Any] = CONFIG.get("webhook", {})

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/github")  # Relative to workspace root for local runs

# Base configuration values
MONITORED_REPOS: List[str] = SENSOR_CONFIG.get("repos", [])
POLL_INTERVAL: int = SENSOR_CONFIG.get("poll_interval", 60)
SENSOR_MODE: str = SENSOR_CONFIG.get("mode", "webhook")
HOST: str = RUNTIME_CONFIG.get("host", "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8001)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()

# Adjust URLs
raw_coordinator_url = EDGES_CONFIG.get("coordinator_url")
if not is_docker and raw_coordinator_url:
    COORDINATOR_URL = raw_coordinator_url.replace(
        "http://coordinator:", f"http://localhost:"
    )
    logger.debug(f"Adjusted Coordinator URL for local run: {COORDINATOR_URL}")
else:
    COORDINATOR_URL = raw_coordinator_url

raw_base_url = RUNTIME_CONFIG.get("base_url")
if not is_docker and raw_base_url:
    BASE_URL = raw_base_url.replace("http://github-sensor:", f"http://localhost:")
    logger.debug(f"Adjusted Base URL for local run: {BASE_URL}")
else:
    BASE_URL = raw_base_url

# Adjust Cache and State Paths
DOCKER_CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", "/data/cache")
# Read state file path from YAML, default to a path within CACHE_DIR if not specified
DOCKER_STATE_FILE = RUNTIME_CONFIG.get(
    "state_file", os.path.join(DOCKER_CACHE_DIR, "github_state.json")
)

if is_docker:
    CACHE_DIR = DOCKER_CACHE_DIR
    # Ensure state file path is absolute for Docker
    STATE_FILE_PATH = (
        Path(DOCKER_STATE_FILE)
        if Path(DOCKER_STATE_FILE).is_absolute()
        else Path("/app") / DOCKER_STATE_FILE
    )
else:
    LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)  # Ensure local base dir exists
    CACHE_DIR = str(LOCAL_DATA_BASE / "cache")
    STATE_FILE_PATH = LOCAL_DATA_BASE / "github_state.json"  # Standardized local path
    logger.debug(f"Using local CACHE_DIR: {CACHE_DIR}")
    logger.debug(f"Using local STATE_FILE_PATH: {STATE_FILE_PATH}")

# Ensure resolved CACHE_DIR exists, especially for local runs
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Load Secrets from Environment Variables ---
GITHUB_TOKEN: str | None = os.getenv("GITHUB_TOKEN")
WEBHOOK_SECRET_ENV_VAR: str | None = WEBHOOK_CONFIG.get("secret_env_var")
GITHUB_WEBHOOK_SECRET: str | None = None
if WEBHOOK_SECRET_ENV_VAR:
    GITHUB_WEBHOOK_SECRET = os.getenv(WEBHOOK_SECRET_ENV_VAR)
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning(
            f"Environment variable '{WEBHOOK_SECRET_ENV_VAR}' specified in config but not found in environment."
        )
else:
    logger.warning("webhook.secret_env_var not specified in github-sensor.yaml")

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL)
    # Optionally set levels for other loggers
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logging.getLogger("github").setLevel(logging.WARNING)  # PyGithub can be verbose
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("GitHub Sensor Configuration Loaded:")
logger.info(f"  Sensor Mode: {SENSOR_MODE}")
logger.info(f"  Monitored Repos: {MONITORED_REPOS}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  State File Path: {STATE_FILE_PATH}")  # Log the final path
logger.info(f"  GitHub Token Loaded: {bool(GITHUB_TOKEN)}")
logger.info(f"  Webhook Secret Loaded: {bool(GITHUB_WEBHOOK_SECRET)}")

# Check required config
if not COORDINATOR_URL:
    logger.critical(
        "Configuration error: edges.coordinator_url is not set or resolved correctly."
    )
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not GITHUB_WEBHOOK_SECRET:
    logger.warning("Configuration warning: GitHub Webhook Secret not loaded.")

# --- State Management (Loading initial state & update function) ---
LAST_PROCESSED_SHA: Dict[str, str] = {}  # Dictionary mapping repo_name -> last_sha


def load_state():
    """Loads the last processed SHA state from the JSON file specified by STATE_FILE_PATH."""
    global LAST_PROCESSED_SHA
    # Path is now guaranteed to be a Path object
    state_path = STATE_FILE_PATH
    try:
        with open(state_path, "r") as f:
            LAST_PROCESSED_SHA = json.load(f)
        logger.info(
            f"Loaded state from '{state_path}': Repos {list(LAST_PROCESSED_SHA.keys())}"
        )
    except FileNotFoundError:
        logger.warning(
            f"State file '{state_path}' not found. Starting with empty state."
        )
        LAST_PROCESSED_SHA = {}
    except json.JSONDecodeError:
        logger.error(
            f"Error decoding JSON from state file '{state_path}'. Starting with empty state."
        )
        LAST_PROCESSED_SHA = {}
    except Exception as e:
        logger.error(
            f"Unexpected error loading state file '{state_path}': {e}",
            exc_info=True,
        )
        LAST_PROCESSED_SHA = {}


def update_state_file(repo_name: str, last_sha: str):
    """Updates the state file with the latest processed SHA for a repo."""
    global LAST_PROCESSED_SHA
    LAST_PROCESSED_SHA[repo_name] = last_sha

    state_path = STATE_FILE_PATH  # Use the context-aware path
    try:
        # Ensure directory exists before writing
        state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(state_path, "w") as f:
            json.dump(LAST_PROCESSED_SHA, f, indent=4)
        logger.debug(
            f"Updated state file '{state_path}' for {repo_name} with SHA: {last_sha}"
        )
    except IOError as e:
        logger.error(f"Failed to write state file '{state_path}': {e}")
    except Exception as e:
        logger.error(
            f"Unexpected error writing state file '{state_path}': {e}",
            exc_info=True,
        )


# Load initial state when config module is imported
load_state()
</file>

<file path="github_sensor_node/core.py">
import os
import shutil
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the refactored config module
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
    LOG_LEVEL,
)  # Import necessary vars
from .types import GithubCommit

logger = logging.getLogger(__name__)

name = "github"

# Identity and cache directory paths remain relative for setup,
# but NodeInterface gets absolute paths derived from config
identity_dir = f".koi/{name}"
# Cache dir is now defined in config, use that path for NodeInterface
# cache_dir_setup = f".koi/{name}/rid_cache_{name}"

# Clear existing state directories logic remains the same
# Consider making this behavior optional or configurable
logger.info(f"Attempting to clear existing identity directory: {identity_dir}")
shutil.rmtree(identity_dir, ignore_errors=True)
# Cache dir managed by Docker volume, no need to clear here
# shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the identity directory
os.makedirs(identity_dir, exist_ok=True)
# Ensure cache directory exists within the container is handled by Docker volume mount implicitly
# os.makedirs(cache_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Ensure required config values are present
if not BASE_URL:
    raise ValueError("Runtime base_url is not configured in github-sensor.yaml")
if not COORDINATOR_URL:
    raise ValueError("Edges coordinator_url is not configured in github-sensor.yaml")

# Initialize the KOI-net Node Interface for the GitHub Sensor
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        # Use BASE_URL from config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[GithubCommit],
            state=[GithubCommit],
        ),
    ),
    use_kobj_processor_thread=True,
    # Use COORDINATOR_URL from config for first_contact
    first_contact=COORDINATOR_URL,
    # State file paths are now relative to the application root in the container
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Use CACHE_DIR from config (should be /data/cache)
    cache_directory_path=CACHE_DIR,
)

logger.info(f"Initialized NodeInterface: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")
</file>

<file path="github_sensor_node/loader.py">
from .core import node
from .handlers import github


def register_handlers():
    print("Registering GITHUB handlers...")
    _ = github
</file>

<file path="github_sensor_node/server.py">
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, Request, Body, Header, HTTPException
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node
from .webhook import router as github_router
from .backfill import perform_backfill
from .loader import register_handlers

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup, backfill, and shutdown."""
    logger.info("Starting FastAPI application lifespan...")
    # Start the KOI-net node
    try:
        register_handlers()
        node.start()
        logger.info("KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        raise RuntimeError("Failed to initialize KOI-net node") from e

    logger.info("Scheduling initial GitHub backfill...")
    backfill_task = asyncio.to_thread(perform_backfill)

    try:
        yield  # Application runs here
    finally:
        logger.info("Shutting down FastAPI application...")
        # Attempt to gracefully cancel the backfill if it's still running
        # This might require more sophisticated task management if backfill is long-running
        # if backfill_task and not backfill_task.done():
        #     try:
        #         backfill_task.cancel()
        #         await backfill_task
        #     except asyncio.CancelledError:
        #         logger.info("Backfill task cancelled.")
        #     except Exception as e:
        #         logger.error(f"Error cancelling backfill task: {e}", exc_info=True)

        try:
            node.stop()
            logger.info("KOI-net node stopped successfully.")
        except Exception as e:
            logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
        logger.info("FastAPI application shutdown complete.")


app = FastAPI(
    title="KOI-net GitHub Sensor Node",
    description="Listens for GitHub webhooks and performs backfill to ingest commit data.",
    version="0.1.0",
    lifespan=lifespan,
)

koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    return {}


@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    return node.network.response_handler.fetch_rids(req)


@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    manifests_payload = node.network.response_handler.fetch_manifests(req)
    return manifests_payload  # The default handler already includes not_found


@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    bundles_payload = node.network.response_handler.fetch_bundles(req)
    return bundles_payload


@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed, e.g., node.is_running()
    return {"status": "healthy"}


app.include_router(koi_net_router)  # KOI-net API endpoints
app.include_router(github_router)  # GitHub webhook endpoint

logger.info("FastAPI application configured with webhook and KOI-net routers.")
</file>

<file path="github_sensor_node/types.py">
from rid_lib.core import ORN


class GithubCommit(ORN):
    """
    Resource Identifier (RID) for a specific GitHub commit.

    Format: orn:github.commit:<owner>/<repo>/<sha>
    Example: orn:github.commit:microsoft/vscode/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
    """

    namespace = "github.commit"

    def __init__(self, owner: str, repo: str, sha: str):
        """
        Initialize a GitHub commit RID.

        Args:
            owner: The repository owner (user or organization)
            repo: The repository name
            sha: The commit SHA (full 40-character or shortened)
        """
        if not owner or not repo or not sha:
            raise ValueError("Owner, repo, and SHA cannot be empty")

        if "/" in owner or "/" in repo:
            raise ValueError("Owner and repo cannot contain '/' character")

        self.owner = owner
        self.repo = repo
        self.sha = sha

    @property
    def reference(self) -> str:
        """Returns the reference part of the RID: '<owner>/<repo>/<sha>'."""
        return f"{self.owner}/{self.repo}/{self.sha}"

    @property
    def repository_full_name(self) -> str:
        """Returns the full repository name: '<owner>/<repo>'."""
        return f"{self.owner}/{self.repo}"

    @property
    def html_url(self) -> str:
        """Returns the HTML URL to view this commit on GitHub."""
        return f"https://github.com/{self.owner}/{self.repo}/commit/{self.sha}"

    @property
    def api_url(self) -> str:
        """Returns the GitHub API URL for this commit."""
        return (
            f"https://api.github.com/repos/{self.owner}/{self.repo}/commits/{self.sha}"
        )

    @classmethod
    def from_reference(cls, reference: str) -> "GithubCommit":
        """
        Creates a GithubCommit instance from its reference string.

        Args:
            reference: String in format '<owner>/<repo>/<sha>'

        Returns:
            GithubCommit instance

        Raises:
            ValueError: If the reference format is invalid
        """
        try:
            parts = reference.split("/", maxsplit=2)
            if len(parts) != 3:
                raise ValueError("Reference must contain exactly two '/' separators")

            owner, repo, sha = parts

            if not owner or not repo or not sha:
                raise ValueError("Owner, repo, and SHA parts cannot be empty")

            # Basic SHA length check
            if len(sha) < 7:  # Minimum length for a short SHA
                raise ValueError(f"SHA part seems too short: {sha}")

            return cls(owner=owner, repo=repo, sha=sha)

        except ValueError as e:
            raise ValueError(
                f"Invalid reference format for GithubCommit. Expected '<owner>/<repo>/<sha>', got '{reference}'. Error: {e}"
            ) from e
        except Exception as e:
            raise TypeError(
                f"Unexpected error parsing GithubCommit reference '{reference}': {e}"
            ) from e
</file>

<file path="github_sensor_node/webhook.py">
import logging
import hmac
import hashlib
import json
from fastapi import APIRouter, Request, Header, HTTPException, Body
from rid_lib.ext import Bundle
from .types import GithubCommit
from .core import node
from .config import (
    GITHUB_WEBHOOK_SECRET,
    MONITORED_REPOS,
    update_state_file,
    LAST_PROCESSED_SHA,
)

logger = logging.getLogger(__name__)

router = APIRouter()


async def verify_signature(request: Request, x_hub_signature_256: str = Header(None)):
    """Verify the GitHub webhook signature."""
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning("Webhook verification skipped: GITHUB_WEBHOOK_SECRET not set.")
        return

    body = await request.body()
    hash_object = hmac.new(
        GITHUB_WEBHOOK_SECRET.encode("utf-8"), msg=body, digestmod=hashlib.sha256
    )
    expected_signature = "sha256=" + hash_object.hexdigest()

    if not hmac.compare_digest(expected_signature, x_hub_signature_256):
        logger.error(
            f"Webhook verification failed: Invalid signature. Expected: {expected_signature}, Got: {x_hub_signature_256}"
        )
        raise HTTPException(status_code=403, detail="Invalid signature")

    logger.debug("Webhook signature verified successfully.")


@router.post("/github/webhook", status_code=202)  # Use 202 Accepted as we process async
async def github_webhook(
    request: Request,
    x_github_event: str = Header(...),  # Required header
    x_hub_signature_256: str = Header(...),  # Required for verification
):
    """Handle incoming GitHub webhook events (specifically 'push')."""
    logger.info(f"Received GitHub webhook event: {x_github_event}")

    try:
        # --- Signature Verification (Optional) ---
        # await verify_signature(request, x_hub_signature_256)

        # --- Parse JSON Payload ---
        raw_body = await request.body()
        try:
            payload = json.loads(raw_body)
        except json.JSONDecodeError:
            logger.error("Invalid JSON in GitHub webhook payload")
            raise HTTPException(status_code=400, detail="Invalid JSON")

        # --- Event Handling ---
        if x_github_event == "ping":
            logger.info("Received 'ping' event from GitHub. Responding OK.")
            return {"message": "Pong!"}

        if x_github_event != "push":
            logger.debug(f"Ignoring non-'push' event: {x_github_event}")
            return {"message": f"Ignoring event type: {x_github_event}"}

        logger.info(f"Processing 'push' event: {payload}")

        # --- Process 'push' Event ---
        repo_info = payload.get("repository", {})
        repo_full_name = repo_info.get("full_name")
        repo_owner = repo_info.get("owner", {}).get("login") or repo_info.get(
            "owner", {}
        ).get("name")
        repo_name = repo_info.get("name")
        commits = payload.get("commits", [])
        head_commit = payload.get("head_commit", {})

        if not repo_full_name or not repo_owner or not repo_name:
            logger.error(f"Webhook payload missing repository details: {repo_info}")
            raise HTTPException(
                status_code=400, detail="Missing repository information in payload"
            )

        # Check if the repository is monitored
        if repo_full_name not in MONITORED_REPOS:
            logger.debug(
                f"Ignoring push event for non-monitored repository: {repo_full_name}"
            )
            return {"message": f"Repository {repo_full_name} not monitored"}

        if not commits and not head_commit:
            logger.warning(
                f"'push' event for {repo_full_name} received without 'commits' or 'head_commit' data. Possibly a branch deletion or tag push? Payload head: {payload.get('ref', '')}"
            )
            return {"message": "No commit data found in push event"}

        # Determine the commit(s) to process and the SHA to potentially update state with
        commits_to_process = []
        sha_to_update_state = None

        head_commit_id = head_commit.get("id")
        if head_commit_id:
            commits_to_process = [head_commit]  # Use head_commit as the primary source
            sha_to_update_state = (
                head_commit_id  # This is the SHA representing the push tip
            )
            logger.debug(f"Processing head_commit: {head_commit_id}")
        elif commits:
            commits_to_process = commits  # Fallback to commits list
            # If using commits list, the last commit's SHA is the best candidate for state update
            if commits:
                sha_to_update_state = commits[-1].get("id")
            logger.debug(
                f"Processing commits list (count: {len(commits)}). Potential state update SHA: {sha_to_update_state}"
            )
        else:
            # This case should ideally not be reached due to the check above, but included for completeness
            logger.warning(
                f"No processable commit data found in push event for {repo_full_name}. Skipping."
            )
            return {"message": "No processable commit data"}

        processed_new_commit = False
        for commit in commits_to_process:
            commit_sha = commit.get("id")
            if not commit_sha:
                logger.warning("Skipping commit in payload with missing 'id'.")
                continue

            # Avoid reprocessing the last known SHA for this repo
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if last_known_sha_for_repo and commit_sha == last_known_sha_for_repo:
                logger.debug(
                    f"Skipping commit {commit_sha} for {repo_full_name} as it matches last known SHA {last_known_sha_for_repo}."
                )
                continue

            # Inner try-except for processing individual commits within the push
            try:
                # Construct RID
                rid = GithubCommit(owner=repo_owner, repo=repo_name, sha=commit_sha)

                # Extract details - ensure keys exist
                author = commit.get("author", {})
                committer = commit.get("committer", {})

                contents = {
                    "sha": commit_sha,
                    "message": commit.get("message"),
                    "author_name": author.get("name"),
                    "author_email": author.get("email"),
                    "author_date": commit.get(
                        "timestamp"
                    ),  # GitHub often uses 'timestamp'
                    "committer_name": committer.get("name"),
                    "committer_email": committer.get("email"),
                    "committer_date": committer.get("timestamp"),
                    "html_url": commit.get("url"),  # Use 'url' from webhook payload
                    "parents": commit.get(
                        "parents", []
                    ),  # Typically a list of SHAs in webhook
                }

                bundle = Bundle.generate(rid=rid, contents=contents)
                # CORRECT USAGE: 'handle' makes the bundle available locally
                # in the sensor's cache/event queue for consumers to poll/fetch.
                # It does NOT push the application-specific GithubCommit bundle directly.
                logger.debug(
                    f"Making webhook commit bundle {rid} available locally via sensor API."
                )
                node.processor.handle(bundle=bundle)

                processed_new_commit = (
                    True  # Mark that we processed at least one new commit
                )

            except Exception as e:
                logger.error(
                    f"Error processing webhook commit {commit_sha} for {repo_full_name}: {e}",
                    exc_info=True,
                )
                # Decide whether to continue processing other commits in the push or stop
                continue  # Continue with next commit in the webhook push

        # Update state file only if we processed a new commit and have a valid SHA representing the push tip
        if processed_new_commit and sha_to_update_state:
            # Check again if the sha_to_update is different from the stored one before writing
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if sha_to_update_state != last_known_sha_for_repo:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. Updating state to SHA: {sha_to_update_state}"
                )
                update_state_file(repo_full_name, sha_to_update_state)
            else:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. State SHA {sha_to_update_state} already stored."
                )
        elif processed_new_commit:
            logger.warning(
                f"Webhook processing complete for {repo_full_name}. Processed new commit(s) but could not determine SHA for state update."
            )
        else:
            logger.info(
                f"Webhook processing complete for {repo_full_name}. No new commits processed or state updated."
            )

        return {"message": "Webhook processed successfully"}

    # Exception handlers are now correctly indented relative to the main 'try' block
    except HTTPException as he:
        # Re-raise HTTP exceptions to return proper status codes
        logger.warning(f"HTTP Exception during webhook processing: {he.detail}")
        raise he
    except Exception as e:
        logger.error(
            f"Unexpected error handling webhook event {x_github_event}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Internal server error handling webhook"
        )
</file>

<file path=".env.example">
# KOI-net GitHub Node Configuration

# Public URL where this GitHub node can be reached by other KOI-net nodes
URL="http://127.0.0.1:8001"

# URL of the coordinator node or another known KOI-net node
FIRST_CONTACT="http://127.0.0.1:8000/koi-net"

# Host the FastAPI server should listen on (0.0.0.0 allows external connections)
HOST="0.0.0.0"

# Port the FastAPI server should listen on
PORT="8001"

# GitHub Personal Access Token (replace with your own token)
GITHUB_TOKEN="your_github_token_here"

# Secret used to verify webhook signatures (generate a secure random string)
GITHUB_WEBHOOK_SECRET="your_webhook_secret_here"

# Comma-separated list of repositories to monitor (format: owner/repo)
MONITORED_REPOS="owner/repo1,owner/repo2"

# Path to the JSON file storing the last processed commit SHA
STATE_FILE_PATH="state.json"

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL="INFO"
</file>

<file path=".gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
</file>

<file path="Dockerfile">
FROM python:3.12-slim

# Install UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# Copy dependency files first for better caching
COPY pyproject.toml /app/

# Install dependencies using UV
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# Copy the application code
COPY . /app/

# Expose the correct KOI port
EXPOSE 8001

# Add correct HEALTHCHECK instruction
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8001/koi-net/health || exit 1

# Start service with correct entrypoint and port
CMD ["uvicorn", "github_sensor_node.server:app", "--host", "0.0.0.0", "--port", "8001"]
</file>

<file path="github-node.service">
[Unit]
Description=KOI-net Github Node Service
After=network.target

[Service]
WorkingDirectory=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/services/github
ExecStart=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/.venv/bin/python3 -m node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "github-sensor-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.3",
    "koi-net==1.0.0b12",
    "PyGithub",
    "aiohttp",
    "rich", # Added for logging
    "ruamel.yaml", # Added for YAML config loading
    "python-dotenv" # Added for loading .env in local runs
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

</files>
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path

# Import LOG_LEVEL from refactored config
from .config import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/hackmd")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "hackmd-sensor-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
logging.getLogger("uvicorn.error").setLevel(logging.WARNING)

logger.info(
    f"Logging configured (Level: {LOG_LEVEL}). Console via Rich, File: {log_file_path}"
)

# Logging level is now set globally based on config,
# specific logger level adjustments can happen in config.py
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/__main__.py">
import uvicorn
import asyncio
import logging
from .backfill import perform_backfill
from .config import HOST, PORT, POLL_INTERVAL, polling_state, save_polling_state
from .core import node
import threading
import time

logger = logging.getLogger(__name__)

stop_event = threading.Event()


def poll_hackmd():
    """Periodically polls HackMD using the backfill logic."""
    while not stop_event.is_set():
        logger.info("Polling HackMD...")
        try:
            perform_backfill(polling_state)
            save_polling_state()
        except Exception as e:
            logger.error(f"Error during HackMD polling loop: {e}", exc_info=True)

        # Wait for the configured interval or until stop event is set
        stop_event.wait(POLL_INTERVAL)


if __name__ == "__main__":
    logger.info(f"HackMD sensor node starting on {HOST}:{PORT}")
    logger.info(f"Polling HackMD every {POLL_INTERVAL} seconds.")

    # Start the KOI node processing thread
    node.start()

    # Start the polling thread
    polling_thread = threading.Thread(target=poll_hackmd, daemon=True)
    polling_thread.start()

    try:
        # Run the FastAPI server
        uvicorn.run(
            "hackmd_sensor_node.server:app",
            # Use HOST and PORT from config
            host=HOST,
            port=PORT,
            log_config=None,
            reload=True,
        )
    finally:
        # Signal threads to stop and wait for them
        logger.info("Shutting down HackMD sensor...")
        stop_event.set()
        polling_thread.join()
        node.stop()
        logger.info("HackMD sensor shut down gracefully.")
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/backfill.py">
import logging
from datetime import datetime, timedelta
from .hackmd_api import get_team_notes, get_note_details
from rid_types import HackMDNote
from rid_lib.ext import Bundle
from .core import node

# Import TEAM_PATH and the new TARGET_NOTE_IDS from config
from .config import TEAM_PATH, TARGET_NOTE_IDS

logger = logging.getLogger(__name__)


# Define the type for the state dictionary for clarity
StateType = dict[str, str]  # Maps note_id to last_modified_timestamp


def perform_backfill(state: StateType):
    """Fetches all notes for the configured team, compares with state, and bundles new/updated notes."""
    logger.info(f"Starting HackMD backfill for team path: '{TEAM_PATH}'")
    if not TEAM_PATH:
        logger.error("HackMD team path is not configured. Backfill skipped.")
        return

    try:
        processed_count = 0
        bundled_count = 0

        # Decide whether to process specific notes or all team notes
        if TARGET_NOTE_IDS:
            logger.info(
                f"Targeting specific HackMD notes for backfill: {TARGET_NOTE_IDS}"
            )
            # Process only specified notes
            for note_id in TARGET_NOTE_IDS:
                processed_count += 1
                logger.debug(f"Fetching targeted note ID: {note_id}")
                # Fetch the full note details, including content and metadata
                note_details = get_note_details(note_id)
                if not note_details:
                    logger.warning(
                        f"Could not fetch details for targeted note ID {note_id}. Skipping."
                    )
                    continue

                last_modified_str = note_details.get("lastChangedAt")
                title = note_details.get(
                    "title", f"Note {note_id}"
                )  # Use ID if title missing

                if not last_modified_str:
                    logger.warning(
                        f"Skipping targeted note {note_id} due to missing lastChangedAt."
                    )
                    continue

                # Check state for this specific note
                if note_id not in state or last_modified_str > state[note_id]:
                    logger.info(
                        f"Processing targeted note '{title}' (ID: {note_id}) - New or updated."
                    )
                    # Content is already part of note_details if get_note_details fetches everything
                    note_content = note_details.get("content")
                    if note_content is None:
                        logger.error(
                            f"Content missing for note ID {note_id} even after fetch. Skipping."
                        )
                        continue

                    # Bundle the note (logic similar to below, adapted for note_details)
                    try:
                        rid = HackMDNote(note_id=note_id)
                        contents = {
                            "id": note_id,
                            "title": title,
                            "content": note_content,
                            "createdAt": note_details.get("createdAt"),
                            "lastChangedAt": last_modified_str,
                            "publishLink": note_details.get("publishLink"),
                            "tags": note_details.get("tags", []),
                        }
                        bundle = Bundle.generate(rid=rid, contents=contents)
                        logger.debug(
                            f"Making backfill targeted note bundle {rid} available locally."
                        )
                        node.processor.handle(bundle=bundle)
                        bundled_count += 1
                        state[note_id] = last_modified_str  # Update state
                    except Exception as e:
                        logger.error(
                            f"Error bundling targeted note {note_id}: {e}",
                            exc_info=True,
                        )
                else:
                    logger.debug(
                        f"Skipping targeted note '{title}' (ID: {note_id}) - Already up-to-date."
                    )

        else:
            # Original logic: process all notes in the team
            logger.info(f"Processing all notes in team path: '{TEAM_PATH}'")
            team_notes = get_team_notes(TEAM_PATH)
            if not team_notes:
                logger.warning(
                    f"No notes found or error fetching notes for team '{TEAM_PATH}'. Backfill ending."
                )
                return

            for note_summary in team_notes:
                processed_count += 1
                note_id = note_summary.get("id")
                last_modified_str = note_summary.get("lastChangedAt")
                title = note_summary.get("title")

                if not note_id or not last_modified_str:
                    logger.warning(
                        f"Skipping note from team list due to missing ID or lastChangedAt: {note_summary}"
                    )
                    continue

                # Check if note needs processing based on state
                if note_id not in state or last_modified_str > state[note_id]:
                    logger.info(
                        f"Processing note '{title}' (ID: {note_id}) from team list - New or updated."
                    )
                    # Fetch full content only when needed
                    note_details = get_note_details(
                        note_id
                    )  # Use get_note_details to get full details
                    if note_details is None or note_details.get("content") is None:
                        logger.error(
                            f"Failed to fetch content/details for note ID {note_id} from team list. Skipping."
                        )
                        continue
                    note_content = note_details.get("content")

                    try:
                        rid = HackMDNote(note_id=note_id)
                        contents = {
                            "id": note_id,
                            "title": title,
                            "content": note_content,
                            "createdAt": note_summary.get(
                                "createdAt"
                            ),  # Use summary data where possible
                            "lastChangedAt": last_modified_str,
                            "publishLink": note_summary.get("publishLink"),
                            "tags": note_summary.get("tags", []),
                        }
                        bundle = Bundle.generate(rid=rid, contents=contents)
                        logger.debug(
                            f"Making backfill note bundle {rid} from team list available locally."
                        )
                        node.processor.handle(bundle=bundle)
                        bundled_count += 1
                        state[note_id] = last_modified_str  # Update state

                    except Exception as e:
                        logger.error(
                            f"Error creating/handling bundle for note {note_id} from team list: {e}",
                            exc_info=True,
                        )
                else:
                    logger.debug(
                        f"Skipping note '{title}' (ID: {note_id}) from team list - Already up-to-date."
                    )

        logger.info(
            f"HackMD backfill complete. Processed {processed_count} notes, bundled {bundled_count} new/updated notes."
        )

    except Exception as e:
        logger.error(f"Unexpected error during HackMD backfill: {e}", exc_info=True)


# Note: The state persistence (load/save) should be handled by the caller
# (e.g., the polling mechanism in server.py or __main__.py)

if __name__ == "__main__":
    # Example usage (requires node setup and async context)
    # import asyncio
    # node.start()
    # asyncio.run(perform_backfill({}))
    # node.stop()
    pass  # Keep placeholder for potential direct execution
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/hackmd_api.py">
import asyncio
import httpx
import logging
import requests
from .config import HACKMD_API_TOKEN

logger = logging.getLogger(__name__)

api_base_url = "https://api.hackmd.io/v1"


def request(path, method="GET"):
    resp = httpx.request(
        method=method,
        url=api_base_url + path,
        headers={"Authorization": "Bearer " + HACKMD_API_TOKEN},
    )

    if resp.status_code == 200:
        return resp.json()

    else:
        print(resp.status_code, resp.text)
        return


async def async_request(path, method="GET"):
    timeout = 60

    while True:
        async with httpx.AsyncClient() as client:

            resp = await client.request(
                method=method,
                url=api_base_url + path,
                headers={"Authorization": "Bearer " + HACKMD_API_TOKEN},
            )

        if resp.status_code == 200:
            return resp.json()

        elif resp.status_code == 429:
            print(resp.status_code, resp.text, f"retrying in {timeout} seconds")
            await asyncio.sleep(timeout)
            timeout *= 2
        else:
            print(resp.status_code, resp.text)
            return


def get_team_notes(team_path: str):
    """Fetch notes for a given HackMD team path."""
    # Check if token is available
    if not HACKMD_API_TOKEN:
        logger.error("HackMD API token is not configured. Cannot fetch team notes.")
        return []

    headers = {
        "Authorization": f"Bearer {HACKMD_API_TOKEN}",
    }
    try:
        # Use f-string for URL formatting
        response = requests.get(
            f"{api_base_url}/teams/{team_path}/notes", headers=headers
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        notes = response.json()
        logger.info(f"Successfully fetched {len(notes)} notes for team '{team_path}'")
        return notes
    except requests.exceptions.RequestException as e:
        logger.error(
            f"Error fetching HackMD notes for team '{team_path}': {e}", exc_info=True
        )
        return []
    except Exception as e:
        logger.error(f"Unexpected error fetching HackMD notes: {e}", exc_info=True)
        return []


def get_note_details(note_id: str):
    """Fetch the full details (including content) of a specific HackMD note by ID."""
    # Check if token is available
    if not HACKMD_API_TOKEN:
        logger.error("HackMD API token is not configured. Cannot fetch note details.")
        return None

    headers = {
        "Authorization": f"Bearer {HACKMD_API_TOKEN}",
    }
    try:
        # Use f-string for URL formatting
        response = requests.get(f"{api_base_url}/notes/{note_id}", headers=headers)
        response.raise_for_status()
        note_details = response.json()
        logger.debug(f"Successfully fetched details for note ID '{note_id}'")
        return note_details  # Return the full dictionary
    except requests.exceptions.RequestException as e:
        logger.error(f"Error fetching HackMD note details for ID '{note_id}': {e}")
        return None
    except Exception as e:
        logger.error(
            f"Unexpected error fetching HackMD note details: {e}", exc_info=True
        )
        return None
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/handlers.py">
import logging
from multiprocessing import process
from koi_net.processor.handler import HandlerType, STOP_CHAIN
from koi_net.processor.knowledge_object import KnowledgeSource, KnowledgeObject
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.ext import Bundle
from rid_lib.types import KoiNetNode

from rid_types import HackMDNote
from .core import node
from . import hackmd_api

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    # when I found out about a new node
    if kobj.normalized_event_type != EventType.NEW:
        return

    node_profile = kobj.bundle.validate_contents(NodeProfile)

    # looking for event provider of nodes
    if KoiNetNode not in node_profile.provides.event:
        return

    logger.debug("Identified a coordinator!")
    logger.debug("Proposing new edge")

    # queued for processing
    processor.handle(
        bundle=generate_edge_bundle(
            source=kobj.rid,
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode],
        )
    )

    logger.debug("Catching up on network state")

    rid_payload = processor.network.request_handler.fetch_rids(
        kobj.rid, rid_types=[KoiNetNode]
    )

    rids = [
        rid
        for rid in rid_payload.rids
        if rid != processor.identity.rid and not processor.cache.exists(rid)
    ]

    bundle_payload = processor.network.request_handler.fetch_bundles(
        kobj.rid, rids=rids
    )

    for bundle in bundle_payload.bundles:
        # marked as external since we are handling RIDs from another node
        # will fetch remotely instead of checking local cache
        processor.handle(bundle=bundle, source=KnowledgeSource.External)
    logger.debug("Done")


@node.processor.register_handler(HandlerType.Manifest)
def custom_manifest_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    if type(kobj.rid) == HackMDNote:
        logger.debug("Skipping HackMD note manifest handling")
        return

    prev_bundle = processor.cache.read(kobj.rid)

    if prev_bundle:
        if kobj.manifest.sha256_hash == prev_bundle.manifest.sha256_hash:
            logger.debug(
                "Hash of incoming manifest is same as existing knowledge, ignoring"
            )
            return STOP_CHAIN
        if kobj.manifest.timestamp <= prev_bundle.manifest.timestamp:
            logger.debug(
                "Timestamp of incoming manifest is the same or older than existing knowledge, ignoring"
            )
            return STOP_CHAIN

        logger.debug("RID previously known to me, labeling as 'UPDATE'")
        kobj.normalized_event_type = EventType.UPDATE

    else:
        logger.debug("RID previously unknown to me, labeling as 'NEW'")
        kobj.normalized_event_type = EventType.NEW

    return kobj


@node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
def custom_hackmd_bundle_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    assert type(kobj.rid) == HackMDNote

    prev_bundle = processor.cache.read(kobj.rid)

    if prev_bundle:
        prevChangedAt = prev_bundle.contents["lastChangedAt"]
        currChangedAt = kobj.contents["lastChangedAt"]
        logger.debug(f"Changed at {prevChangedAt} -> {currChangedAt}")
        if currChangedAt > prevChangedAt:
            logger.debug("Incoming note has been changed more recently!")
            kobj.normalized_event_type = EventType.UPDATE

        else:
            logger.debug("Incoming note is not newer")
            return STOP_CHAIN

    else:
        logger.debug("Incoming note is previously unknown to me")
        kobj.normalized_event_type = EventType.NEW

    logger.debug("Retrieving full note...")
    data = hackmd_api.request(f"/notes/{kobj.rid.note_id}")

    if not data:
        logger.debug("Failed.")
        return STOP_CHAIN

    logger.debug("Done.")

    full_note_bundle = Bundle.generate(rid=kobj.rid, contents=data)

    kobj.manifest = full_note_bundle.manifest
    kobj.contents = full_note_bundle.contents

    return kobj
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/server.py">
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node
from .backfill import perform_backfill


logger = logging.getLogger(__name__)


async def backfill_loop():
    while True:
        await asyncio.sleep(600)


@asynccontextmanager
async def lifespan(app: FastAPI):
    node.start()

    yield
    node.stop()


app = FastAPI(lifespan=lifespan, title="KOI-net Protocol API", version="1.0.0")


koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
def broadcast_events(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        logger.info(f"{event!r}")
        node.processor.handle(event=event, source=KnowledgeSource.External)


@koi_net_router.post(POLL_EVENTS_PATH)
def poll_events(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@koi_net_router.post(FETCH_RIDS_PATH)
def fetch_rids(req: FetchRids) -> RidsPayload:
    return node.network.response_handler.fetch_rids(req)


@koi_net_router.post(FETCH_MANIFESTS_PATH)
def fetch_manifests(req: FetchManifests) -> ManifestsPayload:
    return node.network.response_handler.fetch_manifests(req)


@koi_net_router.post(FETCH_BUNDLES_PATH)
def fetch_bundles(req: FetchBundles) -> BundlesPayload:
    return node.network.response_handler.fetch_bundles(req)


# Add health endpoint
@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed
    return {"status": "healthy"}


app.include_router(koi_net_router)
</file>

<file path="nodes/koi-net-hackmd-sensor-node/.gitignore">
rid_cache
identity.json
events_queue.json
venv
.env
*.json
__pycache__
</file>

<file path="nodes/koi-net-hackmd-sensor-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy pyproject and perform editable installation
COPY pyproject.toml /app/

# 3) install curl for healthcheck
RUN apt-get update && apt-get install -y curl

# 4) install dependencies using UV and pyproject.toml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 5) copy in code
COPY . /app/

# 6) expose port and set environment
EXPOSE 8002

# 7) add healthcheck using the main app's endpoint
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8002/koi-net/health || exit 1

# 8) start the main service using shell form for ENV var substitution
CMD ["uvicorn", "hackmd_sensor_node.server:app", "--host", "0.0.0.0", "--port", "8002"]
</file>

<file path="nodes/koi-net-hackmd-sensor-node/koi-net-hackmd-sensor-node.service">
[Unit]
Description=KOI-net HackMD Sensor Node Service
After=network.target

[Service]
WorkingDirectory=/home/dev/koi-net-hackmd-sensor-node
ExecStart=/home/dev/koi-net-hackmd-sensor-node/venv/bin/python3 -m hackmd_sensor_node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="nodes/koi-net-hackmd-sensor-node/LICENSE">
MIT License

Copyright (c) 2025 BlockScience

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="nodes/koi-net-hackmd-sensor-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "koi-net-hackmd-sensor-node"
version = "0.1.0"
dependencies = [
    "koi-net==1.0.0b12",
    "rid-lib>=3.2.3",
    "rich",
    "fastapi",
    "uvicorn",
    "python-dotenv",
    "requests",
    "ruamel.yaml",
    "python-dotenv"
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="nodes/koi-net-hackmd-sensor-node/README.md">
# koi-net-hackmd-sensor-node
 HackMD sensor node implementation for BlockScience's KOI-net
</file>

<file path="nodes/koi-net-hackmd-sensor-node/requirements.txt">
koi-net==1.0.0b12
rid-lib>=3.2.3
rich
fastapi
uvicorn
python-dotenv
requests
ruamel.yaml
</file>

<file path="nodes/koi-net-hackmd-sensor-node/rid_types.py">
from rid_lib.core import ORN


class HackMDNote(ORN):
    namespace = "hackmd.note"

    def __init__(self, note_id: str):
        self.note_id = note_id

    @property
    def reference(self):
        return self.note_id

    @classmethod
    def from_reference(cls, reference):
        return cls(reference)
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path

# Import LOG_LEVEL from config (will be defined there)
# from .config import LOG_LEVEL

# Temporary log level until config is implemented
TEMP_LOG_LEVEL = "INFO"

# Get the root logger
logger = logging.getLogger()
# Set the base level
logger.setLevel(TEMP_LOG_LEVEL)

# Create Rich Handler for console
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(TEMP_LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler
log_dir = Path(".koi/processor-a") # Adjust node name
log_dir.mkdir(parents=True, exist_ok=True)
log_file_path = log_dir / "processor-a-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)
file_handler.setLevel(TEMP_LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries later in config.py
# logging.getLogger("uvicorn.error").setLevel(logging.WARNING)

logger.info(
    f"Logging configured (Level: {TEMP_LOG_LEVEL}). Console via Rich, File: {log_file_path}"
)
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/__main__.py">
import uvicorn
import logging
# Import HOST and PORT from config (will be defined there)
# from .config import HOST, PORT

# Temporary values until config is implemented
TEMP_HOST = "0.0.0.0"
TEMP_PORT = 8011 # Default port for Processor A

logger = logging.getLogger(__name__)

logger.info(f"Processor A node starting on {TEMP_HOST}:{TEMP_PORT}")
uvicorn.run(
    "processor_a_node.server:app", # Adjust app path
    host=TEMP_HOST,
    port=TEMP_PORT,
    log_config=None,
    reload=True, # Enable reload for development
)
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/config.py">
import logging
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import Dict, Any

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker --- 
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
    # In Docker, ensure KOI_CONFIG_MODE is set to docker if RUN_CONTEXT is docker
    if CONFIG_MODE != "docker":
        logger.warning(f"RUN_CONTEXT=docker but KOI_CONFIG_MODE='{CONFIG_MODE}'. Forcing KOI_CONFIG_MODE to 'docker'.")
        CONFIG_MODE = "docker"
else:
    # Assume local run, base path relative to this file's parent's parent
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

CONFIG_PATH = CONFIG_DIR / "processor-a.yaml" # Specific config file
ENV_PATH = CONFIG_DIR / "global.env"

logger.info(f"Attempting to load config from: {CONFIG_PATH}")
logger.info(f"Attempting to load env from: {ENV_PATH}")

# Load env vars from the selected global.env first
if ENV_PATH.is_file():
    from dotenv import load_dotenv
    load_dotenv(dotenv_path=ENV_PATH, override=True)
    logger.info(f"Loaded environment variables from {ENV_PATH}")
else:
    logger.warning(f"Global environment file not found at {ENV_PATH}")

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None: # Handle empty file case
            CONFIG = {}
        logger.info(f"Successfully loaded YAML config from {CONFIG_PATH}")
    except Exception as e:
        logger.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logger.error(f"Processor A config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Determine Run Context & Extract Settings --- 
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"

RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
PROCESSOR_A_CONFIG: Dict[str, Any] = CONFIG.get("processor_a", {}) # Processor specific settings

# --- Context-Aware Configuration --- 
LOCAL_DATA_BASE = Path("./.koi/processor-a") # Standard local path base
DOCKER_CACHE_DIR_DEFAULT = "/data/cache" # Default for shared cache in Docker

# Base configuration values (URLs are now directly from the correct mode's YAML)
HOST: str = RUNTIME_CONFIG.get("host", "127.0.0.1" if not is_docker else "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8011)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()
BASE_URL = RUNTIME_CONFIG.get("base_url") # Should be defined in yaml
COORDINATOR_URL = EDGES_CONFIG.get("coordinator_url") # Should be defined in yaml

# Optional specific sensor RID
GITHUB_SENSOR_RID: str | None = PROCESSOR_A_CONFIG.get("github_sensor_rid")

# Determine Cache Dir
if is_docker:
    CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", DOCKER_CACHE_DIR_DEFAULT)
else:
    # For local, use the config value or construct relative path
    cache_dir_config = RUNTIME_CONFIG.get("cache_dir")
    if cache_dir_config:
        CACHE_DIR = str(Path(cache_dir_config)) # Respect config if set
    else:
        LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
        CACHE_DIR = str(LOCAL_DATA_BASE / "cache") # Fallback local path relative to node state

# Ensure the resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Update Logging Level Based on Config --- 
try:
    logging.getLogger().setLevel(LOG_LEVEL.upper())
    # Set level for uvicorn access logs specifically if needed
    # logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO.")
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("Processor A Configuration Loaded:")
logger.info(f"  Config Mode: {CONFIG_MODE}")
logger.info(f"  Is Docker Context: {is_docker}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Specific GitHub Sensor RID: {GITHUB_SENSOR_RID or 'Not Set'}")

# Check required config
if not BASE_URL:
    logger.critical("Configuration error: runtime.base_url is not set.")
if not COORDINATOR_URL:
    logger.critical("Configuration error: edges.coordinator_url is not set.")
if not CACHE_DIR:
     logger.critical("Configuration error: runtime.cache_dir is not set or resolved correctly.")
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/core.py">
import os
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the final config loader
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
) 

# Import the RID type this processor consumes
logger = logging.getLogger(__name__) 
try:
    # Attempt import - adjust path if project structure changes
    # This path assumes nodes/ are sibling directories in the project root
    from nodes.koi_net_github_sensor_node.github_sensor_node.types import GithubCommit
except ImportError as e:
    logger.error(f"Failed to import GithubCommit: {e}. Ensure sensor node is accessible or use shared RID definitions. Using placeholder.")
    # Define a placeholder if import fails during scaffolding
    class GithubCommit:
        pass 

name = "processor-a"

# Identity directory setup - relative to where the node is run
identity_dir = f".koi/{name}"
os.makedirs(identity_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Check required config values (basic check)
if not BASE_URL or not COORDINATOR_URL or not CACHE_DIR:
    # Config loader already logs critical errors, this is an extra safeguard
    raise ValueError("Essential configuration (BASE_URL, COORDINATOR_URL, CACHE_DIR) is missing or failed to load!")

# Initialize the KOI-net Node Interface for Processor A
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides( # Processor A provides no new RIDs per PRD
            event=[], 
            state=[]
        ),
        # Consumes GithubCommit implicitly via handlers
    ),
    use_kobj_processor_thread=True, # Run processing in a separate thread
    first_contact=COORDINATOR_URL,
    # Use absolute paths for state files to avoid ambiguity
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR, # Use the resolved cache directory path
)

logger.info(f"Initialized NodeInterface for Processor A: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")

# Import handlers after node is initialized to allow decorator registration
from . import handlers
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/handlers.py">
import logging

from .core import node
from koi_net.processor import ProcessorInterface, HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode, KoiNetEdge
from rid_lib.core import ORN

# Import config to potentially check for specific sensor RID
from .config import GITHUB_SENSOR_RID 



logger = logging.getLogger(__name__)

# Simple in-memory index (as defined in Processor.md)
# Structure: { sha: commit_message, keyword: [sha1, sha2, ...], ... }
search_index = {}

# --- Network Handlers --- 

@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of Coordinator and potential GitHub Sensor nodes."""
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    # Basic validation of the discovered node's profile
    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if not profile or not profile.provides:
             logger.warning(f"Received KoiNetNode event for {kobj.rid} with invalid/missing profile.")
             return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    # --- Coordinator Handshake --- 
    # Assume any node providing KoiNetNode *could* be a coordinator/peer
    # Propose edge back for bidirectional comms if it's not ourselves
    if kobj.rid != processor.identity.rid:
        logger.info(f"Discovered potential peer/coordinator: {kobj.rid}. Proposing edge.")
        try:
            edge_bundle = generate_edge_bundle(
                source=kobj.rid, # The discovered node is the source
                target=processor.identity.rid, # We are the target
                edge_type=EdgeType.WEBHOOK,
                rid_types=[KoiNetNode, KoiNetEdge], # What we expect to exchange
            )
            processor.handle(bundle=edge_bundle)
        except Exception as e:
            logger.error(f"Failed edge proposal to {kobj.rid}: {e}", exc_info=True)

    # --- GitHub Sensor Discovery & Handshake ---
    # Check if the discovered node provides the RIDs we need (GithubCommit)
    # Note: Using string comparison since we don't have the actual RID class
    provides_github_commits = False
    
    # Use string comparison for RID type detection
    rid_type_str = "orn:github.commit"
    provides_event = [str(rt) for rt in profile.provides.event if hasattr(profile.provides, 'event')]
    provides_state = [str(rt) for rt in profile.provides.state if hasattr(profile.provides, 'state')]
    
    if rid_type_str in provides_event or rid_type_str in provides_state:
        provides_github_commits = True

    if provides_github_commits:
        # Check if a specific sensor RID is configured
        if GITHUB_SENSOR_RID and str(kobj.rid) != GITHUB_SENSOR_RID:
            logger.debug(f"Discovered GitHub sensor {kobj.rid}, but configured to connect only to {GITHUB_SENSOR_RID}. Ignoring.")
            return

        logger.info(f"Discovered GitHub Sensor: {kobj.rid}. Proposing edge.")
        try:
            # Propose an edge TO the sensor to receive events
            edge_bundle = generate_edge_bundle(
                source=kobj.rid, # Sensor is the source of events
                target=processor.identity.rid, # We are the target
                edge_type=EdgeType.WEBHOOK,
                rid_types=[GithubCommit], # Specify we want GithubCommit events
            )
            processor.handle(bundle=edge_bundle)
            logger.info(f"Edge proposed to GitHub Sensor {kobj.rid}")
        except Exception as e:
            logger.error(f"Failed edge proposal to GitHub Sensor {kobj.rid}: {e}", exc_info=True)

# --- Manifest Handler --- 

@node.processor.register_handler(HandlerType.Manifest, rid_types=[GithubCommit])
def handle_commit_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles incoming commit manifests, triggers bundle fetch for indexing."""
    # Check if we can work with the RID
    try:
        rid = kobj.rid
        if not hasattr(rid, 'reference'):
            logger.warning(f"Handler received RID without reference attribute: {rid}. Skipping.")
            return
    except Exception as e:
        logger.warning(f"Error checking RID {kobj.rid}: {e}")
        return

    manifest = kobj.manifest
    logger.info(f"Received manifest for commit: {rid.reference}")

    # PRD: Optionally dereference bundles. For indexing, we need the content.
    # Decision: Always dereference for this implementation.
    logger.debug(f"Requesting bundle for {rid} for indexing.")
    try:
        processor.handle(rid=rid, source=KnowledgeSource.External) # Trigger bundle fetch
    except Exception as e:
        logger.error(f"Error requesting bundle for {rid}: {e}", exc_info=True)

# --- Bundle Handler --- 

@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Processes commit bundle contents and updates the search index."""
    # Check if we can work with the RID
    try:
        rid = kobj.rid
        if not hasattr(rid, 'reference'):
            logger.warning(f"Handler received RID without reference attribute: {rid}. Skipping.")
            return
    except Exception as e:
        logger.warning(f"Error checking RID {kobj.rid}: {e}")
        return

    if not kobj.contents or not isinstance(kobj.contents, dict):
        logger.warning(f"Bundle for {kobj.rid} has no contents or invalid format.")
        return

    contents = kobj.contents
    sha = contents.get("sha")
    if not sha:
        logger.warning(f"Commit bundle {kobj.rid} missing SHA in contents. Skipping index update.")
        return
        
    logger.info(f"Processing bundle for commit: {sha[:7]}")

    # --- Update Search Index (Example Implementation) ---
    message = contents.get("message", "")
    
    # Clear old keyword associations for this SHA first if re-indexing
    for keyword, sha_list in list(search_index.items()):
        if isinstance(sha_list, list) and sha in sha_list:
            search_index[keyword].remove(sha)
            if not search_index[keyword]: # Remove keyword if list becomes empty
                del search_index[keyword]
                
    # Index by full SHA
    search_index[sha] = message 
    
    # Index by keywords (simple example) 
    # Consider stemming, stop words, etc. for a real implementation
    keywords_processed = set()
    for keyword in message.lower().split(): 
        # Basic filtering: length > 3, alphanumeric, avoid duplicates per message
        if len(keyword) > 3 and keyword.isalnum() and keyword not in keywords_processed:
            if keyword not in search_index:
                search_index[keyword] = []
            # Add SHA to keyword list if not already present
            if sha not in search_index[keyword]:
                 search_index[keyword].append(sha)
            keywords_processed.add(keyword)
            
    logger.debug(f"Updated search index for SHA: {sha[:7]}. Index size (keys): {len(search_index)}")

# --- Helper for Search Endpoint --- 
def query_search_index(query: str) -> list:
    """Queries the in-memory search index."""
    results = []
    query_lower = query.lower()
    
    # 1. Check if query is a SHA (full or partial >= 7 chars)
    if len(query) >= 7:
        # Check full SHA match
        if query in search_index and isinstance(search_index[query], str): 
            # Need owner/repo to construct full RID - requires better index storage
            # For now, return SHA and message
            results.append({
                "sha": query, 
                "match_context": search_index[query][:100] + "..." # Truncate message
            })
            return results # Exact SHA match takes precedence
            
        # Check partial SHA match (less efficient)
        for sha_key, msg in search_index.items():
             # Ensure it's a SHA key (check length or type if index structure is mixed)
             if isinstance(msg, str) and sha_key.startswith(query): 
                 results.append({
                     "sha": sha_key, 
                     "match_context": msg[:100] + "..."
                 })
                 # Optionally break after first partial match or collect all

    # 2. Check if query is a keyword
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        for sha in search_index[query_lower]:
            # Avoid adding duplicates if already found via partial SHA match
            if not any(r["sha"] == sha for r in results):
                 message = search_index.get(sha, "") # Get message associated with SHA
                 results.append({
                     "sha": sha,
                     "match_context": message[:100] + "..."
                 })
                 
    # 3. (Optional) Search within commit messages (less efficient)
    # for sha, message in search_index.items():
    #     if isinstance(message, str) and query_lower in message.lower():
    #         # Add logic to avoid duplicates
    #         results.append(...)        

    return results

logger.info("Processor A handlers registered.")
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/server.py">
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, HTTPException

from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from koi_net.processor.knowledge_object import KnowledgeSource

from .core import node # Import the initialized node instance
# Import the query helper and index from handlers
from .handlers import query_search_index, search_index

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup and shutdown."""
    logger.info("Starting Processor A lifespan...")
    try:
        node.start()
        logger.info("Processor A KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        # Potentially exit or raise to prevent FastAPI from starting improperly
        raise RuntimeError("Failed to initialize KOI-net node") from e
    
    yield # Application runs here
    
    logger.info("Shutting down Processor A...")
    try:
        node.stop()
        logger.info("Processor A KOI-net node stopped successfully.")
    except Exception as e:
        logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
    logger.info("Processor A shutdown complete.")


app = FastAPI(
    title="KOI-net Processor A Node (Repo Indexer)",
    description="Processes GitHub commits and provides a search API.",
    version="0.1.0",
    lifespan=lifespan,
)

# --- KOI Protocol Router --- 
koi_net_router = APIRouter(prefix="/koi-net")

@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    # Basic validation
    if not req.events:
         logger.debug("Received empty broadcast event list.")
         return {}
         
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    # Asynchronously handle each event to avoid blocking the endpoint
    for event in req.events:
        try:
            # Assuming node.processor.handle is thread-safe or handles async appropriately
            node.processor.handle(event=event, source=KnowledgeSource.External)
        except Exception as e:
            logger.error(f"Error handling broadcast event {event.rid}: {e}", exc_info=True)
            # Decide if one error should stop processing others
    return {}

@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH} for {req.rid}")
    try:
        events = node.network.flush_poll_queue(req.rid)
        logger.debug(f"Returning {len(events)} events for {req.rid}")
        return EventsPayload(events=events)
    except Exception as e:
         logger.error(f"Error polling events for {req.rid}: {e}", exc_info=True)
         return EventsPayload(events=[]) # Return empty list on error

@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    try:
        return node.network.response_handler.fetch_rids(req)
    except Exception as e:
        logger.error(f"Error fetching RIDs: {e}", exc_info=True)
        # Return empty payload or raise HTTP exception?
        return RidsPayload(rids=[]) 

@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    try:
        return node.network.response_handler.fetch_manifests(req)
    except Exception as e:
        logger.error(f"Error fetching Manifests: {e}", exc_info=True)
        return ManifestsPayload(manifests=[], not_found=req.rids or [])

@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    try:
        return node.network.response_handler.fetch_bundles(req)
    except Exception as e:
        logger.error(f"Error fetching Bundles: {e}", exc_info=True)
        return BundlesPayload(bundles=[], not_found=req.rids or [])

@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint."""
    # Could add checks like node.is_running() if available
    if node and node.is_running(): # Basic check assuming is_running method exists
         return {"status": "healthy", "node_status": "running"}
    else:
         return {"status": "unhealthy", "node_status": "stopped_or_not_initialized"}

app.include_router(koi_net_router)

# --- Custom Search API Router ---
search_router = APIRouter()

@search_router.get("/search")
async def search_commits_endpoint(q: str):
    """Endpoint to search the indexed commit data."""
    if not q:
        raise HTTPException(status_code=400, detail="Query parameter 'q' is required.")
        
    logger.info(f"Search request received: q='{q}'")
    try:
        # Use the helper function from handlers
        results = query_search_index(q)
        logger.info(f"Search for '{q}' yielded {len(results)} results.")
        # Note: The current index only returns SHA and context.
        # A real implementation would likely reconstruct the full RID.
        return {"query": q, "results": results}
    except Exception as e:
        logger.error(f"Error during search for query '{q}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal server error during search.")

# Include the custom search router *without* the /koi-net prefix
app.include_router(search_router)

logger.info("Processor A FastAPI application configured with KOI and Search routers.")
</file>

<file path="nodes/koi-net-processor-a-node/.gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Config/State
*.yaml
*.env
*.json
.koi/
</file>

<file path="nodes/koi-net-processor-a-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy pyproject and perform editable installation
COPY pyproject.toml /app/

# 3) install curl for healthcheck
RUN apt-get update && apt-get install -y curl --no-install-recommends && rm -rf /var/lib/apt/lists/*

# 4) install dependencies using UV and pyproject.toml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 5) copy in code
# Copy the specific processor node code 
COPY processor_a_node /app/processor_a_node 

# 6) expose port
EXPOSE 8011 

# 7) add healthcheck using the main app's endpoint
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8011/koi-net/health || exit 1

# 8) start the main service using shell form for ENV var substitution
CMD ["uvicorn", "processor_a_node.server:app", "--host", "0.0.0.0", "--port", "8011"]
</file>

<file path="nodes/koi-net-processor-a-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "processor-a-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "httpx", 
    "python-dotenv",
    "rid-lib>=3.2.3", # Ensure compatible versions
    "koi-net==1.0.0b12", 
    "rich", 
    "ruamel.yaml",
    # Add any specific dependencies for Processor A later
]

[tool.setuptools]
package-dir = {"" = "."}
packages = ["processor_a_node"] # Specify the package
</file>

<file path="nodes/koi-net-processor-a-node/README.md">
# Processor A - GitHub Repository Indexer

## Core Logic Overview

This section provides a deep dive into Processor A's implementation details, highlighting the core components and logic that power its functionality.

### 1. Key Constants & Classes

- **`GITHUB_SENSOR_RID`**: Optional configuration parameter that allows specifying a particular GitHub sensor node to connect to, rather than discovering any available GitHub sensor.
- **`GithubCommit`**: The primary RID class that Processor A consumes, representing GitHub commit data.
- **`search_index`**: In-memory structure that maps SHA hashes and keywords to commit information.
- **`KnowledgeSource.External`**: Constants used to indicate the source of knowledge objects during processing.

### 2. How KOI & RID-lib Are Used

#### Node Interface Initialization

```python
node = NodeInterface(
    name="processor-a",
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[],
            state=[]
        ),
        # Consumes GithubCommit implicitly via handlers
    ),
    use_kobj_processor_thread=True,
    first_contact=COORDINATOR_URL,
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,
)
```

#### Registered Event Handlers

1. **Network Handler**: Processes node discovery events

   ```python
   @node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
   def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Handles discovery of Coordinator and GitHub Sensor nodes
   ```

2. **Manifest Handler**: Processes commit manifest events

   ```python
   @node.processor.register_handler(HandlerType.Manifest, rid_types=[GithubCommit])
   def handle_commit_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Handles incoming commit manifests, triggers bundle fetch
   ```

3. **Bundle Handler**: Processes commit bundle data
   ```python
   @node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
   def handle_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Processes commit bundle contents and updates the search index
   ```

#### RID-lib Usage

Processor A uses RID-lib for:

- Handling standard KOI-net RIDs (`KoiNetNode`, `KoiNetEdge`)
- Processing GitHub commit RIDs represented as `GithubCommit` classes with `reference` properties
- Automatic bundle dereferencing using the processor's built-in mechanisms

### 3. Proprietary Analysis Logic

The core analysis logic resides in the bundle handler where GitHub commits are indexed:

```python
# Extract message for keyword indexing
message = contents.get("message", "")

# Index by keywords (simple example implementation)
keywords_processed = set()
for keyword in message.lower().split():
    # Basic filtering: length > 3, alphanumeric, avoid duplicates per message
    if len(keyword) > 3 and keyword.isalnum() and keyword not in keywords_processed:
        if keyword not in search_index:
            search_index[keyword] = []
        # Add SHA to keyword list if not already present
        if sha not in search_index[keyword]:
             search_index[keyword].append(sha)
        keywords_processed.add(keyword)
```

The indexing strategy includes:

- Direct SHA-based indexing for exact match lookups
- Keyword extraction from commit messages for content-based search
- Basic filtering rules (length > 3, alphanumeric content)
- Handling of re-indexing situations for updated commits

### 4. Custom Modules & Functions

The node is implemented in a modular structure:

| Module          | Key Functions                  | Purpose                                         |
| --------------- | ------------------------------ | ----------------------------------------------- |
| **core.py**     | `NodeInterface` initialization | Establishes the node's identity and connections |
| **handlers.py** | `handle_network_discovery()`   | Detects Coordinator and GitHub sensor nodes     |
|                 | `handle_commit_manifest()`     | Processes commit manifest events                |
|                 | `handle_commit_bundle()`       | Indexes commit data from bundles                |
|                 | `query_search_index()`         | Implements search functionality                 |
| **server.py**   | `broadcast_events_endpoint()`  | Receives events from other nodes                |
|                 | `search_commits_endpoint()`    | Exposes the search API                          |

#### Search Query Implementation

```python
def query_search_index(query: str) -> list:
    """Queries the in-memory search index."""
    results = []
    query_lower = query.lower()

    # Check if query is a SHA (full or partial)
    if len(query) >= 7:
        # Check full and partial SHA matches
        # ...

    # Check if query is a keyword
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        for sha in search_index[query_lower]:
            # Add keyword matches to results
            # ...

    return results
```

### 5. Configuration & Environment Variables

Key configuration parameters include:

| Parameter           | Default                                                                           | Purpose                      |
| ------------------- | --------------------------------------------------------------------------------- | ---------------------------- |
| `BASE_URL`          | http://{host}:{port}/koi-net                                                      | The node's public endpoint   |
| `COORDINATOR_URL`   | http://coordinator:8080/koi-net (Docker)<br>http://127.0.0.1:8080/koi-net (Local) | The Coordinator node URL     |
| `CACHE_DIR`         | /data/cache (Docker)<br>./.koi/processor-a/cache (Local)                          | Location for cached data     |
| `HOST`              | 0.0.0.0 (Docker)<br>127.0.0.1 (Local)                                             | Interface to bind to         |
| `PORT`              | 8011                                                                              | HTTP server port             |
| `LOG_LEVEL`         | INFO                                                                              | Logging verbosity            |
| `GITHUB_SENSOR_RID` | None                                                                              | Optional specific sensor RID |

The configuration is loaded from YAML files in `config/docker/` or `config/local/` depending on the deployment mode, which is determined by the `KOI_CONFIG_MODE` and `RUN_CONTEXT` environment variables.

### 6. Sample Snippet: Core Processing Flow

The following code shows the complete lifecycle of processing a GitHub commit:

```python
# Step 1: Discover and connect to GitHub sensor
@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
    # Validate discovered node
    profile = kobj.bundle.validate_contents(NodeProfile)

    # Check if node provides GitHub commits
    provides_github_commits = False
    rid_type_str = "orn:github.commit"
    provides_event = [str(rt) for rt in profile.provides.event if hasattr(profile.provides, 'event')]

    if rid_type_str in provides_event:
        provides_github_commits = True

    # If it's a GitHub sensor, propose an edge to receive commits
    if provides_github_commits:
        logger.info(f"Discovered GitHub Sensor: {kobj.rid}. Proposing edge.")
        edge_bundle = generate_edge_bundle(
            source=kobj.rid,  # Sensor is the source
            target=processor.identity.rid,  # We are the target
            edge_type=EdgeType.WEBHOOK,
            rid_types=[GithubCommit],  # We want GithubCommit events
        )
        processor.handle(bundle=edge_bundle)

# Step 2: Receive commit manifest and request bundle
@node.processor.register_handler(HandlerType.Manifest, rid_types=[GithubCommit])
def handle_commit_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    rid = kobj.rid
    logger.info(f"Received manifest for commit: {rid.reference}")

    # Request full bundle for indexing
    processor.handle(rid=rid, source=KnowledgeSource.External)

# Step 3: Process commit bundle and update index
@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    contents = kobj.contents
    sha = contents.get("sha")
    logger.info(f"Processing bundle for commit: {sha[:7]}")

    # Update search index
    message = contents.get("message", "")
    search_index[sha] = message  # Index by full SHA

    # Index by keywords from commit message
    for keyword in message.lower().split():
        if len(keyword) > 3 and keyword.isalnum():
            if keyword not in search_index:
                search_index[keyword] = []
            if sha not in search_index[keyword]:
                search_index[keyword].append(sha)
```

This code demonstrates how Processor A discovers GitHub sensors, establishes connections, receives commit data, and builds its search index - all using the KOI-net protocol's event-driven architecture.

## Overview

The Processor A node is a specialized KOI-net component that consumes GitHub commit data from the GitHub sensor node, builds a searchable index, and provides a search API for querying repository information. It serves as a critical component in the KOI-net knowledge mesh by transforming raw commit data into a queryable knowledge base.

## Features

- Automatically connects to the KOI-net Coordinator and GitHub sensor nodes
- Processes GitHub commit data in real-time
- Maintains an in-memory search index for quick lookup
- Exposes a simple HTTP API for searching indexed commits
- Follows the KOI protocol for data exchange and network operations
- Supports both Docker and local deployment modes

## Architecture

### Network Integration

Processor A establishes bidirectional connections with:

1. **Coordinator Node**: For network discovery and general communication
2. **GitHub Sensor**: To receive GitHub commit events

The node uses the KOI protocol's edge mechanism to establish these connections, with handlers registering for specific Resource ID (RID) types:

```
Coordinator <---> Processor A <--- GitHub Sensor
```

### Data Processing Flow

1. **Discovery**: The network handler detects GitHub sensor nodes on the network
2. **Connection**: Automatically proposes edges to receive GitHub commit events
3. **Manifest Processing**: Receives commit manifests and requests the full bundle
4. **Indexing**: Processes commit bundles to build and maintain a search index
5. **Search API**: Provides endpoint for searching through indexed commits

### Search Index

The search index uses a simple in-memory structure:

```
{
  sha: commit_message,              # Direct SHA lookup
  keyword: [sha1, sha2, ...],       # Keyword to SHAs mapping
  ...
}
```

This enables multiple search capabilities:

- Exact SHA lookups
- Partial SHA matching (≥7 characters)
- Keyword-based search

## API Endpoints

### KOI Protocol Endpoints

- `POST /koi-net/broadcast-events`: Receive events from other nodes
- `POST /koi-net/poll-events`: Provide events to nodes polling this one
- `POST /koi-net/fetch-rids`: Return RIDs matching requested types
- `POST /koi-net/fetch-manifests`: Return manifests for requested RIDs
- `POST /koi-net/fetch-bundles`: Return bundles for requested RIDs
- `GET /koi-net/health`: Check node health status

### Custom Endpoints

- `GET /search?q=<query>`: Search indexed commits
  - Query can be a SHA (full or partial), or keyword
  - Returns matching commit information

## Configuration

### Local Mode

Configuration is loaded from `config/local/processor-a.yaml`:

```yaml
runtime:
  base_url: http://127.0.0.1:8011/koi-net
  cache_dir: ./.koi/processor-a/cache
  host: 127.0.0.1
  log_level: DEBUG
  port: 8011

edges:
  coordinator_url: http://127.0.0.1:8080/koi-net
# Optional: specify a particular GitHub sensor RID
# processor_a:
#   github_sensor_rid: "..."
```

### Docker Mode

When running in Docker, configuration is loaded from `config/docker/processor-a.yaml`:

```yaml
runtime:
  base_url: http://processor-a:8011/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8011

edges:
  coordinator_url: http://coordinator:8080/koi-net
```

## Deployment

### Local Development

1. Ensure the Python dependencies are installed:

   ```
   pip install -e .
   ```

2. Run the node:
   ```
   python -m processor_a_node
   ```

### Docker Deployment

Use Docker Compose to deploy the entire KOI-net system:

```
docker-compose up
```

Or run just this node:

```
docker-compose up processor-a
```

## Development and Extension

The node is designed with the following module structure:

- `__init__.py`: Configures logging
- `__main__.py`: Entry point that starts the node
- `config.py`: Configuration loading and validation
- `core.py`: Core node initialization
- `handlers.py`: Event and data processing handlers
- `server.py`: FastAPI application and endpoint definitions

To extend the search capabilities:

1. Modify the indexing logic in `handlers.py`
2. Update the query implementation in `query_search_index()`
3. Adjust the search endpoint in `server.py` as needed

## Dependencies

- Python 3.12+
- FastAPI and Uvicorn
- KOI-net libraries (v1.0.0b12+)
- RID-lib (v3.2.3+)
- Additional libraries listed in pyproject.toml

## Limitations and Future Work

- The current implementation uses an in-memory index (data is lost on restart)
- Search is limited to SHA and basic keyword matching
- More advanced text analysis could be implemented for better search results
- Persistent storage could be added for the index
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path

# Import LOG_LEVEL from config (will be defined there)
# from .config import LOG_LEVEL

# Temporary log level until config is implemented
TEMP_LOG_LEVEL = "INFO"

# Get the root logger
logger = logging.getLogger()
# Set the base level
logger.setLevel(TEMP_LOG_LEVEL)

# Create Rich Handler for console
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(TEMP_LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler
log_dir = Path(".koi/processor-b") # Adjust node name
log_dir.mkdir(parents=True, exist_ok=True)
log_file_path = log_dir / "processor-b-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)
file_handler.setLevel(TEMP_LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries later in config.py
# logging.getLogger("uvicorn.error").setLevel(logging.WARNING)

logger.info(
    f"Logging configured (Level: {TEMP_LOG_LEVEL}). Console via Rich, File: {log_file_path}"
)
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/__main__.py">
import uvicorn
import logging
# Import HOST and PORT from config (will be defined there)
# from .config import HOST, PORT

# Temporary values until config is implemented
TEMP_HOST = "0.0.0.0"
TEMP_PORT = 8012 # Default port for Processor B

logger = logging.getLogger(__name__)

logger.info(f"Processor B node starting on {TEMP_HOST}:{TEMP_PORT}")
uvicorn.run(
    "processor_b_node.server:app", # Adjust app path
    host=TEMP_HOST,
    port=TEMP_PORT,
    log_config=None,
    reload=True, # Enable reload for development
)
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/config.py">
import logging
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import Dict, Any, List

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker --- 
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
    if CONFIG_MODE != "docker":
        logger.warning(f"RUN_CONTEXT=docker but KOI_CONFIG_MODE='{CONFIG_MODE}'. Forcing KOI_CONFIG_MODE to 'docker'.")
        CONFIG_MODE = "docker"
else:
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

CONFIG_PATH = CONFIG_DIR / "processor-b.yaml" # Specific config file
ENV_PATH = CONFIG_DIR / "global.env"

logger.info(f"Attempting to load config from: {CONFIG_PATH}")
logger.info(f"Attempting to load env from: {ENV_PATH}")

# Load env vars from the selected global.env first
if ENV_PATH.is_file():
    from dotenv import load_dotenv
    load_dotenv(dotenv_path=ENV_PATH, override=True)
    logger.info(f"Loaded environment variables from {ENV_PATH}")
else:
    logger.warning(f"Global environment file not found at {ENV_PATH}")

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None: # Handle empty file case
            CONFIG = {}
        logger.info(f"Successfully loaded YAML config from {CONFIG_PATH}")
    except Exception as e:
        logger.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logger.error(f"Processor B config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Determine Run Context & Extract Settings --- 
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"

RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
PROCESSOR_B_CONFIG: Dict[str, Any] = CONFIG.get("processor_b", {}) # Processor specific settings

# --- Context-Aware Configuration --- 
LOCAL_DATA_BASE = Path("./.koi/processor-b") # Standard local path base
DOCKER_CACHE_DIR_DEFAULT = "/data/cache" # Default for shared cache in Docker

# Base configuration values
HOST: str = RUNTIME_CONFIG.get("host", "127.0.0.1" if not is_docker else "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8012)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()
BASE_URL = RUNTIME_CONFIG.get("base_url") # Should be defined in yaml
COORDINATOR_URL = EDGES_CONFIG.get("coordinator_url") # Should be defined in yaml

# Optional specific sensor RID
HACKMD_SENSOR_RID: str | None = PROCESSOR_B_CONFIG.get("hackmd_sensor_rid")

# Determine Cache Dir
if is_docker:
    CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", DOCKER_CACHE_DIR_DEFAULT)
else:
    cache_dir_config = RUNTIME_CONFIG.get("cache_dir")
    if cache_dir_config:
        CACHE_DIR = str(Path(cache_dir_config)) # Respect config if set
    else:
        LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
        CACHE_DIR = str(LOCAL_DATA_BASE / "cache") # Fallback local path

# Ensure the resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Update Logging Level Based on Config --- 
try:
    logging.getLogger().setLevel(LOG_LEVEL.upper())
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO.")
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config --- 
logger.info("Processor B Configuration Loaded:")
logger.info(f"  Config Mode: {CONFIG_MODE}")
logger.info(f"  Is Docker Context: {is_docker}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Specific HackMD Sensor RID: {HACKMD_SENSOR_RID or 'Not Set'}")

# Check required config
if not BASE_URL:
    logger.critical("Configuration error: runtime.base_url is not set.")
if not COORDINATOR_URL:
    logger.critical("Configuration error: edges.coordinator_url is not set.")
if not CACHE_DIR:
     logger.critical("Configuration error: runtime.cache_dir is not set or resolved correctly.")
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/core.py">
import os
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the final config loader
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
) 

# Import the RID type this processor consumes
logger = logging.getLogger(__name__) 
try:
    # Attempt import - adjust path if project structure changes
    from nodes.koi_net_hackmd_sensor_node.rid_types import HackMDNote
except ImportError as e:
    logger.error(f"Failed to import HackMDNote: {e}. Ensure sensor node is accessible or use shared RID definitions. Using placeholder.")
    # Define a placeholder if import fails
    class HackMDNote:
        pass 

name = "processor-b"

# Identity directory setup
identity_dir = f".koi/{name}"
os.makedirs(identity_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Check required config values
if not BASE_URL or not COORDINATOR_URL or not CACHE_DIR:
    raise ValueError("Essential configuration (BASE_URL, COORDINATOR_URL, CACHE_DIR) is missing or failed to load!")

# Initialize the KOI-net Node Interface for Processor B
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides( # Processor B provides no new RIDs per PRD
            event=[], 
            state=[]
        ),
        # Consumes HackMDNote implicitly via handlers
    ),
    use_kobj_processor_thread=True,
    first_contact=COORDINATOR_URL,
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,
)

logger.info(f"Initialized NodeInterface for Processor B: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")

# Import handlers after node is initialized
from . import handlers
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/handlers.py">
import logging

from .core import node
from koi_net.processor import ProcessorInterface, HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode

# Import config to potentially check for specific sensor RID
from .config import HACKMD_SENSOR_RID 

# Import the specific RID types this processor handles
try:
    from nodes.koi_net_hackmd_sensor_node.rid_types import HackMDNote
except ImportError:
    logging.error("CRITICAL: Failed to import HackMDNote RID type.")
    # Define a placeholder if import fails
    class HackMDNote:
        pass 

logger = logging.getLogger(__name__)

# Simple in-memory index (as defined in Processor.md)
# Structure: 
# search_index = { "tag": [rid_str1, rid_str2], "title_word": [rid_str1, rid_str3], note_id: [rid_str] }
# note_metadata = { rid_str: {"title": title, "tags": tags, "lastChangedAt": ts}}
search_index = {}
note_metadata = {}

# --- Network Handlers --- 

@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of Coordinator and potential HackMD Sensor nodes."""
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if not profile or not profile.provides:
             logger.warning(f"Received KoiNetNode event for {kobj.rid} with invalid/missing profile.")
             return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    # --- Coordinator/Peer Handshake ---
    if kobj.rid != processor.identity.rid:
        logger.info(f"Discovered potential peer/coordinator: {kobj.rid}. Proposing edge.")
        try:
            edge_bundle = generate_edge_bundle(
                source=kobj.rid, 
                target=processor.identity.rid,
                edge_type=EdgeType.WEBHOOK,
                rid_types=[KoiNetNode, KoiNetEdge],
            )
            processor.handle(bundle=edge_bundle)
        except Exception as e:
            logger.error(f"Failed edge proposal to {kobj.rid}: {e}", exc_info=True)

    # --- HackMD Sensor Discovery & Handshake ---
    provides_hackmd_notes = False
    if HackMDNote != object: # Check if HackMDNote was imported successfully
         # Check if the node provides HackMDNote events or state
         if HackMDNote in profile.provides.event or HackMDNote in profile.provides.state:
              provides_hackmd_notes = True

    if provides_hackmd_notes:
        # Check if a specific sensor RID is configured
        if HACKMD_SENSOR_RID and str(kobj.rid) != HACKMD_SENSOR_RID:
            logger.debug(f"Discovered HackMD sensor {kobj.rid}, but configured to connect only to {HACKMD_SENSOR_RID}. Ignoring.")
            return

        logger.info(f"Discovered HackMD Sensor: {kobj.rid}. Proposing edge.")
        try:
            # Propose an edge TO the sensor to receive events
            edge_bundle = generate_edge_bundle(
                source=kobj.rid, # Sensor is the source
                target=processor.identity.rid, # We are the target
                edge_type=EdgeType.WEBHOOK,
                rid_types=[HackMDNote], # We want HackMDNote events
            )
            processor.handle(bundle=edge_bundle)
            logger.info(f"Edge proposed to HackMD Sensor {kobj.rid}")
        except Exception as e:
            logger.error(f"Failed edge proposal to HackMD Sensor {kobj.rid}: {e}", exc_info=True)

# --- Manifest Handler --- 

@node.processor.register_handler(HandlerType.Manifest, rid_types=[HackMDNote])
def handle_note_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles incoming note manifests, triggers bundle fetch for indexing."""
    if HackMDNote == object:
        logger.error("HackMDNote type not properly imported. Cannot process manifests.")
        return
        
    if not isinstance(kobj.rid, HackMDNote):
         logger.warning(f"Handler received non-HackMDNote RID: {kobj.rid}. Skipping.")
         return

    manifest = kobj.manifest
    rid: HackMDNote = kobj.rid
    logger.info(f"Received manifest for HackMD note: {rid.reference}")

    # PRD: Always dereference notes for indexing.
    logger.debug(f"Requesting bundle for {rid} for indexing.")
    try:
        processor.handle(rid=rid, source=KnowledgeSource.External) # Trigger bundle fetch
    except Exception as e:
        logger.error(f"Error requesting bundle for {rid}: {e}", exc_info=True)

# --- Bundle Handler --- 

@node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
def handle_note_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Processes note bundle contents and updates the search index."""
    if HackMDNote == object:
        logger.error("HackMDNote type not properly imported. Cannot process bundles.")
        return
        
    if not isinstance(kobj.rid, HackMDNote):
         logger.warning(f"Handler received non-HackMDNote RID: {kobj.rid}. Skipping.")
         return

    if not kobj.contents or not isinstance(kobj.contents, dict):
        logger.warning(f"Bundle for {kobj.rid} has no contents or invalid format.")
        return

    rid: HackMDNote = kobj.rid
    contents = kobj.contents
    rid_str = str(rid) # Use string representation for dict keys
    note_id = rid.note_id
    title = contents.get("title", f"Note {note_id}") # Use ID if title missing

    logger.info(f"Processing bundle for note: {note_id} - '{title}'")

    # --- Check Timestamp to Avoid Re-indexing --- 
    last_changed = contents.get("lastChangedAt")
    if rid_str in note_metadata and last_changed == note_metadata[rid_str].get("lastChangedAt"):
        logger.debug(f"Note {note_id} has not changed ({last_changed}) since last index. Skipping.")
        return

    # --- Update Metadata Cache --- 
    current_tags = contents.get("tags", [])
    note_metadata[rid_str] = {
        "title": title,
        "tags": current_tags,
        "lastChangedAt": last_changed
    }

    # --- Update Search Index (Example: Tags, Title words, Note ID) ---
    # Clear old index entries for this note first
    for key, rid_list in list(search_index.items()):
        if isinstance(rid_list, list) and rid_str in rid_list:
            search_index[key].remove(rid_str)
            # Remove key if list becomes empty after removing the RID
            if not search_index[key]: 
                del search_index[key]

    # Index by tags
    for tag in current_tags:
        tag_key = tag.lower() # Case-insensitive tag indexing
        if tag_key not in search_index:
            search_index[tag_key] = []
        # Avoid adding duplicates if somehow the clear logic failed
        if rid_str not in search_index[tag_key]:
            search_index[tag_key].append(rid_str)

    # Index by title words
    for word in title.lower().split():
        if len(word) > 2: # Basic filtering
            if word not in search_index:
                search_index[word] = []
            if rid_str not in search_index[word]:
                search_index[word].append(rid_str)

    # Index by note ID itself for direct lookup
    note_id_key = note_id # Use the actual note ID as the key
    if note_id_key not in search_index:
        search_index[note_id_key] = []
    if rid_str not in search_index[note_id_key]:
        search_index[note_id_key].append(rid_str)

    # Note: Markdown content parsing is omitted as per simplified scope. 
    # md_content = contents.get("content", "")
    # If implemented, parse md_content and add relevant keywords/entities to search_index.

    logger.debug(f"Updated search index for note {note_id}. Index size (keys): {len(search_index)}")

# --- Helper for Search Endpoint --- 
def query_note_index(query: str) -> list:
    """Queries the in-memory note search index."""
    results_rids = set() # Use a set to automatically handle duplicates
    query_lower = query.lower()
    
    # 1. Check if query is a direct Note ID match
    if query in search_index and isinstance(search_index[query], list):
         results_rids.update(search_index[query])

    # 2. Check if query matches a tag (case-insensitive)
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        results_rids.update(search_index[query_lower])
        
    # 3. Check if query matches a title word (case-insensitive)
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        results_rids.update(search_index[query_lower])

    # Format results using metadata cache
    results = []
    for rid_str in results_rids:
        meta = note_metadata.get(rid_str, {}) # Get cached metadata
        results.append({
            "rid": rid_str,
            "title": meta.get("title", "N/A"),
            "tags": meta.get("tags", [])
        })
        
    # Optional: Sort results, e.g., alphabetically by title
    results.sort(key=lambda x: x.get("title", "").lower())

    return results

logger.info("Processor B handlers registered.")
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/server.py">
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, HTTPException

from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from koi_net.processor.knowledge_object import KnowledgeSource

from .core import node # Import the initialized node instance
# Import the query helper from handlers
from .handlers import query_note_index

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup and shutdown."""
    logger.info("Starting Processor B lifespan...")
    try:
        node.start()
        logger.info("Processor B KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        raise RuntimeError("Failed to initialize KOI-net node") from e
    
    yield # Application runs here
    
    logger.info("Shutting down Processor B...")
    try:
        node.stop()
        logger.info("Processor B KOI-net node stopped successfully.")
    except Exception as e:
        logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
    logger.info("Processor B shutdown complete.")


app = FastAPI(
    title="KOI-net Processor B Node (Note Indexer)",
    description="Processes HackMD notes and provides a search API.",
    version="0.1.0",
    lifespan=lifespan,
)

# --- KOI Protocol Router --- 
koi_net_router = APIRouter(prefix="/koi-net")

@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    if not req.events:
         logger.debug("Received empty broadcast event list.")
         return {}
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        try:
            node.processor.handle(event=event, source=KnowledgeSource.External)
        except Exception as e:
            logger.error(f"Error handling broadcast event {event.rid}: {e}", exc_info=True)
    return {}

@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH} for {req.rid}")
    try:
        events = node.network.flush_poll_queue(req.rid)
        logger.debug(f"Returning {len(events)} events for {req.rid}")
        return EventsPayload(events=events)
    except Exception as e:
         logger.error(f"Error polling events for {req.rid}: {e}", exc_info=True)
         return EventsPayload(events=[]) 

@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    try:
        return node.network.response_handler.fetch_rids(req)
    except Exception as e:
        logger.error(f"Error fetching RIDs: {e}", exc_info=True)
        return RidsPayload(rids=[])

@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    try:
        return node.network.response_handler.fetch_manifests(req)
    except Exception as e:
        logger.error(f"Error fetching Manifests: {e}", exc_info=True)
        return ManifestsPayload(manifests=[], not_found=req.rids or [])

@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    try:
        return node.network.response_handler.fetch_bundles(req)
    except Exception as e:
        logger.error(f"Error fetching Bundles: {e}", exc_info=True)
        return BundlesPayload(bundles=[], not_found=req.rids or [])

@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint."""
    if node and node.is_running():
         return {"status": "healthy", "node_status": "running"}
    else:
         return {"status": "unhealthy", "node_status": "stopped_or_not_initialized"}

app.include_router(koi_net_router)

# --- Custom Search API Router ---
search_router = APIRouter()

@search_router.get("/search")
async def search_notes_endpoint(q: str):
    """Endpoint to search the indexed note data."""
    if not q:
        raise HTTPException(status_code=400, detail="Query parameter 'q' is required.")
        
    logger.info(f"Search request received: q='{q}'")
    try:
        # Use the helper function from handlers
        results = query_note_index(q)
        logger.info(f"Search for '{q}' yielded {len(results)} results.")
        return {"query": q, "results": results}
    except Exception as e:
        logger.error(f"Error during search for query '{q}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal server error during search.")

app.include_router(search_router) 

logger.info("Processor B FastAPI application configured with KOI and Search routers.")
</file>

<file path="nodes/koi-net-processor-b-node/.gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Config/State
*.yaml
*.env
*.json
.koi/
</file>

<file path="nodes/koi-net-processor-b-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy pyproject and perform editable installation
COPY pyproject.toml /app/

# 3) install curl for healthcheck
RUN apt-get update && apt-get install -y curl --no-install-recommends && rm -rf /var/lib/apt/lists/*

# 4) install dependencies using UV and pyproject.toml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 5) copy in code
COPY processor_b_node /app/processor_b_node 

# 6) expose port
EXPOSE 8012 

# 7) add healthcheck using the main app's endpoint
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8012/koi-net/health || exit 1

# 8) start the main service using shell form for ENV var substitution
CMD ["uvicorn", "processor_b_node.server:app", "--host", "0.0.0.0", "--port", "8012"]
</file>

<file path="nodes/koi-net-processor-b-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "processor-b-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "httpx", 
    "python-dotenv",
    "rid-lib>=3.2.3", # Ensure compatible versions
    "koi-net==1.0.0b12", 
    "rich", 
    "ruamel.yaml",
]

[tool.setuptools]
package-dir = {"" = "."}
packages = ["processor_b_node"] # Specify the package
</file>

<file path="nodes/koi-net-processor-b-node/README.md">
# Processor B - HackMD Note Indexer

## Core Logic Overview

This section provides an in-depth look at Processor B's implementation details, highlighting the key components and logic that drive its note indexing functionality.

### 1. Key Constants & Classes

- **`HACKMD_SENSOR_RID`**: Optional configuration parameter that allows specifying a particular HackMD sensor node to connect to, instead of auto-discovering available HackMD sensors.
- **`HackMDNote`**: The primary RID class that Processor B consumes, representing HackMD note data.
- **`search_index`**: In-memory structure that maps search terms (tags, title words, note IDs) to matching note RIDs.
- **`note_metadata`**: In-memory cache that stores metadata for indexed notes to avoid re-indexing unchanged content and enrich search results.

### 2. How KOI & RID-lib Are Used

#### Node Interface Initialization

```python
node = NodeInterface(
    name="processor-b",
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[],
            state=[]
        ),
        # Consumes HackMDNote implicitly via handlers
    ),
    use_kobj_processor_thread=True,
    first_contact=COORDINATOR_URL,
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,
)
```

#### Registered Event Handlers

1. **Network Handler**: Processes node discovery events

   ```python
   @node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
   def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Handles discovery of Coordinator and HackMD Sensor nodes
   ```

2. **Manifest Handler**: Processes note manifest events

   ```python
   @node.processor.register_handler(HandlerType.Manifest, rid_types=[HackMDNote])
   def handle_note_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Handles incoming note manifests, triggers bundle fetch
   ```

3. **Bundle Handler**: Processes note bundle data
   ```python
   @node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
   def handle_note_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Processes note bundle contents and updates the search index
   ```

#### RID-lib Usage

Processor B uses RID-lib for:

- Handling standard KOI-net RIDs (`KoiNetNode`, `KoiNetEdge`)
- Processing HackMD note RIDs represented as `HackMDNote` classes with `note_id` properties
- Automatic bundle dereferencing using the processor's built-in mechanisms
- RID string conversion for indexing: `rid_str = str(rid)`

### 3. Proprietary Analysis Logic

The core analysis logic centers around efficient note indexing and change detection:

```python
# Check if note has changed since last indexing
last_changed = contents.get("lastChangedAt")
if rid_str in note_metadata and last_changed == note_metadata[rid_str].get("lastChangedAt"):
    logger.debug(f"Note {note_id} has not changed ({last_changed}) since last index. Skipping.")
    return

# Update metadata cache
current_tags = contents.get("tags", [])
note_metadata[rid_str] = {
    "title": title,
    "tags": current_tags,
    "lastChangedAt": last_changed
}

# Index by tags (critical for topic-based search)
for tag in current_tags:
    tag_key = tag.lower()  # Case-insensitive tag indexing
    if tag_key not in search_index:
        search_index[tag_key] = []
    if rid_str not in search_index[tag_key]:
        search_index[tag_key].append(rid_str)

# Index by title words (enables title-based discovery)
for word in title.lower().split():
    if len(word) > 2:  # Basic filtering
        if word not in search_index:
            search_index[word] = []
        if rid_str not in search_index[word]:
            search_index[word].append(rid_str)
```

The indexing strategy includes:

- Change detection using `lastChangedAt` timestamps to avoid redundant processing
- Tag-based indexing for topical organization and search
- Title word indexing for content relevance
- Direct note ID indexing for exact lookups
- Metadata caching for enriched search results

### 4. Custom Modules & Functions

The node is implemented with the following modular structure:

| Module          | Key Functions                  | Purpose                                     |
| --------------- | ------------------------------ | ------------------------------------------- |
| **core.py**     | `NodeInterface` initialization | Establishes node identity and connections   |
| **handlers.py** | `handle_network_discovery()`   | Detects Coordinator and HackMD sensor nodes |
|                 | `handle_note_manifest()`       | Processes note manifest events              |
|                 | `handle_note_bundle()`         | Indexes note data from bundles              |
|                 | `query_note_index()`           | Implements search functionality             |
| **server.py**   | `broadcast_events_endpoint()`  | Receives events from other nodes            |
|                 | `search_notes_endpoint()`      | Exposes the search API                      |

#### Search Query Implementation

```python
def query_note_index(query: str) -> list:
    """Queries the in-memory note search index."""
    results_rids = set()  # Use a set to automatically handle duplicates
    query_lower = query.lower()

    # 1. Check if query is a direct Note ID match
    if query in search_index and isinstance(search_index[query], list):
         results_rids.update(search_index[query])

    # 2. Check if query matches a tag (case-insensitive)
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        results_rids.update(search_index[query_lower])

    # 3. Check if query matches a title word (case-insensitive)
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        results_rids.update(search_index[query_lower])

    # Format results using metadata cache for rich responses
    results = []
    for rid_str in results_rids:
        meta = note_metadata.get(rid_str, {})
        results.append({
            "rid": rid_str,
            "title": meta.get("title", "N/A"),
            "tags": meta.get("tags", [])
        })

    # Sort results alphabetically by title
    results.sort(key=lambda x: x.get("title", "").lower())

    return results
```

### 5. Configuration & Environment Variables

Key configuration parameters include:

| Parameter           | Default                                                                           | Purpose                      |
| ------------------- | --------------------------------------------------------------------------------- | ---------------------------- |
| `BASE_URL`          | http://{host}:{port}/koi-net                                                      | The node's public endpoint   |
| `COORDINATOR_URL`   | http://coordinator:8080/koi-net (Docker)<br>http://127.0.0.1:8080/koi-net (Local) | The Coordinator node URL     |
| `CACHE_DIR`         | /data/cache (Docker)<br>./.koi/processor-b/cache (Local)                          | Location for cached data     |
| `HOST`              | 0.0.0.0 (Docker)<br>127.0.0.1 (Local)                                             | Interface to bind to         |
| `PORT`              | 8012                                                                              | HTTP server port             |
| `LOG_LEVEL`         | INFO                                                                              | Logging verbosity            |
| `HACKMD_SENSOR_RID` | None                                                                              | Optional specific sensor RID |

The configuration is loaded from YAML files in `config/docker/` or `config/local/` depending on the deployment mode, which is controlled by the `KOI_CONFIG_MODE` and `RUN_CONTEXT` environment variables.

### 6. Sample Snippet: Core Processing Flow

The following code demonstrates the complete lifecycle of processing a HackMD note:

```python
# Step 1: Discover and connect to HackMD sensor
@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
    # Validate discovered node
    profile = kobj.bundle.validate_contents(NodeProfile)

    # Check if the node provides HackMD notes
    provides_hackmd_notes = False
    if HackMDNote in profile.provides.event or HackMDNote in profile.provides.state:
        provides_hackmd_notes = True

    # If it's a HackMD sensor, propose an edge to receive notes
    if provides_hackmd_notes:
        # Check if we're configured to use a specific sensor
        if HACKMD_SENSOR_RID and str(kobj.rid) != HACKMD_SENSOR_RID:
            logger.debug(f"Discovered HackMD sensor {kobj.rid}, but configured to connect only to {HACKMD_SENSOR_RID}. Ignoring.")
            return

        logger.info(f"Discovered HackMD Sensor: {kobj.rid}. Proposing edge.")
        edge_bundle = generate_edge_bundle(
            source=kobj.rid,  # Sensor is the source
            target=processor.identity.rid,  # We are the target
            edge_type=EdgeType.WEBHOOK,
            rid_types=[HackMDNote],  # We want HackMDNote events
        )
        processor.handle(bundle=edge_bundle)

# Step 2: Receive note manifest and request bundle
@node.processor.register_handler(HandlerType.Manifest, rid_types=[HackMDNote])
def handle_note_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    rid = kobj.rid
    logger.info(f"Received manifest for HackMD note: {rid.note_id}")

    # Request full bundle for indexing
    processor.handle(rid=rid, source=KnowledgeSource.External)

# Step 3: Process note bundle and update index
@node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
def handle_note_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    rid = kobj.rid
    contents = kobj.contents
    rid_str = str(rid)
    note_id = rid.note_id
    title = contents.get("title", f"Note {note_id}")

    logger.info(f"Processing bundle for note: {note_id} - '{title}'")

    # Skip unchanged notes using timestamp comparison
    last_changed = contents.get("lastChangedAt")
    if rid_str in note_metadata and last_changed == note_metadata[rid_str].get("lastChangedAt"):
        return

    # Update metadata and index the note
    current_tags = contents.get("tags", [])
    note_metadata[rid_str] = {
        "title": title,
        "tags": current_tags,
        "lastChangedAt": last_changed
    }

    # Index by tags, title words, and note ID
    # ... [indexing logic as shown earlier] ...
```

This code demonstrates how Processor B discovers HackMD sensors, establishes connections, receives note data, and builds its search index - all within the KOI-net protocol's event-driven architecture.

## Overview

The Processor B node is a specialized KOI-net component that consumes HackMD note data from the HackMD sensor node, constructs a comprehensive search index, and provides a search API for discovering and retrieving notes. As an integral part of the KOI-net knowledge mesh, it transforms raw note data into an easily searchable knowledge repository.

## Features

- Seamlessly connects to the KOI-net Coordinator and HackMD sensor nodes
- Processes HackMD note data as it arrives
- Builds a multi-faceted search index based on note metadata
- Offers a simple HTTP API for searching through indexed notes
- Fully implements the KOI protocol for network operations
- Supports both Docker and local deployment configurations

## Architecture

### Network Integration

Processor B establishes and maintains connections with:

1. **Coordinator Node**: For network discovery and mesh communication
2. **HackMD Sensor**: To receive HackMD note events and updates

The node leverages the KOI protocol's edge mechanism to establish these connections, automatically registering for the relevant Resource ID (RID) types:

```
Coordinator <---> Processor B <--- HackMD Sensor
```

### Data Processing Flow

1. **Network Discovery**: The network handler identifies HackMD sensor nodes
2. **Edge Establishment**: Automatically proposes edges to receive note events
3. **Manifest Processing**: Receives note manifests and requests full bundle data
4. **Index Construction**: Processes note bundles to build and update the search index
5. **Search Service**: Provides endpoints for querying the indexed notes

### Search Index Structure

The search index uses a dual in-memory structure:

```python
# Map search terms to matching RIDs
search_index = {
    "tag": [rid_str1, rid_str2],         # Tag-based lookup
    "title_word": [rid_str1, rid_str3],  # Title word lookup
    "note_id": [rid_str]                 # Direct ID lookup
}

# Store note metadata for rich search results
note_metadata = {
    rid_str: {
        "title": "Note Title",
        "tags": ["tag1", "tag2"],
        "lastChangedAt": timestamp
    }
}
```

This enables multiple search capabilities:

- Tag-based search
- Title word search
- Direct note ID lookup
- Change tracking to avoid reindexing unchanged notes

## API Endpoints

### KOI Protocol Endpoints

- `POST /koi-net/broadcast-events`: Receive events from other nodes
- `POST /koi-net/poll-events`: Provide events to nodes polling this one
- `POST /koi-net/fetch-rids`: Return RIDs matching requested types
- `POST /koi-net/fetch-manifests`: Return manifests for requested RIDs
- `POST /koi-net/fetch-bundles`: Return bundles for requested RIDs
- `GET /koi-net/health`: Check node health status

### Custom Endpoints

- `GET /search?q=<query>`: Search indexed notes
  - Query can be a note ID, tag, or word from a note title
  - Returns matching notes with titles and tags

## Configuration

### Local Mode

Configuration is loaded from `config/local/processor-b.yaml`:

```yaml
runtime:
  base_url: http://127.0.0.1:8012/koi-net
  cache_dir: ./.koi/processor-b/cache
  host: 127.0.0.1
  log_level: DEBUG
  port: 8012

edges:
  coordinator_url: http://127.0.0.1:8080/koi-net
# Optional: specify a particular HackMD sensor
# processor_b:
#   hackmd_sensor_rid: "..."
```

### Docker Mode

When running in Docker, configuration is loaded from `config/docker/processor-b.yaml`:

```yaml
runtime:
  base_url: http://processor-b:8012/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8012

edges:
  coordinator_url: http://coordinator:8080/koi-net
```

## Deployment

### Local Development

1. Install the required dependencies:

   ```
   pip install -e .
   ```

2. Start the node:
   ```
   python -m processor_b_node
   ```

### Docker Deployment

Use Docker Compose to deploy the entire KOI-net system:

```
docker-compose up
```

Or run only this specific node:

```
docker-compose up processor-b
```

## Development and Extension

The node follows a modular structure:

- `__init__.py`: Logging configuration
- `__main__.py`: Entry point for node execution
- `config.py`: Configuration loading and environment setup
- `core.py`: KOI-net node initialization
- `handlers.py`: Event processing and index management
- `server.py`: FastAPI application and endpoints

To enhance search capabilities:

1. Extend the indexing logic in `handlers.py`
2. Modify the query implementation in `query_note_index()`
3. Enhance the search endpoint response in `server.py`

Current indexing is based on:

- Note IDs (direct lookup)
- Tags (keyword categorization)
- Title words (content relevance)

## Dependencies

- Python 3.12+
- FastAPI and Uvicorn
- KOI-net libraries (v1.0.0b12+)
- RID-lib (v3.2.3+)
- Additional dependencies as listed in pyproject.toml

## Limitations and Future Work

- Current implementation uses in-memory storage (no persistence between restarts)
- Full-text search of note content is not implemented but could be added
- No pagination for search results (returns all matches)
- Could be extended with:
  - Persistent storage for the index
  - Advanced text analysis (stemming, entity extraction)
  - Support for more complex query syntax
  - Note content parsing and keyword extraction
</file>

<file path="scripts/prd.txt">
{CONTEXT}
### Implementation Scope — what still must be written

*everything else already exists*

Only four codebases are available today:

* **koi‑net** – NodeInterface, FastAPI wiring, event queues, edge‑handshake logic (`pip install koi-net`).
* **rid‑lib** – RID classes plus manifest / bundle helpers (`pip install rid-lib`).
* **GitHubSensor** & **HackMDSensor** – working ingress nodes that already emit manifests and serve bundles (GitHub repos).
* **Coordinator** – fully functional subnet root (GitHub repo).

Everything else—including **Docker images** and **processors**—must be produced:

| Item to build                                                                    | Purpose                                                                                       | Minimum feature set                                                                                                                                                           |                                                   |
| -------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
| **Dockerfiles + images** for **Coordinator**, **GitHubSensor**, **HackMDSensor** | Containerise the existing Python repos.                                                       | • `FROM python:3.11‑slim` → `pip install -e .` → `ENTRYPOINT ["python", "main.py"]`.<br/>• Expose KOI port; add `/health` route for Compose.                                  |                                                   |
| **Processor A (Repo Indexer)**                                                   | Subscribe to GitHub manifests, optionally dereference bundles, keep a local searchable index. | • Use `koi_net.NodeInterface`.<br/>• Subscribe to `CommitManifestRID` & `PullRequestManifestRID`.<br/>• On deref, store bundles in RID cache.<br/>• Provide \`/search?q=\<sha | keyword>\` endpoint; **no new RIDs are emitted**. |
| **Processor B (Note Indexer)**                                                   | Subscribe to HackMD manifests, dereference bundles, keep a Markdown index.                    | • Same NodeInterface skeleton.<br/>• Subscribe to `NoteManifestRID`.<br/>• Store bundles; expose \`/search?q=\<tag                                                            | title>\`.                                         |
| *(optional)* **Shared utility lib**                                              | Re‑usable helper for simple text/sha searches across the RID cache.                           | Tiny Python module, no external deps.                                                                                                                                         |                                                   |
| **Dockerfiles + images** for **Processor A/B**                                   | Containerise the two new processors.                                                          | Standard slim image, install repo, expose KOI + `/search` port.                                                                                                               |                                                   |
| **Config directory**                                                             | YAMLs for all five nodes + `.env` tokens.                                                     | Sensors point to real repos / HackMD team; processors list which sensor edges to follow.                                                                                      |                                                   |
| **docker‑compose.yml**                                                           | Orchestrate the five services on one bridge network.                                          | Service‑name DNS discovery; health checks on `/health`.                                                                                                                       |                                                   |
| *(optional)* **orchestrate.py**                                                  | One‑command launcher for CI / local dev.                                                      | Read YAML → render `docker-compose.yml` → run `docker compose up`.                                                                                                            |                                                   |

> **Simplification:** Processor A and Processor B **do not ingest data directly**. They listen to manifests from the sensors, pull bundles when needed, and serve local search APIs—no risk scores, TODO extraction, or new RIDs.
{CONTEXT}

## 1  Executive Summary

The Autonomous Sensor‑Processor demo showcases a _self‑forming knowledge mesh_ that turns two everyday SaaS feeds—GitHub repositories and HackMD documents—into a continuously updated, query‑ready knowledge graph. At its core, the system pairs **sensors** (specialised ingress nodes) with **processors** (analytic nodes) and binds them together through a lightweight **Coordinator** that speaks the KOI (Core Orchestration Interface) protocol.

When the Coordinator boots it exposes an empty **subnet**—a logical namespace inside KOI. Each sensor registers, back‑fills historical data as _manifest_ RIDs, and advertises its ability to serve full _bundle_ payloads on demand. Processors arrive later, automatically peer with their respective sensors, selectively dereference the manifests they care about, and publish new knowledge objects (e.g., risk scores or action items). Once Processor A and Processor B discover each other they negotiate a direct edge and begin exchanging their derived insights, completing an entirely autonomous handshake without manual wiring.

The demo therefore illustrates three key capabilities:

1. **Declarative ingestion** – any data source is reduced to strongly‑typed RIDs plus a dereference link.
2. **Autonomous topology formation** – nodes propose and accept edges through a single Coordinator API.
3. **Composable reasoning** – processors can be added, removed, or swapped without touching the ingestion layer.

---

## 2  Core Libraries & Frameworks
| Library                                    | Role & Integration Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **KOI (Core Orchestration Interface)**     | KOI defines the canonical HTTP endpoints (`/edges/*`, `/events/*`, `/bundles/*`) that every node exposes. In this demo all communication—manifest broadcast, edge proposals, dereference requests—travels over KOI. Each node initialises a _NodeInterface_ provided by the `koi‑net` package, which mounts these routes onto a FastAPI app and publishes node metadata (ID, capabilities) back to the Coordinator on start‑up.                                                                |
| **RID‑lib (Reference Identifier Library)** | RID‑lib supplies the strongly‑typed identifier classes (e.g., `GitHubCommitRID`, `HackMDNoteRID`). Every sensor constructs **Manifest** objects (hash + metadata) and stores them in a disk cache; processors later resolve them into **Bundle** objects with full payloads. Because RIDs inherit from Pydantic `BaseModel`, the shapes are validated at runtime and serialised automatically in the KOI event stream. No additional configuration is required beyond setting `RID_CACHE_DIR`. |
| **Generic Sensor SDK**                     | A thin helper layer that wraps `koi‑net` to make writing new sensors trivial. It furnishes an async _back‑fill loop_, optional webhook plumbing, and helper mixins for “manifest first, bundle on dereference” behaviour. In the demo both GitHub and HackMD sensors are less than 200 lines thanks to this SDK.                                                                                                                                                                               |



## 3  System Architecture

### 3.1 Architecture Overview

A **subnet** is simply a GUID namespace plus a gossip address book. The first node you launch—the **Coordinator**—creates a new subnet and becomes its root. All subsequent nodes point to the Coordinator’s URL in their config; on boot they execute a `/edges/propose` call that says _“I am node X, I offer streams \[event, state], here is my public URL.”_

Sensors hold a **unidirectional** edge back to the Coordinator but a **bidirectional** edge to the processor(s) that consume their data. Once a processor accepts a sensor edge it begins receiving _manifest_ events. Processors are themselves peers and may negotiate a **knowledge edge** with each other, enabling them to exchange derived RIDs without burdening the Coordinator.

Because every data object is referenced by a RID, processors can request the _minimal_ data required: first a manifest to see if anything changed, then a bundle only when analysis is required. This keeps bandwidth low while preserving full‑fidelity access when needed.

### 3.2 Mermaid Component Diagram

Architecture diagram


// Main Processors
COMPONENT: Processor A
  TYPE: Processing Node
  CONNECTED_TO: [GitHub Sensor, Coordinator, Processor B, local rules & local context]
  CONFIG: {where it runs, how it behaves}

COMPONENT: Processor B
  TYPE: Processing Node
  CONNECTED_TO: [Hackmd Sensor, Coordinator, Processor A, local rules & local context]
  CONFIG: {where it runs, how it behaves}

// Coordinator
COMPONENT: Coordinator
  TYPE: Central Node
  STYLE: Dashed border (indicates logical/virtual component)
  CONNECTED_TO: [Processor A, Processor B, GitHub Sensor, Hackmd Sensor]
  CONNECTION_STYLE: Dashed lines (indicates logical connections)

// Sensors
COMPONENT: GitHub Sensor
  TYPE: Data Source Connector
  CONNECTED_TO: [Processor A, repos "sensed"]
  CONFIG: {where it runs, what it senses}

COMPONENT: Hackmd Sensor
  TYPE: Data Source Connector
  CONNECTED_TO: [Processor B, documents "sensed"]
  CONFIG: {where it runs, what it senses}

// Storage Components
COMPONENT: local rules & local context (left)
  TYPE: Database
  SHAPE: Cylinder
  CONNECTED_TO: [Processor A]

COMPONENT: local rules & local context (right)
  TYPE: Database
  SHAPE: Cylinder
  CONNECTED_TO: [Processor B]

// Data Sources
COMPONENT: repos "sensed"
  TYPE: External Data Source
  SHAPE: Document/Note
  CONNECTED_TO: [GitHub Sensor]
  CONNECTION_STYLE: Dashed line (indicates data flow)

COMPONENT: documents "sensed"
  TYPE: External Data Source
  SHAPE: Document/Note
  CONNECTED_TO: [Hackmd Sensor]
  CONNECTION_STYLE: Dashed line (indicates data flow)

// Config Components
COMPONENT: config (top left)
  TYPE: Configuration
  SHAPE: Circle
  CONTENT: "config: where it runs, how it behaves"
  CONNECTED_TO: [Processor A]

COMPONENT: config (top right)
  TYPE: Configuration
  SHAPE: Circle
  CONTENT: "config: where it runs, how it behaves"
  CONNECTED_TO: [Processor B]

COMPONENT: config (bottom left)
  TYPE: Configuration
  SHAPE: Circle
  CONTENT: "config: where it runs, what it senses"
  CONNECTED_TO: [GitHub Sensor]

COMPONENT: config (bottom right)
  TYPE: Configuration
  SHAPE: Circle
  CONTENT: "config: where it runs, what it senses"
  CONNECTED_TO: [Hackmd Sensor]

// Layout Information
LAYOUT:
  - Grid-based background
  - Processors at top center
  - Coordinator in middle center
  - Sensors below processors
  - Data sources at bottom
  - Config circles adjacent to their respective components
  - Databases on left and right sides

```mermaid
flowchart LR
    %% Main components with clear labels
    subgraph Orchestration["0 • Bootstrap (docker-compose + config.yaml)"]
        Orchestrator["orchestrate.py"]
    end

    subgraph Sensors["2 • Sensors register"]
        GitHubSensor["GitHub Sensor"]
        HackMDSensor["HackMD Sensor"]
    end

    Coordinator["1 • Coordinator (subnet root)"]

    subgraph Processors["3 • Full nodes (processors)"]
        ProcA["Processor A"]
        ProcB["Processor B"]
    end

    %% Cleaner edge relationships with consistent styling
    Orchestrator -->|"1. compose up"| Coordinator
    Orchestrator -->|"2. initialize"| GitHubSensor & HackMDSensor
    Orchestrator -->|"3. initialize"| ProcA & ProcB

    GitHubSensor -->|"4. edge proposal"| Coordinator
    HackMDSensor -->|"5. edge proposal"| Coordinator

    GitHubSensor -->|"6. manifests"| ProcA
    HackMDSensor -->|"7. manifests"| ProcB

    ProcA -->|"8. edge proposal"| Coordinator
    ProcB -->|"9. edge proposal"| Coordinator

    ProcA <-->|"10. knowledge edge"| ProcB

    %% Styling
    classDef orchestration fill:#fffae6,stroke:#d6b656
    classDef sensors fill:#dae8fc,stroke:#6c8ebf
    classDef coordinator fill:#d5e8d4,stroke:#82b366
    classDef processors fill:#e1d5e7,stroke:#9673a6

    class Orchestration orchestration
    class Sensors sensors
    class Coordinator coordinator
    class Processors processors
```



## 4  Custom Code Components

Each component below implements the KOI node interface, participates in the Coordinator‑mediated edge handshake, and either **emits** or **consumes** RIDs.

| Component                    | Purpose                                                                             | Inputs → Outputs                                                                                    | Internal Flow                                                                                                                                                                                                                                                                                                                                                         |
| ---------------------------- | ----------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **CoordinatorLauncher**      | Launches the FastAPI coordinator service and seeds the subnet’s address book.       | `/edges/propose` → `/edges/accept`                                                                  | 1. Start FastAPI on `0.0.0.0:8080`.<br/>2. Load or create **`identity.json`** (node’s own RID and profile).<br/>3. Accept every proposal (demo mode) and return a signed token in `/edges/accept`.<br/>4. Store remote nodes and edges as RID **bundles** in the local cache.<br/>5. Rebuild the in‑memory `NetworkGraph` from those cached bundles on every restart. |
| **GitHubSensorNode**         | Watches target repos and emits RIDs for commits, issues, PRs.                       | GitHub webhook JSON → `CommitManifestRID` / `IssueManifestRID`<br/>Dereference request → zip bundle | • Setup webhook or polling.<br/>• For each event create Manifest + store `.tar.gz` in cache.<br/>• Propose edge → Coordinator.<br/>• Serve bundle on `GET /bundles/{rid}`.                                                                                                                                                                                            |
| **HackMDSensorNode**         | Streams HackMD note events.                                                         | HackMD API JSON → `NoteManifestRID`<br/>Dereference request → Markdown bundle                       | • Poll `/teams/{id}/notes` every 30 s.<br/>• Emit manifests with `lastChangedAt` hash.<br/>• Serve full note on dereference.                                                                                                                                                                                                                                          |
| **ProcessorA**               | Scores GitHub PRs for risk, publishes `RiskScoreRID`.                               | `CommitManifestRID` → deref → zip → `RiskScoreRID`                                                  | • Subscribe to GitHubSensor edge.<br/>• Filter for `pull_request` events.<br/>• Run `risk = f(churn, files_changed)`.<br/>• Publish risk score manifest + bundle.                                                                                                                                                                                                     |
| **ProcessorB**               | Extracts action items from HackMD notes, links them to PR risk scores.              | `NoteManifestRID` + `RiskScoreRID` → `ActionItemRID`                                                | • Subscribe to HackMDSensor.<br/>• Parse Markdown ‑> tasks list.<br/>• Look up matching `RiskScoreRID` via graph query.<br/>• Publish enriched action items.                                                                                                                                                                                                          |
| **OrchestratorScript (CLI)** | One‑stop launcher that assembles configs, pulls images, and invokes Docker Compose. | `config/*.yaml`, `global.env` → running containers                                                  | • Parse YAMLs.<br/>• Render `docker‑compose.yml` template.<br/>• `subprocess.run("docker compose up")`.<br/>• Tail logs and exit when health checks green.                                                                                                                                                                                                            |

**Example Pydantic model (simplified)**

```python
class CommitManifestRID(RID):
    scheme: Literal["urn"]
    namespace: Literal["github"]
    reference: str  # e.g. "org/repo@sha"

class CommitManifest(Manifest):
    rid: CommitManifestRID
    sha: str
    author: str
    message: str
```



## 5  Runtime Handshake & Data Flow

The demo progresses through five automatic stages:

1. **Deploy Coordinator** – `CoordinatorLauncher` starts first, creating an empty subnet.
2. **Deploy Sensors** – each sensor container comes up, discovers the Coordinator, and proposes an edge. The Coordinator responds with an _accept_ message, completing registration.
3. **Deploy Processors** – processors repeat the same proposal/accept dance, then subscribe to sensor event streams.
4. **Autonomous Edge Negotiation** – sensors and processors negotiate direct edges; processors also discover each other and form a _knowledge edge_.
5. **Bidirectional Knowledge Exchange** – manifests flow sensor → processor; bundles are pulled on demand; derived RIDs flow processor → processor.

### 5.1 Mermaid Sequence Diagram

```mermaid
sequenceDiagram
  autonumber
  participant C as Coordinator
  participant G as GitHubSensor
  participant H as HackMDSensor
  participant A as ProcessorA
  participant B as ProcessorB

  C->>G: POST /edges/propose
  G-->>C: /edges/accept
  C->>H: POST /edges/propose
  H-->>C: /edges/accept
  C->>A: POST /edges/propose
  A-->>C: /edges/accept
  C->>B: POST /edges/propose
  B-->>C: /edges/accept
  A->>G: subscribe(manifests)
  B->>H: subscribe(manifests)
  A->>G: GET /bundles/{rid}
  G-->>A: zip bundle
  B->>H: GET /bundles/{rid}
  H-->>B: md bundle
  A->>B: POST /events (RiskScoreRID)
  B->>A: POST /events (ActionItemRID)
```



## 6  Configuration

Each container reads a YAML config plus an optional `.env` file. This keeps code immutable and shifts environment‑specific values (API tokens, repo lists, polling intervals) into declarative files that Ops can tweak without a rebuild.

### 6.1 Repository Layout

```
demo/
├─ config/
│  ├─ global.env
│  ├─ coordinator.yaml
│  ├─ github-sensor.yaml
│  ├─ hackmd-sensor.yaml
│  ├─ processor-a.yaml
│  └─ processor-b.yaml
└─ orchestrate.py
```

### 6.2 Sample Sensor Config

```yaml
# config/github-sensor.yaml
sensor:
  kind: github
  mode: poll # "webhook" also supported
  poll_interval: 30 # seconds
  repos:
    - "blockscience/demo-repo"
    - "blockscience/infra-repo"
edges:
  coordinator_url: "http://coordinator:8080"
runtime:
  cache_dir: "/data/cache"
env:
  GITHUB_TOKEN: ${GITHUB_TOKEN}
```

- **Fields explained**

  - `kind` – loader in Generic Sensor SDK chooses GitHub adapter.
  - `poll_interval` – controls back‑fill cadence; `0` tells the SDK to rely solely on webhooks.
  - `repos` – list of `<org>/<repo>` targets; wildcards supported.
  - `runtime.cache_dir` – mount point that persists manifests/bundles across restarts.

### 6.3 Environment Variables (`global.env`)

```
GITHUB_TOKEN=ghp_xxxxxxxxxxx
HACKMD_TOKEN=tok_xxxxxxxxxxx
SUBNET_ID=demo-subnet
RID_CACHE_DIR=/data/cache
```

**Defaults vs. overrides** – Each YAML provides sane defaults; any key can be overridden by exporting an env var of the same name (e.g., `POLL_INTERVAL=10 docker compose up`).



## 7  Docker Orchestration

Docker Compose offers a repeatable, single‑command launch that mirrors production topology while remaining laptop‑friendly. Service names (`coordinator`, `github-sensor`, …) double as DNS hostnames on the **demo-net** bridge, which means each node can discover its peers via simple `http://<service>:8080` URLs—no extra service discovery layer required.

### 7.1 Service Definitions & Network

| Service           | Image                       | Ports | Volumes    | Purpose                    |
| ----------------- | --------------------------- | ----- | ---------- | -------------------------- |
| **coordinator**   | `demo/coordinator:latest`   | 8080  | –          | Subnet root & edge broker  |
| **github‑sensor** | `demo/github-sensor:latest` | 8001  | `gh-cache` | Ingests GitHub repo events |
| **hackmd‑sensor** | `demo/hackmd-sensor:latest` | 8002  | `hm-cache` | Ingests HackMD note events |
| **processor‑a**   | `demo/processor-a:latest`   | 8011  | `pa-cache` | GitHub analytics           |
| **processor‑b**   | `demo/processor-b:latest`   | 8012  | `pb-cache` | HackMD analytics           |

All containers join **demo-net**, a user‑defined bridge network. The Coordinator implicitly “assigns” the subnet label but actual routing is handled by Docker’s embedded DNS.

### 7.2 Startup Sequence Diagram

```mermaid
flowchart TD
  Start(["docker compose up"])
  Ctr[Coordinator]
  GS[GitHub Sensor]
  HS[HackMD Sensor]
  PA[Processor A]
  PB[Processor B]

  Start --> Ctr
  Ctr --> GS
  Ctr --> HS
  GS --> PA
  HS --> PB
  PA --> PB
```

### 7.3 Example `docker-compose.yml`

```yaml
version: "3.8"

services:
  coordinator:
    image: demo/coordinator:latest
    container_name: coordinator
    ports:
      - "8080:8080"
    networks: [demo-net]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 10s
      retries: 5

  github-sensor:
    image: demo/github-sensor:latest
    container_name: github-sensor
    depends_on:
      coordinator:
        condition: service_healthy
    volumes:
      - gh-cache:/data/cache
    env_file: config/global.env
    networks: [demo-net]

  hackmd-sensor:
    image: demo/hackmd-sensor:latest
    container_name: hackmd-sensor
    depends_on:
      coordinator:
        condition: service_healthy
    volumes:
      - hm-cache:/data/cache
    env_file: config/global.env
    networks: [demo-net]

  processor-a:
    image: demo/processor-a:latest
    container_name: processor-a
    depends_on:
      - github-sensor
    volumes:
      - pa-cache:/data/cache
    env_file: config/global.env
    networks: [demo-net]

  processor-b:
    image: demo/processor-b:latest
    container_name: processor-b
    depends_on:
      - hackmd-sensor
    volumes:
      - pb-cache:/data/cache
    env_file: config/global.env
    networks: [demo-net]

networks:
  demo-net:
    driver: bridge

volumes:
  gh-cache:
  hm-cache:
  pa-cache:
  pb-cache:
```

> **Key sections**
>
> - `depends_on` with `condition: service_healthy` ensures sensors wait for the Coordinator’s health check before registering.
> - Named volumes keep RID caches persistent for quicker restarts.
> - All services join _one_ bridge network to keep URL discovery trivial.

With this Compose file and the configs above, the entire demo spins up with a single command and showcases an end‑to‑end, autonomous data pipeline—from raw GitHub commits and HackMD edits to enriched cross‑referenced knowledge objects—without a single manual edge configuration.
</file>

<file path=".gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
**/*.env

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
/.koi
/.cursor
services/github/.env
**/*/state.json

# Added by Claude Task Master
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
dev-debug.log
# Dependency directories
node_modules/
# Environment variables
.env
# Editor directories and files
.idea
.vscode
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
# OS specific
# Task files
tasks.json
tasks/
</file>

<file path="code.txt">
This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: nodes/**/*, config/**/*, docker-compose.yaml
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
config/
  docker/
    coordinator.yaml
    github-sensor.yaml
    global.env.example
    hackmd-sensor.yaml
  local/
    coordinator.yaml
    github-sensor.yaml
    global.env.example
    hackmd-sensor.yaml
nodes/
  koi-net-coordinator-node/
    coordinator_node/
      __init__.py
      __main__.py
      config_loader.py
      core.py
      handlers.py
      server.py
    .gitignore
    Dockerfile
    koi-net-coordinator-node.service
    LICENSE
    pyproject.toml
    README.md
    requirements.txt
  koi-net-github-sensor-node/
    github_sensor_node/
      handlers/
        github.py
      __init__.py
      __main__.py
      backfill.py
      config.py
      core.py
      loader.py
      server.py
      types.py
      webhook.py
    .env.example
    .gitignore
    Dockerfile
    github-node.service
    pyproject.toml
    repomix-output.xml
  koi-net-hackmd-sensor-node/
    hackmd_sensor_node/
      __init__.py
      __main__.py
      backfill.py
      config.py
      core.py
      hackmd_api.py
      handlers.py
      server.py
    .gitignore
    Dockerfile
    koi-net-hackmd-sensor-node.service
    LICENSE
    pyproject.toml
    README.md
    requirements.txt
    rid_types.py
docker-compose.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="config/docker/github-sensor.yaml">
edges:
  coordinator_url: http://coordinator:8080/koi-net
runtime:
  base_url: http://github-sensor:8001/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8001
  state_file: /data/cache/github_state.json
sensor:
  kind: github
  mode: webhook
  poll_interval: 60
  repos:
    - BlockScience/koi
webhook:
  secret_env_var: GITHUB_WEBHOOK_SECRET
</file>

<file path="config/docker/global.env.example">
GITHUB_TOKEN=<github-token>
HACKMD_TOKEN=<hackmd-token>
GITHUB_WEBHOOK_SECRET=<github-webhook-secret>
</file>

<file path="config/docker/hackmd-sensor.yaml">
api:
  token_env_var: HACKMD_TOKEN
edges:
  coordinator_url: http://coordinator:8080/koi-net
runtime:
  base_url: http://hackmd-sensor:8002/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8002
sensor:
  kind: hackmd
  poll_interval: 300
  target_note_ids:
  - C1xso4C8SH-ZzDaloTq4Uw
  team_path: blockscience
</file>

<file path="config/local/github-sensor.yaml">
edges:
  coordinator_url: http://0.0.0.0:8080/koi-net
runtime:
  base_url: http://0.0.0.0:8001/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8001
  state_file: /data/cache/github_state.json
sensor:
  kind: github
  mode: webhook
  poll_interval: 60
  repos:
    - blockscience/target-repo-1
    - blockscience/target-repo-2
webhook:
  secret_env_var: GITHUB_WEBHOOK_SECRET
</file>

<file path="config/local/global.env.example">
GITHUB_TOKEN=<github-token>
HACKMD_TOKEN=<hackmd-token>
GITHUB_WEBHOOK_SECRET=<github-webhook-secret>
</file>

<file path="config/local/hackmd-sensor.yaml">
api:
  token_env_var: HACKMD_TOKEN
edges:
  coordinator_url: http://0.0.0.0:8080/koi-net
runtime:
  base_url: http://0.0.0.0:8002/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8002
sensor:
  kind: hackmd
  poll_interval: 300
  target_note_ids:
    - C1xso4C8SH-ZzDaloTq4Uw
  team_path: blockscience
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path
from .config_loader import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/coordinator")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "coordinator-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
# logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
# logging.getLogger("anyio").setLevel(logging.WARNING)

logger.info(
    f"Logging configured (Level: {LOG_LEVEL}). Console via Rich, File: {log_file_path}"
)
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/__main__.py">
import uvicorn

# Remove import from old config
# from .config import PORT

# Port is now handled by Docker CMD and config_loader if needed elsewhere
uvicorn.run(
    "coordinator_node.server:app",
    host="0.0.0.0",
    port=8080,
    log_config=None,
    reload=True,
)
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/server.py">
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node


logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    node.start()
    yield
    node.stop()


app = FastAPI(
    lifespan=lifespan,
    root_path="/koi-net",
    title="KOI-net Protocol API",
    version="1.0.0",
)


@app.get("/health")
def health_check():
    """Basic health check endpoint."""
    # You could potentially add more checks here later, like checking
    # the status of the `node` object if it provides such a method.
    logger.debug("Health check endpoint hit")
    return {"status": "healthy"}


@app.post(BROADCAST_EVENTS_PATH)
def broadcast_events(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)


@app.post(POLL_EVENTS_PATH)
def poll_events(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@app.post(FETCH_RIDS_PATH)
def fetch_rids(req: FetchRids) -> RidsPayload:
    return node.network.response_handler.fetch_rids(req)


@app.post(FETCH_MANIFESTS_PATH)
def fetch_manifests(req: FetchManifests) -> ManifestsPayload:
    return node.network.response_handler.fetch_manifests(req)


@app.post(FETCH_BUNDLES_PATH)
def fetch_bundles(req: FetchBundles) -> BundlesPayload:
    return node.network.response_handler.fetch_bundles(req)
</file>

<file path="nodes/koi-net-coordinator-node/.gitignore">
rid_cache
identity.json
events_queue.json
venv
.env
*.json
__pycache__
</file>

<file path="nodes/koi-net-coordinator-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy pyproject and perform editable installation
COPY pyproject.toml /app/

# 3) install curl for healthcheck
RUN apt-get update && apt-get install -y curl

# 4) install dependencies using UV and pyproject.toml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 5) copy in code
COPY . /app/

# 6) expose port and set environment
EXPOSE 8080

# 7) add healthcheck using the main app's endpoint
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8080/koi-net/health || exit 1

# 8) start the main service using shell form for ENV var substitution
CMD ["uvicorn", "coordinator_node.server:app", "--host", "0.0.0.0", "--port", "8080"]
</file>

<file path="nodes/koi-net-coordinator-node/koi-net-coordinator-node.service">
[Unit]
Description=KOI-net Coordinator Node Service
After=network.target

[Service]
WorkingDirectory=/home/dev/koi-net-coordinator-node
ExecStart=/home/dev/koi-net-coordinator-node/venv/bin/python3 -m coordinator_node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="nodes/koi-net-coordinator-node/LICENSE">
MIT License

Copyright (c) 2025 BlockScience

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="nodes/koi-net-coordinator-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "coordinator-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.1",
    "koi-net==1.0.0b12",
    "rich",
    "ruamel.yaml",
    "python-dotenv"
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="nodes/koi-net-coordinator-node/README.md">
# koi-net-coordinator-node
Coordinator node implementation for BlockScience's KOI-net
</file>

<file path="nodes/koi-net-coordinator-node/requirements.txt">
koi-net==1.0.0b12
rid-lib>=3.2.2
fastapi
uvicorn
rich
ruamel.yaml
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/handlers/github.py">
import logging
from ..core import node
from ..types import GithubCommit

from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of the coordinator node (or other nodes providing KoiNetNode events).

    On discovering a NEW coordinator, proposes a WEBHOOK edge for bidirectional
    communication and requests a list of other known nodes (sync).
    (Based on refactor.md example)
    """
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if KoiNetNode not in profile.provides.event:
            logger.debug(
                f"Node {kobj.rid} does not provide KoiNetNode events. Ignoring."
            )
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    logger.info(
        f"Identified potential coordinator/peer: {kobj.rid}; proposing WEBHOOK edge"
    )
    try:

        edge_bundle = generate_edge_bundle(
            source=kobj.rid,
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode],
        )
        processor.handle(bundle=edge_bundle)
    except Exception as e:
        logger.error(
            f"Failed to generate or handle WEBHOOK edge bundle for {kobj.rid}: {e}",
            exc_info=True,
        )

    logger.info(f"Syncing network nodes from {kobj.rid}")
    try:
        payload = processor.network.request_handler.fetch_rids(
            kobj.rid, rid_types=[KoiNetNode]
        )
        if not payload or not payload.rids:
            logger.warning(f"Received empty RIDs payload from {kobj.rid} during sync.")
            return

        logger.debug(f"Received {len(payload.rids)} RIDs from {kobj.rid}")
        for rid in payload.rids:

            if rid == processor.identity.rid or processor.cache.exists(rid):
                continue
            logger.debug(f"Handling discovered RID from sync: {rid}")

            processor.handle(rid=rid, source=KnowledgeSource.External)
    except Exception as e:
        logger.error(f"Failed during network sync with {kobj.rid}: {e}", exc_info=True)


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit(processor: ProcessorInterface, kobj: KnowledgeObject):
    """
    Basic handler for processing GithubCommit bundles.
    Currently just logs information.

    Args:
        bundle: The Bundle object containing the GithubCommit RID and contents.
    """
    try:

        bundle = kobj.bundle
        rid: GithubCommit = bundle.rid
        contents: dict = bundle.contents

        logger.info(
            f"Processing commit: {rid} (Normalized Type: {kobj.normalized_event_type}, Source: {kobj.source})"
        )

        logger.debug(
            f"  Author: {contents.get('author_name')} <{contents.get('author_email')}>"
        )
        logger.debug(
            f"  Message: {contents.get('message', '').splitlines()[0][:80]}..."
        )
        logger.debug(f"  URL: {contents.get('html_url')}")

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Example handler demonstrating processing before cache write."""
    logger.debug(f"Handling GithubCommit bundle PRE-CACHE for {kobj.rid}")

    try:

        if kobj.contents and "message" in kobj.contents:
            logger.info(
                f"Processing commit {kobj.rid.sha[:7]}: {kobj.contents['message'].splitlines()[0]}"
            )

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


logger.info("GithubCommit Bundle handler registered.")
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path

# Import LOG_LEVEL from refactored config
from .config import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/github")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "github-sensor-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
logging.getLogger("github").setLevel(logging.WARNING)  # PyGithub can be verbose
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/__main__.py">
import uvicorn
import logging
from .config import HOST, PORT

logger = logging.getLogger(__name__)

logger.info(f"GitHub sensor node starting on {HOST}:{PORT}")
uvicorn.run(
    "github_sensor_node.server:app",
    host=HOST,
    port=PORT,
    log_config=None,
    reload=True,
)
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/backfill.py">
import logging
from github import Github, GithubException, RateLimitExceededException
from github.Commit import Commit
from rid_lib.ext import Bundle

# Assuming GithubCommit RID type is accessible
from .types import GithubCommit
from .core import node

# Import necessary config values
from .config import GITHUB_TOKEN, MONITORED_REPOS, LAST_PROCESSED_SHA, update_state_file

logger = logging.getLogger(__name__)

# Initialize GitHub client (authenticated if token provided)
# Use the GITHUB_TOKEN loaded from config
github_client = Github(GITHUB_TOKEN) if GITHUB_TOKEN else Github()
logger.info(f"GitHub client initialized. Authenticated: {bool(GITHUB_TOKEN)}")


def perform_backfill():
    """
    One-time startup backfill: fetch all commits since LAST_PROCESSED_SHA
    for each monitored repo, bundle them as NEW, and persist the latest SHA processed.
    Processes commits oldest-to-newest after fetching.
    """
    logger.info("Starting GitHub backfill process...")

    # Load the state dictionary once before the loop
    current_state = (
        LAST_PROCESSED_SHA.copy()
    )  # Use a copy to avoid modifying during iteration if needed
    newest_sha_processed_overall_map = (
        {}
    )  # Track newest SHA per repo processed in this run

    if not MONITORED_REPOS:
        logger.warning(
            "No repositories configured in MONITORED_REPOS. Backfill skipped."
        )
        return

    for repo_full_name in MONITORED_REPOS:
        try:
            owner, repo_name_only = repo_full_name.split("/")
            # Get the last processed SHA for *this specific repository*
            last_sha_for_repo = current_state.get(repo_full_name)
            logger.info(
                f"Backfilling repository: {repo_full_name} since SHA: {last_sha_for_repo or 'beginning'}"
            )

            gh_repo = github_client.get_repo(repo_full_name)
            commits_to_process_buffer: list[Commit] = []

            # Iterate commits newest-first until we find the last processed one
            paginated_commits = gh_repo.get_commits()
            logger.debug(f"Fetching commits for {repo_full_name}...")
            commit_count = 0
            for commit in paginated_commits:
                commit_count += 1
                # Check against the specific SHA for this repo
                if last_sha_for_repo and commit.sha == last_sha_for_repo:
                    logger.info(
                        f"Found last processed SHA {last_sha_for_repo} in {repo_full_name}. Stopping fetch for this repo."
                    )
                    break
                commits_to_process_buffer.append(commit)
                # Safety break for potentially huge repos without a known SHA
                # Adjust limit as needed
                if commit_count % 100 == 0:
                    logger.debug(
                        f"Fetched {commit_count} commits for {repo_full_name} so far..."
                    )
                # if commit_count > 1000:
                #    logger.warning(f"Reached fetch limit (1000) for {repo_full_name}. Consider adjusting.")
                #    break

            logger.info(
                f"Found {len(commits_to_process_buffer)} new commits in {repo_full_name} to backfill."
            )

            # Process commits oldest → newest
            newest_sha_processed_in_repo = None
            for commit in reversed(commits_to_process_buffer):
                try:
                    rid = GithubCommit(owner=owner, repo=repo_name_only, sha=commit.sha)
                    # Extract commit details carefully, handling potential missing attributes
                    author = commit.commit.author
                    committer = commit.commit.committer

                    contents = {
                        "sha": commit.sha,
                        "message": commit.commit.message,
                        "author_name": author.name if author else None,
                        "author_email": author.email if author else None,
                        "author_date": (
                            author.date.isoformat() if author and author.date else None
                        ),
                        "committer_name": committer.name if committer else None,
                        "committer_email": committer.email if committer else None,
                        "committer_date": (
                            committer.date.isoformat()
                            if committer and committer.date
                            else None
                        ),
                        "html_url": commit.html_url,
                        "parents": [
                            p.sha for p in commit.parents
                        ],  # List of parent SHAs
                    }
                    bundle = Bundle.generate(rid=rid, contents=contents)
                    # CORRECT USAGE: 'handle' makes the bundle available locally
                    # in the sensor's cache/event queue for consumers to poll/fetch.
                    # It does NOT push the application-specific GithubCommit bundle directly.
                    logger.debug(
                        f"Making backfill commit bundle {rid} available locally via sensor API."
                    )
                    node.processor.handle(bundle=bundle)

                    # Track the newest SHA processed in this run for this repo
                    newest_sha_processed_in_repo = commit.sha

                except Exception as e:
                    logger.error(
                        f"Error processing commit {commit.sha} in {repo_full_name}: {e}",
                        exc_info=True,
                    )

            # Store the newest SHA processed for this repo during this run
            if newest_sha_processed_in_repo:
                newest_sha_processed_overall_map[repo_full_name] = (
                    newest_sha_processed_in_repo
                )
                logger.debug(
                    f"Newest SHA processed for {repo_full_name} in this run: {newest_sha_processed_in_repo}"
                )

        except RateLimitExceededException:
            logger.error(
                f"GitHub API rate limit exceeded while backfilling {repo_full_name}. Aborting backfill. Try again later or use a GITHUB_TOKEN."
            )
            # Depending on requirements, could wait and retry, but for now, we stop.
            return  # Stop the entire backfill process
        except GithubException as e:
            logger.error(
                f"GitHub API error for repository {repo_full_name}: {e}. Skipping this repo."
            )
            continue  # Skip to the next repository
        except Exception as e:
            logger.error(
                f"Unexpected error backfilling repository {repo_full_name}: {e}",
                exc_info=True,
            )
            continue  # Skip to the next repository

    # After processing all repos, persist the newest SHAs found (if changed)
    updated_count = 0
    for repo_full_name, newest_sha in newest_sha_processed_overall_map.items():
        if newest_sha != current_state.get(repo_full_name):
            update_state_file(
                repo_full_name, newest_sha
            )  # Call function from config.py
            updated_count += 1
        else:
            logger.debug(
                f"No state update needed for {repo_full_name}, newest SHA {newest_sha} is same as stored."
            )

    if updated_count > 0:
        logger.info(
            f"Backfill complete. Updated state for {updated_count} repositories."
        )
    else:
        logger.info(
            f"Backfill complete. No new commits found or state changes required across monitored repositories."
        )


if __name__ == "__main__":
    # Example of how to run backfill directly for testing
    # Requires node to be started if handle() depends on active components
    # In practice, this is called by server.py during startup
    logging.basicConfig(level=logging.INFO)
    logger.info("Running backfill directly for testing...")
    # node.start() # Might be needed depending on node.processor.handle implementation
    perform_backfill()
    # node.stop()
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/loader.py">
from .core import node
from .handlers import github


def register_handlers():
    print("Registering GITHUB handlers...")
    _ = github
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/server.py">
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, Request, Body, Header, HTTPException
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node
from .webhook import router as github_router
from .backfill import perform_backfill
from .loader import register_handlers

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup, backfill, and shutdown."""
    logger.info("Starting FastAPI application lifespan...")
    # Start the KOI-net node
    try:
        register_handlers()
        node.start()
        logger.info("KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        raise RuntimeError("Failed to initialize KOI-net node") from e

    logger.info("Scheduling initial GitHub backfill...")
    backfill_task = asyncio.to_thread(perform_backfill)

    try:
        yield  # Application runs here
    finally:
        logger.info("Shutting down FastAPI application...")
        # Attempt to gracefully cancel the backfill if it's still running
        # This might require more sophisticated task management if backfill is long-running
        # if backfill_task and not backfill_task.done():
        #     try:
        #         backfill_task.cancel()
        #         await backfill_task
        #     except asyncio.CancelledError:
        #         logger.info("Backfill task cancelled.")
        #     except Exception as e:
        #         logger.error(f"Error cancelling backfill task: {e}", exc_info=True)

        try:
            node.stop()
            logger.info("KOI-net node stopped successfully.")
        except Exception as e:
            logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
        logger.info("FastAPI application shutdown complete.")


app = FastAPI(
    title="KOI-net GitHub Sensor Node",
    description="Listens for GitHub webhooks and performs backfill to ingest commit data.",
    version="0.1.0",
    lifespan=lifespan,
)

koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    return {}


@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    return node.network.response_handler.fetch_rids(req)


@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    manifests_payload = node.network.response_handler.fetch_manifests(req)
    return manifests_payload  # The default handler already includes not_found


@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    bundles_payload = node.network.response_handler.fetch_bundles(req)
    return bundles_payload


@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed, e.g., node.is_running()
    return {"status": "healthy"}


app.include_router(koi_net_router)  # KOI-net API endpoints
app.include_router(github_router)  # GitHub webhook endpoint

logger.info("FastAPI application configured with webhook and KOI-net routers.")
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/types.py">
from rid_lib.core import ORN


class GithubCommit(ORN):
    """
    Resource Identifier (RID) for a specific GitHub commit.

    Format: orn:github.commit:<owner>/<repo>/<sha>
    Example: orn:github.commit:microsoft/vscode/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
    """

    namespace = "github.commit"

    def __init__(self, owner: str, repo: str, sha: str):
        """
        Initialize a GitHub commit RID.

        Args:
            owner: The repository owner (user or organization)
            repo: The repository name
            sha: The commit SHA (full 40-character or shortened)
        """
        if not owner or not repo or not sha:
            raise ValueError("Owner, repo, and SHA cannot be empty")

        if "/" in owner or "/" in repo:
            raise ValueError("Owner and repo cannot contain '/' character")

        self.owner = owner
        self.repo = repo
        self.sha = sha

    @property
    def reference(self) -> str:
        """Returns the reference part of the RID: '<owner>/<repo>/<sha>'."""
        return f"{self.owner}/{self.repo}/{self.sha}"

    @property
    def repository_full_name(self) -> str:
        """Returns the full repository name: '<owner>/<repo>'."""
        return f"{self.owner}/{self.repo}"

    @property
    def html_url(self) -> str:
        """Returns the HTML URL to view this commit on GitHub."""
        return f"https://github.com/{self.owner}/{self.repo}/commit/{self.sha}"

    @property
    def api_url(self) -> str:
        """Returns the GitHub API URL for this commit."""
        return (
            f"https://api.github.com/repos/{self.owner}/{self.repo}/commits/{self.sha}"
        )

    @classmethod
    def from_reference(cls, reference: str) -> "GithubCommit":
        """
        Creates a GithubCommit instance from its reference string.

        Args:
            reference: String in format '<owner>/<repo>/<sha>'

        Returns:
            GithubCommit instance

        Raises:
            ValueError: If the reference format is invalid
        """
        try:
            parts = reference.split("/", maxsplit=2)
            if len(parts) != 3:
                raise ValueError("Reference must contain exactly two '/' separators")

            owner, repo, sha = parts

            if not owner or not repo or not sha:
                raise ValueError("Owner, repo, and SHA parts cannot be empty")

            # Basic SHA length check
            if len(sha) < 7:  # Minimum length for a short SHA
                raise ValueError(f"SHA part seems too short: {sha}")

            return cls(owner=owner, repo=repo, sha=sha)

        except ValueError as e:
            raise ValueError(
                f"Invalid reference format for GithubCommit. Expected '<owner>/<repo>/<sha>', got '{reference}'. Error: {e}"
            ) from e
        except Exception as e:
            raise TypeError(
                f"Unexpected error parsing GithubCommit reference '{reference}': {e}"
            ) from e
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/webhook.py">
import logging
import hmac
import hashlib
import json
from fastapi import APIRouter, Request, Header, HTTPException, Body
from rid_lib.ext import Bundle
from .types import GithubCommit
from .core import node
from .config import (
    GITHUB_WEBHOOK_SECRET,
    MONITORED_REPOS,
    update_state_file,
    LAST_PROCESSED_SHA,
)

logger = logging.getLogger(__name__)

router = APIRouter()


async def verify_signature(request: Request, x_hub_signature_256: str = Header(None)):
    """Verify the GitHub webhook signature."""
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning("Webhook verification skipped: GITHUB_WEBHOOK_SECRET not set.")
        return

    body = await request.body()
    hash_object = hmac.new(
        GITHUB_WEBHOOK_SECRET.encode("utf-8"), msg=body, digestmod=hashlib.sha256
    )
    expected_signature = "sha256=" + hash_object.hexdigest()

    if not hmac.compare_digest(expected_signature, x_hub_signature_256):
        logger.error(
            f"Webhook verification failed: Invalid signature. Expected: {expected_signature}, Got: {x_hub_signature_256}"
        )
        raise HTTPException(status_code=403, detail="Invalid signature")

    logger.debug("Webhook signature verified successfully.")


@router.post("/github/webhook", status_code=202)  # Use 202 Accepted as we process async
async def github_webhook(
    request: Request,
    x_github_event: str = Header(...),  # Required header
    x_hub_signature_256: str = Header(...),  # Required for verification
):
    """Handle incoming GitHub webhook events (specifically 'push')."""
    logger.info(f"Received GitHub webhook event: {x_github_event}")

    try:
        # --- Signature Verification (Optional) ---
        # await verify_signature(request, x_hub_signature_256)

        # --- Parse JSON Payload ---
        raw_body = await request.body()
        try:
            payload = json.loads(raw_body)
        except json.JSONDecodeError:
            logger.error("Invalid JSON in GitHub webhook payload")
            raise HTTPException(status_code=400, detail="Invalid JSON")

        # --- Event Handling ---
        if x_github_event == "ping":
            logger.info("Received 'ping' event from GitHub. Responding OK.")
            return {"message": "Pong!"}

        if x_github_event != "push":
            logger.debug(f"Ignoring non-'push' event: {x_github_event}")
            return {"message": f"Ignoring event type: {x_github_event}"}

        logger.info(f"Processing 'push' event: {payload}")

        # --- Process 'push' Event ---
        repo_info = payload.get("repository", {})
        repo_full_name = repo_info.get("full_name")
        repo_owner = repo_info.get("owner", {}).get("login") or repo_info.get(
            "owner", {}
        ).get("name")
        repo_name = repo_info.get("name")
        commits = payload.get("commits", [])
        head_commit = payload.get("head_commit", {})

        if not repo_full_name or not repo_owner or not repo_name:
            logger.error(f"Webhook payload missing repository details: {repo_info}")
            raise HTTPException(
                status_code=400, detail="Missing repository information in payload"
            )

        # Check if the repository is monitored
        if repo_full_name not in MONITORED_REPOS:
            logger.debug(
                f"Ignoring push event for non-monitored repository: {repo_full_name}"
            )
            return {"message": f"Repository {repo_full_name} not monitored"}

        if not commits and not head_commit:
            logger.warning(
                f"'push' event for {repo_full_name} received without 'commits' or 'head_commit' data. Possibly a branch deletion or tag push? Payload head: {payload.get('ref', '')}"
            )
            return {"message": "No commit data found in push event"}

        # Determine the commit(s) to process and the SHA to potentially update state with
        commits_to_process = []
        sha_to_update_state = None

        head_commit_id = head_commit.get("id")
        if head_commit_id:
            commits_to_process = [head_commit]  # Use head_commit as the primary source
            sha_to_update_state = (
                head_commit_id  # This is the SHA representing the push tip
            )
            logger.debug(f"Processing head_commit: {head_commit_id}")
        elif commits:
            commits_to_process = commits  # Fallback to commits list
            # If using commits list, the last commit's SHA is the best candidate for state update
            if commits:
                sha_to_update_state = commits[-1].get("id")
            logger.debug(
                f"Processing commits list (count: {len(commits)}). Potential state update SHA: {sha_to_update_state}"
            )
        else:
            # This case should ideally not be reached due to the check above, but included for completeness
            logger.warning(
                f"No processable commit data found in push event for {repo_full_name}. Skipping."
            )
            return {"message": "No processable commit data"}

        processed_new_commit = False
        for commit in commits_to_process:
            commit_sha = commit.get("id")
            if not commit_sha:
                logger.warning("Skipping commit in payload with missing 'id'.")
                continue

            # Avoid reprocessing the last known SHA for this repo
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if last_known_sha_for_repo and commit_sha == last_known_sha_for_repo:
                logger.debug(
                    f"Skipping commit {commit_sha} for {repo_full_name} as it matches last known SHA {last_known_sha_for_repo}."
                )
                continue

            # Inner try-except for processing individual commits within the push
            try:
                # Construct RID
                rid = GithubCommit(owner=repo_owner, repo=repo_name, sha=commit_sha)

                # Extract details - ensure keys exist
                author = commit.get("author", {})
                committer = commit.get("committer", {})

                contents = {
                    "sha": commit_sha,
                    "message": commit.get("message"),
                    "author_name": author.get("name"),
                    "author_email": author.get("email"),
                    "author_date": commit.get(
                        "timestamp"
                    ),  # GitHub often uses 'timestamp'
                    "committer_name": committer.get("name"),
                    "committer_email": committer.get("email"),
                    "committer_date": committer.get("timestamp"),
                    "html_url": commit.get("url"),  # Use 'url' from webhook payload
                    "parents": commit.get(
                        "parents", []
                    ),  # Typically a list of SHAs in webhook
                }

                bundle = Bundle.generate(rid=rid, contents=contents)
                # CORRECT USAGE: 'handle' makes the bundle available locally
                # in the sensor's cache/event queue for consumers to poll/fetch.
                # It does NOT push the application-specific GithubCommit bundle directly.
                logger.debug(
                    f"Making webhook commit bundle {rid} available locally via sensor API."
                )
                node.processor.handle(bundle=bundle)

                processed_new_commit = (
                    True  # Mark that we processed at least one new commit
                )

            except Exception as e:
                logger.error(
                    f"Error processing webhook commit {commit_sha} for {repo_full_name}: {e}",
                    exc_info=True,
                )
                # Decide whether to continue processing other commits in the push or stop
                continue  # Continue with next commit in the webhook push

        # Update state file only if we processed a new commit and have a valid SHA representing the push tip
        if processed_new_commit and sha_to_update_state:
            # Check again if the sha_to_update is different from the stored one before writing
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if sha_to_update_state != last_known_sha_for_repo:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. Updating state to SHA: {sha_to_update_state}"
                )
                update_state_file(repo_full_name, sha_to_update_state)
            else:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. State SHA {sha_to_update_state} already stored."
                )
        elif processed_new_commit:
            logger.warning(
                f"Webhook processing complete for {repo_full_name}. Processed new commit(s) but could not determine SHA for state update."
            )
        else:
            logger.info(
                f"Webhook processing complete for {repo_full_name}. No new commits processed or state updated."
            )

        return {"message": "Webhook processed successfully"}

    # Exception handlers are now correctly indented relative to the main 'try' block
    except HTTPException as he:
        # Re-raise HTTP exceptions to return proper status codes
        logger.warning(f"HTTP Exception during webhook processing: {he.detail}")
        raise he
    except Exception as e:
        logger.error(
            f"Unexpected error handling webhook event {x_github_event}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Internal server error handling webhook"
        )
</file>

<file path="nodes/koi-net-github-sensor-node/.env.example">
# KOI-net GitHub Node Configuration

# Public URL where this GitHub node can be reached by other KOI-net nodes
URL="http://127.0.0.1:8001"

# URL of the coordinator node or another known KOI-net node
FIRST_CONTACT="http://127.0.0.1:8000/koi-net"

# Host the FastAPI server should listen on (0.0.0.0 allows external connections)
HOST="0.0.0.0"

# Port the FastAPI server should listen on
PORT="8001"

# GitHub Personal Access Token (replace with your own token)
GITHUB_TOKEN="your_github_token_here"

# Secret used to verify webhook signatures (generate a secure random string)
GITHUB_WEBHOOK_SECRET="your_webhook_secret_here"

# Comma-separated list of repositories to monitor (format: owner/repo)
MONITORED_REPOS="owner/repo1,owner/repo2"

# Path to the JSON file storing the last processed commit SHA
STATE_FILE_PATH="state.json"

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL="INFO"
</file>

<file path="nodes/koi-net-github-sensor-node/.gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
</file>

<file path="nodes/koi-net-github-sensor-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy pyproject and perform editable installation
COPY pyproject.toml /app/

# 3) install curl for healthcheck
RUN apt-get update && apt-get install -y curl

# 4) install dependencies using UV and pyproject.toml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 5) copy in code
COPY . /app/

# 6) expose port and set environment
EXPOSE 8001

# 7) add healthcheck using the main app's endpoint
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8001/koi-net/health || exit 1

# 8) start the main service using shell form for ENV var substitution
CMD ["uvicorn", "github_sensor_node.server:app", "--host", "0.0.0.0", "--port", "8001"]
</file>

<file path="nodes/koi-net-github-sensor-node/github-node.service">
[Unit]
Description=KOI-net Github Node Service
After=network.target

[Service]
WorkingDirectory=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/services/github
ExecStart=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/.venv/bin/python3 -m node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="nodes/koi-net-github-sensor-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "github-sensor-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.3",
    "koi-net==1.0.0b12",
    "PyGithub",
    "aiohttp",
    "rich", # Added for logging
    "ruamel.yaml", # Added for YAML config loading
    "python-dotenv" # Added for loading .env in local runs
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="nodes/koi-net-github-sensor-node/repomix-output.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
github_sensor_node/
  handlers/
    github.py
  __init__.py
  __main__.py
  backfill.py
  config.py
  core.py
  loader.py
  server.py
  types.py
  webhook.py
.env.example
.gitignore
Dockerfile
github-node.service
pyproject.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="github_sensor_node/handlers/github.py">
import logging
from ..core import node
from ..types import GithubCommit

from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of the coordinator node (or other nodes providing KoiNetNode events).

    On discovering a NEW coordinator, proposes a WEBHOOK edge for bidirectional
    communication and requests a list of other known nodes (sync).
    (Based on refactor.md example)
    """
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if KoiNetNode not in profile.provides.event:
            logger.debug(
                f"Node {kobj.rid} does not provide KoiNetNode events. Ignoring."
            )
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    logger.info(
        f"Identified potential coordinator/peer: {kobj.rid}; proposing WEBHOOK edge"
    )
    try:

        edge_bundle = generate_edge_bundle(
            source=kobj.rid,
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode],
        )
        processor.handle(bundle=edge_bundle)
    except Exception as e:
        logger.error(
            f"Failed to generate or handle WEBHOOK edge bundle for {kobj.rid}: {e}",
            exc_info=True,
        )

    logger.info(f"Syncing network nodes from {kobj.rid}")
    try:
        payload = processor.network.request_handler.fetch_rids(
            kobj.rid, rid_types=[KoiNetNode]
        )
        if not payload or not payload.rids:
            logger.warning(f"Received empty RIDs payload from {kobj.rid} during sync.")
            return

        logger.debug(f"Received {len(payload.rids)} RIDs from {kobj.rid}")
        for rid in payload.rids:

            if rid == processor.identity.rid or processor.cache.exists(rid):
                continue
            logger.debug(f"Handling discovered RID from sync: {rid}")

            processor.handle(rid=rid, source=KnowledgeSource.External)
    except Exception as e:
        logger.error(f"Failed during network sync with {kobj.rid}: {e}", exc_info=True)


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit(processor: ProcessorInterface, kobj: KnowledgeObject):
    """
    Basic handler for processing GithubCommit bundles.
    Currently just logs information.

    Args:
        bundle: The Bundle object containing the GithubCommit RID and contents.
    """
    try:

        bundle = kobj.bundle
        rid: GithubCommit = bundle.rid
        contents: dict = bundle.contents

        logger.info(
            f"Processing commit: {rid} (Normalized Type: {kobj.normalized_event_type}, Source: {kobj.source})"
        )

        logger.debug(
            f"  Author: {contents.get('author_name')} <{contents.get('author_email')}>"
        )
        logger.debug(
            f"  Message: {contents.get('message', '').splitlines()[0][:80]}..."
        )
        logger.debug(f"  URL: {contents.get('html_url')}")

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Example handler demonstrating processing before cache write."""
    logger.debug(f"Handling GithubCommit bundle PRE-CACHE for {kobj.rid}")

    try:

        if kobj.contents and "message" in kobj.contents:
            logger.info(
                f"Processing commit {kobj.rid.sha[:7]}: {kobj.contents['message'].splitlines()[0]}"
            )

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


logger.info("GithubCommit Bundle handler registered.")
</file>

<file path="github_sensor_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path

# Import LOG_LEVEL from refactored config
from .config import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/github")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "github-sensor-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
logging.getLogger("github").setLevel(logging.WARNING)  # PyGithub can be verbose
</file>

<file path="github_sensor_node/__main__.py">
import uvicorn
import logging
from .config import HOST, PORT

logger = logging.getLogger(__name__)

logger.info(f"GitHub sensor node starting on {HOST}:{PORT}")
uvicorn.run(
    "github_sensor_node.server:app",
    host=HOST,
    port=PORT,
    log_config=None,
    reload=True,
)
</file>

<file path="github_sensor_node/backfill.py">
import logging
from github import Github, GithubException, RateLimitExceededException
from github.Commit import Commit
from rid_lib.ext import Bundle

# Assuming GithubCommit RID type is accessible
from .types import GithubCommit
from .core import node

# Import necessary config values
from .config import GITHUB_TOKEN, MONITORED_REPOS, LAST_PROCESSED_SHA, update_state_file

logger = logging.getLogger(__name__)

# Initialize GitHub client (authenticated if token provided)
# Use the GITHUB_TOKEN loaded from config
github_client = Github(GITHUB_TOKEN) if GITHUB_TOKEN else Github()
logger.info(f"GitHub client initialized. Authenticated: {bool(GITHUB_TOKEN)}")


def perform_backfill():
    """
    One-time startup backfill: fetch all commits since LAST_PROCESSED_SHA
    for each monitored repo, bundle them as NEW, and persist the latest SHA processed.
    Processes commits oldest-to-newest after fetching.
    """
    logger.info("Starting GitHub backfill process...")

    # Load the state dictionary once before the loop
    current_state = (
        LAST_PROCESSED_SHA.copy()
    )  # Use a copy to avoid modifying during iteration if needed
    newest_sha_processed_overall_map = (
        {}
    )  # Track newest SHA per repo processed in this run

    if not MONITORED_REPOS:
        logger.warning(
            "No repositories configured in MONITORED_REPOS. Backfill skipped."
        )
        return

    for repo_full_name in MONITORED_REPOS:
        try:
            owner, repo_name_only = repo_full_name.split("/")
            # Get the last processed SHA for *this specific repository*
            last_sha_for_repo = current_state.get(repo_full_name)
            logger.info(
                f"Backfilling repository: {repo_full_name} since SHA: {last_sha_for_repo or 'beginning'}"
            )

            gh_repo = github_client.get_repo(repo_full_name)
            commits_to_process_buffer: list[Commit] = []

            # Iterate commits newest-first until we find the last processed one
            paginated_commits = gh_repo.get_commits()
            logger.debug(f"Fetching commits for {repo_full_name}...")
            commit_count = 0
            for commit in paginated_commits:
                commit_count += 1
                # Check against the specific SHA for this repo
                if last_sha_for_repo and commit.sha == last_sha_for_repo:
                    logger.info(
                        f"Found last processed SHA {last_sha_for_repo} in {repo_full_name}. Stopping fetch for this repo."
                    )
                    break
                commits_to_process_buffer.append(commit)
                # Safety break for potentially huge repos without a known SHA
                # Adjust limit as needed
                if commit_count % 100 == 0:
                    logger.debug(
                        f"Fetched {commit_count} commits for {repo_full_name} so far..."
                    )
                # if commit_count > 1000:
                #    logger.warning(f"Reached fetch limit (1000) for {repo_full_name}. Consider adjusting.")
                #    break

            logger.info(
                f"Found {len(commits_to_process_buffer)} new commits in {repo_full_name} to backfill."
            )

            # Process commits oldest → newest
            newest_sha_processed_in_repo = None
            for commit in reversed(commits_to_process_buffer):
                try:
                    rid = GithubCommit(owner=owner, repo=repo_name_only, sha=commit.sha)
                    # Extract commit details carefully, handling potential missing attributes
                    author = commit.commit.author
                    committer = commit.commit.committer

                    contents = {
                        "sha": commit.sha,
                        "message": commit.commit.message,
                        "author_name": author.name if author else None,
                        "author_email": author.email if author else None,
                        "author_date": (
                            author.date.isoformat() if author and author.date else None
                        ),
                        "committer_name": committer.name if committer else None,
                        "committer_email": committer.email if committer else None,
                        "committer_date": (
                            committer.date.isoformat()
                            if committer and committer.date
                            else None
                        ),
                        "html_url": commit.html_url,
                        "parents": [
                            p.sha for p in commit.parents
                        ],  # List of parent SHAs
                    }
                    bundle = Bundle.generate(rid=rid, contents=contents)
                    # CORRECT USAGE: 'handle' makes the bundle available locally
                    # in the sensor's cache/event queue for consumers to poll/fetch.
                    # It does NOT push the application-specific GithubCommit bundle directly.
                    logger.debug(
                        f"Making backfill commit bundle {rid} available locally via sensor API."
                    )
                    node.processor.handle(bundle=bundle)

                    # Track the newest SHA processed in this run for this repo
                    newest_sha_processed_in_repo = commit.sha

                except Exception as e:
                    logger.error(
                        f"Error processing commit {commit.sha} in {repo_full_name}: {e}",
                        exc_info=True,
                    )

            # Store the newest SHA processed for this repo during this run
            if newest_sha_processed_in_repo:
                newest_sha_processed_overall_map[repo_full_name] = (
                    newest_sha_processed_in_repo
                )
                logger.debug(
                    f"Newest SHA processed for {repo_full_name} in this run: {newest_sha_processed_in_repo}"
                )

        except RateLimitExceededException:
            logger.error(
                f"GitHub API rate limit exceeded while backfilling {repo_full_name}. Aborting backfill. Try again later or use a GITHUB_TOKEN."
            )
            # Depending on requirements, could wait and retry, but for now, we stop.
            return  # Stop the entire backfill process
        except GithubException as e:
            logger.error(
                f"GitHub API error for repository {repo_full_name}: {e}. Skipping this repo."
            )
            continue  # Skip to the next repository
        except Exception as e:
            logger.error(
                f"Unexpected error backfilling repository {repo_full_name}: {e}",
                exc_info=True,
            )
            continue  # Skip to the next repository

    # After processing all repos, persist the newest SHAs found (if changed)
    updated_count = 0
    for repo_full_name, newest_sha in newest_sha_processed_overall_map.items():
        if newest_sha != current_state.get(repo_full_name):
            update_state_file(
                repo_full_name, newest_sha
            )  # Call function from config.py
            updated_count += 1
        else:
            logger.debug(
                f"No state update needed for {repo_full_name}, newest SHA {newest_sha} is same as stored."
            )

    if updated_count > 0:
        logger.info(
            f"Backfill complete. Updated state for {updated_count} repositories."
        )
    else:
        logger.info(
            f"Backfill complete. No new commits found or state changes required across monitored repositories."
        )


if __name__ == "__main__":
    # Example of how to run backfill directly for testing
    # Requires node to be started if handle() depends on active components
    # In practice, this is called by server.py during startup
    logging.basicConfig(level=logging.INFO)
    logger.info("Running backfill directly for testing...")
    # node.start() # Might be needed depending on node.processor.handle implementation
    perform_backfill()
    # node.stop()
</file>

<file path="github_sensor_node/config.py">
import logging
import json
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Load Environment Variables First ---
# Define potential paths for global.env
DOCKER_ENV_PATH = Path("/app/config/global.env")  # Path inside docker
LOCAL_ENV_PATH = Path(__file__).parent.parent.parent.parent / "config" / "global.env"

if DOCKER_ENV_PATH.is_file():
    load_dotenv(dotenv_path=DOCKER_ENV_PATH, override=True)
    logger.debug(f"Loaded environment variables from Docker path: {DOCKER_ENV_PATH}")
elif LOCAL_ENV_PATH.is_file():
    load_dotenv(dotenv_path=LOCAL_ENV_PATH, override=True)
    logger.debug(f"Loaded environment variables from Local path: {LOCAL_ENV_PATH}")
else:
    logger.warning(
        f"global.env file not found at Docker path {DOCKER_ENV_PATH} or local path {LOCAL_ENV_PATH}. Secrets might be missing."
    )
# --- End Environment Variable Loading ---


# Define potential configuration file paths for github-sensor.yaml
DOCKER_CONFIG_PATH = Path("/app/config/github-sensor.yaml")
LOCAL_CONFIG_PATH = (
    Path(__file__).parent.parent.parent.parent / "config" / "github-sensor.yaml"
)


def find_config_path() -> Path | None:
    """Finds the valid configuration file path."""
    if DOCKER_CONFIG_PATH.is_file():
        logger.debug(f"Using Docker config path: {DOCKER_CONFIG_PATH}")
        return DOCKER_CONFIG_PATH
    elif LOCAL_CONFIG_PATH.is_file():
        logger.debug(f"Using local config path: {LOCAL_CONFIG_PATH}")
        return LOCAL_CONFIG_PATH
    else:
        logger.error(
            f"Configuration file not found at Docker path {DOCKER_CONFIG_PATH} or local path {LOCAL_CONFIG_PATH}"
        )
        return None


def load_yaml_config() -> Dict[str, Any]:
    """Loads configuration from the YAML file."""
    config_path = find_config_path()
    if not config_path:
        logger.error("No valid YAML configuration file found. Using empty default.")
        return {}

    try:
        yaml = YAML(typ="safe")
        with open(config_path, "r") as f:
            config_data = yaml.load(f)
        logger.info(f"Successfully loaded YAML configuration from {config_path}")
        return config_data
    except Exception as e:
        logger.exception(f"Error loading YAML configuration from {config_path}: {e}")
        return {}


# Load YAML configuration
CONFIG = load_yaml_config()

# --- Determine Run Context ---
is_docker = os.getenv("RUN_CONTEXT") == "docker"

# --- Extract specific configurations with defaults ---
SENSOR_CONFIG: Dict[str, Any] = CONFIG.get("sensor", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
WEBHOOK_CONFIG: Dict[str, Any] = CONFIG.get("webhook", {})

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/github")  # Relative to workspace root for local runs

# Base configuration values
MONITORED_REPOS: List[str] = SENSOR_CONFIG.get("repos", [])
POLL_INTERVAL: int = SENSOR_CONFIG.get("poll_interval", 60)
SENSOR_MODE: str = SENSOR_CONFIG.get("mode", "webhook")
HOST: str = RUNTIME_CONFIG.get("host", "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8001)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()

# Adjust URLs
raw_coordinator_url = EDGES_CONFIG.get("coordinator_url")
if not is_docker and raw_coordinator_url:
    COORDINATOR_URL = raw_coordinator_url.replace(
        "http://coordinator:", f"http://localhost:"
    )
    logger.debug(f"Adjusted Coordinator URL for local run: {COORDINATOR_URL}")
else:
    COORDINATOR_URL = raw_coordinator_url

raw_base_url = RUNTIME_CONFIG.get("base_url")
if not is_docker and raw_base_url:
    BASE_URL = raw_base_url.replace("http://github-sensor:", f"http://localhost:")
    logger.debug(f"Adjusted Base URL for local run: {BASE_URL}")
else:
    BASE_URL = raw_base_url

# Adjust Cache and State Paths
DOCKER_CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", "/data/cache")
# Read state file path from YAML, default to a path within CACHE_DIR if not specified
DOCKER_STATE_FILE = RUNTIME_CONFIG.get(
    "state_file", os.path.join(DOCKER_CACHE_DIR, "github_state.json")
)

if is_docker:
    CACHE_DIR = DOCKER_CACHE_DIR
    # Ensure state file path is absolute for Docker
    STATE_FILE_PATH = (
        Path(DOCKER_STATE_FILE)
        if Path(DOCKER_STATE_FILE).is_absolute()
        else Path("/app") / DOCKER_STATE_FILE
    )
else:
    LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)  # Ensure local base dir exists
    CACHE_DIR = str(LOCAL_DATA_BASE / "cache")
    STATE_FILE_PATH = LOCAL_DATA_BASE / "github_state.json"  # Standardized local path
    logger.debug(f"Using local CACHE_DIR: {CACHE_DIR}")
    logger.debug(f"Using local STATE_FILE_PATH: {STATE_FILE_PATH}")

# Ensure resolved CACHE_DIR exists, especially for local runs
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Load Secrets from Environment Variables ---
GITHUB_TOKEN: str | None = os.getenv("GITHUB_TOKEN")
WEBHOOK_SECRET_ENV_VAR: str | None = WEBHOOK_CONFIG.get("secret_env_var")
GITHUB_WEBHOOK_SECRET: str | None = None
if WEBHOOK_SECRET_ENV_VAR:
    GITHUB_WEBHOOK_SECRET = os.getenv(WEBHOOK_SECRET_ENV_VAR)
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning(
            f"Environment variable '{WEBHOOK_SECRET_ENV_VAR}' specified in config but not found in environment."
        )
else:
    logger.warning("webhook.secret_env_var not specified in github-sensor.yaml")

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL)
    # Optionally set levels for other loggers
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logging.getLogger("github").setLevel(logging.WARNING)  # PyGithub can be verbose
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("GitHub Sensor Configuration Loaded:")
logger.info(f"  Sensor Mode: {SENSOR_MODE}")
logger.info(f"  Monitored Repos: {MONITORED_REPOS}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  State File Path: {STATE_FILE_PATH}")  # Log the final path
logger.info(f"  GitHub Token Loaded: {bool(GITHUB_TOKEN)}")
logger.info(f"  Webhook Secret Loaded: {bool(GITHUB_WEBHOOK_SECRET)}")

# Check required config
if not COORDINATOR_URL:
    logger.critical(
        "Configuration error: edges.coordinator_url is not set or resolved correctly."
    )
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not GITHUB_WEBHOOK_SECRET:
    logger.warning("Configuration warning: GitHub Webhook Secret not loaded.")

# --- State Management (Loading initial state & update function) ---
LAST_PROCESSED_SHA: Dict[str, str] = {}  # Dictionary mapping repo_name -> last_sha


def load_state():
    """Loads the last processed SHA state from the JSON file specified by STATE_FILE_PATH."""
    global LAST_PROCESSED_SHA
    # Path is now guaranteed to be a Path object
    state_path = STATE_FILE_PATH
    try:
        with open(state_path, "r") as f:
            LAST_PROCESSED_SHA = json.load(f)
        logger.info(
            f"Loaded state from '{state_path}': Repos {list(LAST_PROCESSED_SHA.keys())}"
        )
    except FileNotFoundError:
        logger.warning(
            f"State file '{state_path}' not found. Starting with empty state."
        )
        LAST_PROCESSED_SHA = {}
    except json.JSONDecodeError:
        logger.error(
            f"Error decoding JSON from state file '{state_path}'. Starting with empty state."
        )
        LAST_PROCESSED_SHA = {}
    except Exception as e:
        logger.error(
            f"Unexpected error loading state file '{state_path}': {e}",
            exc_info=True,
        )
        LAST_PROCESSED_SHA = {}


def update_state_file(repo_name: str, last_sha: str):
    """Updates the state file with the latest processed SHA for a repo."""
    global LAST_PROCESSED_SHA
    LAST_PROCESSED_SHA[repo_name] = last_sha

    state_path = STATE_FILE_PATH  # Use the context-aware path
    try:
        # Ensure directory exists before writing
        state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(state_path, "w") as f:
            json.dump(LAST_PROCESSED_SHA, f, indent=4)
        logger.debug(
            f"Updated state file '{state_path}' for {repo_name} with SHA: {last_sha}"
        )
    except IOError as e:
        logger.error(f"Failed to write state file '{state_path}': {e}")
    except Exception as e:
        logger.error(
            f"Unexpected error writing state file '{state_path}': {e}",
            exc_info=True,
        )


# Load initial state when config module is imported
load_state()
</file>

<file path="github_sensor_node/core.py">
import os
import shutil
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the refactored config module
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
    LOG_LEVEL,
)  # Import necessary vars
from .types import GithubCommit

logger = logging.getLogger(__name__)

name = "github"

# Identity and cache directory paths remain relative for setup,
# but NodeInterface gets absolute paths derived from config
identity_dir = f".koi/{name}"
# Cache dir is now defined in config, use that path for NodeInterface
# cache_dir_setup = f".koi/{name}/rid_cache_{name}"

# Clear existing state directories logic remains the same
# Consider making this behavior optional or configurable
logger.info(f"Attempting to clear existing identity directory: {identity_dir}")
shutil.rmtree(identity_dir, ignore_errors=True)
# Cache dir managed by Docker volume, no need to clear here
# shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the identity directory
os.makedirs(identity_dir, exist_ok=True)
# Ensure cache directory exists within the container is handled by Docker volume mount implicitly
# os.makedirs(cache_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Ensure required config values are present
if not BASE_URL:
    raise ValueError("Runtime base_url is not configured in github-sensor.yaml")
if not COORDINATOR_URL:
    raise ValueError("Edges coordinator_url is not configured in github-sensor.yaml")

# Initialize the KOI-net Node Interface for the GitHub Sensor
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        # Use BASE_URL from config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[GithubCommit],
            state=[GithubCommit],
        ),
    ),
    use_kobj_processor_thread=True,
    # Use COORDINATOR_URL from config for first_contact
    first_contact=COORDINATOR_URL,
    # State file paths are now relative to the application root in the container
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Use CACHE_DIR from config (should be /data/cache)
    cache_directory_path=CACHE_DIR,
)

logger.info(f"Initialized NodeInterface: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")
</file>

<file path="github_sensor_node/loader.py">
from .core import node
from .handlers import github


def register_handlers():
    print("Registering GITHUB handlers...")
    _ = github
</file>

<file path="github_sensor_node/server.py">
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, Request, Body, Header, HTTPException
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node
from .webhook import router as github_router
from .backfill import perform_backfill
from .loader import register_handlers

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup, backfill, and shutdown."""
    logger.info("Starting FastAPI application lifespan...")
    # Start the KOI-net node
    try:
        register_handlers()
        node.start()
        logger.info("KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        raise RuntimeError("Failed to initialize KOI-net node") from e

    logger.info("Scheduling initial GitHub backfill...")
    backfill_task = asyncio.to_thread(perform_backfill)

    try:
        yield  # Application runs here
    finally:
        logger.info("Shutting down FastAPI application...")
        # Attempt to gracefully cancel the backfill if it's still running
        # This might require more sophisticated task management if backfill is long-running
        # if backfill_task and not backfill_task.done():
        #     try:
        #         backfill_task.cancel()
        #         await backfill_task
        #     except asyncio.CancelledError:
        #         logger.info("Backfill task cancelled.")
        #     except Exception as e:
        #         logger.error(f"Error cancelling backfill task: {e}", exc_info=True)

        try:
            node.stop()
            logger.info("KOI-net node stopped successfully.")
        except Exception as e:
            logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
        logger.info("FastAPI application shutdown complete.")


app = FastAPI(
    title="KOI-net GitHub Sensor Node",
    description="Listens for GitHub webhooks and performs backfill to ingest commit data.",
    version="0.1.0",
    lifespan=lifespan,
)

koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    return {}


@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    return node.network.response_handler.fetch_rids(req)


@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    manifests_payload = node.network.response_handler.fetch_manifests(req)
    return manifests_payload  # The default handler already includes not_found


@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    bundles_payload = node.network.response_handler.fetch_bundles(req)
    return bundles_payload


@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed, e.g., node.is_running()
    return {"status": "healthy"}


app.include_router(koi_net_router)  # KOI-net API endpoints
app.include_router(github_router)  # GitHub webhook endpoint

logger.info("FastAPI application configured with webhook and KOI-net routers.")
</file>

<file path="github_sensor_node/types.py">
from rid_lib.core import ORN


class GithubCommit(ORN):
    """
    Resource Identifier (RID) for a specific GitHub commit.

    Format: orn:github.commit:<owner>/<repo>/<sha>
    Example: orn:github.commit:microsoft/vscode/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
    """

    namespace = "github.commit"

    def __init__(self, owner: str, repo: str, sha: str):
        """
        Initialize a GitHub commit RID.

        Args:
            owner: The repository owner (user or organization)
            repo: The repository name
            sha: The commit SHA (full 40-character or shortened)
        """
        if not owner or not repo or not sha:
            raise ValueError("Owner, repo, and SHA cannot be empty")

        if "/" in owner or "/" in repo:
            raise ValueError("Owner and repo cannot contain '/' character")

        self.owner = owner
        self.repo = repo
        self.sha = sha

    @property
    def reference(self) -> str:
        """Returns the reference part of the RID: '<owner>/<repo>/<sha>'."""
        return f"{self.owner}/{self.repo}/{self.sha}"

    @property
    def repository_full_name(self) -> str:
        """Returns the full repository name: '<owner>/<repo>'."""
        return f"{self.owner}/{self.repo}"

    @property
    def html_url(self) -> str:
        """Returns the HTML URL to view this commit on GitHub."""
        return f"https://github.com/{self.owner}/{self.repo}/commit/{self.sha}"

    @property
    def api_url(self) -> str:
        """Returns the GitHub API URL for this commit."""
        return (
            f"https://api.github.com/repos/{self.owner}/{self.repo}/commits/{self.sha}"
        )

    @classmethod
    def from_reference(cls, reference: str) -> "GithubCommit":
        """
        Creates a GithubCommit instance from its reference string.

        Args:
            reference: String in format '<owner>/<repo>/<sha>'

        Returns:
            GithubCommit instance

        Raises:
            ValueError: If the reference format is invalid
        """
        try:
            parts = reference.split("/", maxsplit=2)
            if len(parts) != 3:
                raise ValueError("Reference must contain exactly two '/' separators")

            owner, repo, sha = parts

            if not owner or not repo or not sha:
                raise ValueError("Owner, repo, and SHA parts cannot be empty")

            # Basic SHA length check
            if len(sha) < 7:  # Minimum length for a short SHA
                raise ValueError(f"SHA part seems too short: {sha}")

            return cls(owner=owner, repo=repo, sha=sha)

        except ValueError as e:
            raise ValueError(
                f"Invalid reference format for GithubCommit. Expected '<owner>/<repo>/<sha>', got '{reference}'. Error: {e}"
            ) from e
        except Exception as e:
            raise TypeError(
                f"Unexpected error parsing GithubCommit reference '{reference}': {e}"
            ) from e
</file>

<file path="github_sensor_node/webhook.py">
import logging
import hmac
import hashlib
import json
from fastapi import APIRouter, Request, Header, HTTPException, Body
from rid_lib.ext import Bundle
from .types import GithubCommit
from .core import node
from .config import (
    GITHUB_WEBHOOK_SECRET,
    MONITORED_REPOS,
    update_state_file,
    LAST_PROCESSED_SHA,
)

logger = logging.getLogger(__name__)

router = APIRouter()


async def verify_signature(request: Request, x_hub_signature_256: str = Header(None)):
    """Verify the GitHub webhook signature."""
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning("Webhook verification skipped: GITHUB_WEBHOOK_SECRET not set.")
        return

    body = await request.body()
    hash_object = hmac.new(
        GITHUB_WEBHOOK_SECRET.encode("utf-8"), msg=body, digestmod=hashlib.sha256
    )
    expected_signature = "sha256=" + hash_object.hexdigest()

    if not hmac.compare_digest(expected_signature, x_hub_signature_256):
        logger.error(
            f"Webhook verification failed: Invalid signature. Expected: {expected_signature}, Got: {x_hub_signature_256}"
        )
        raise HTTPException(status_code=403, detail="Invalid signature")

    logger.debug("Webhook signature verified successfully.")


@router.post("/github/webhook", status_code=202)  # Use 202 Accepted as we process async
async def github_webhook(
    request: Request,
    x_github_event: str = Header(...),  # Required header
    x_hub_signature_256: str = Header(...),  # Required for verification
):
    """Handle incoming GitHub webhook events (specifically 'push')."""
    logger.info(f"Received GitHub webhook event: {x_github_event}")

    try:
        # --- Signature Verification (Optional) ---
        # await verify_signature(request, x_hub_signature_256)

        # --- Parse JSON Payload ---
        raw_body = await request.body()
        try:
            payload = json.loads(raw_body)
        except json.JSONDecodeError:
            logger.error("Invalid JSON in GitHub webhook payload")
            raise HTTPException(status_code=400, detail="Invalid JSON")

        # --- Event Handling ---
        if x_github_event == "ping":
            logger.info("Received 'ping' event from GitHub. Responding OK.")
            return {"message": "Pong!"}

        if x_github_event != "push":
            logger.debug(f"Ignoring non-'push' event: {x_github_event}")
            return {"message": f"Ignoring event type: {x_github_event}"}

        logger.info(f"Processing 'push' event: {payload}")

        # --- Process 'push' Event ---
        repo_info = payload.get("repository", {})
        repo_full_name = repo_info.get("full_name")
        repo_owner = repo_info.get("owner", {}).get("login") or repo_info.get(
            "owner", {}
        ).get("name")
        repo_name = repo_info.get("name")
        commits = payload.get("commits", [])
        head_commit = payload.get("head_commit", {})

        if not repo_full_name or not repo_owner or not repo_name:
            logger.error(f"Webhook payload missing repository details: {repo_info}")
            raise HTTPException(
                status_code=400, detail="Missing repository information in payload"
            )

        # Check if the repository is monitored
        if repo_full_name not in MONITORED_REPOS:
            logger.debug(
                f"Ignoring push event for non-monitored repository: {repo_full_name}"
            )
            return {"message": f"Repository {repo_full_name} not monitored"}

        if not commits and not head_commit:
            logger.warning(
                f"'push' event for {repo_full_name} received without 'commits' or 'head_commit' data. Possibly a branch deletion or tag push? Payload head: {payload.get('ref', '')}"
            )
            return {"message": "No commit data found in push event"}

        # Determine the commit(s) to process and the SHA to potentially update state with
        commits_to_process = []
        sha_to_update_state = None

        head_commit_id = head_commit.get("id")
        if head_commit_id:
            commits_to_process = [head_commit]  # Use head_commit as the primary source
            sha_to_update_state = (
                head_commit_id  # This is the SHA representing the push tip
            )
            logger.debug(f"Processing head_commit: {head_commit_id}")
        elif commits:
            commits_to_process = commits  # Fallback to commits list
            # If using commits list, the last commit's SHA is the best candidate for state update
            if commits:
                sha_to_update_state = commits[-1].get("id")
            logger.debug(
                f"Processing commits list (count: {len(commits)}). Potential state update SHA: {sha_to_update_state}"
            )
        else:
            # This case should ideally not be reached due to the check above, but included for completeness
            logger.warning(
                f"No processable commit data found in push event for {repo_full_name}. Skipping."
            )
            return {"message": "No processable commit data"}

        processed_new_commit = False
        for commit in commits_to_process:
            commit_sha = commit.get("id")
            if not commit_sha:
                logger.warning("Skipping commit in payload with missing 'id'.")
                continue

            # Avoid reprocessing the last known SHA for this repo
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if last_known_sha_for_repo and commit_sha == last_known_sha_for_repo:
                logger.debug(
                    f"Skipping commit {commit_sha} for {repo_full_name} as it matches last known SHA {last_known_sha_for_repo}."
                )
                continue

            # Inner try-except for processing individual commits within the push
            try:
                # Construct RID
                rid = GithubCommit(owner=repo_owner, repo=repo_name, sha=commit_sha)

                # Extract details - ensure keys exist
                author = commit.get("author", {})
                committer = commit.get("committer", {})

                contents = {
                    "sha": commit_sha,
                    "message": commit.get("message"),
                    "author_name": author.get("name"),
                    "author_email": author.get("email"),
                    "author_date": commit.get(
                        "timestamp"
                    ),  # GitHub often uses 'timestamp'
                    "committer_name": committer.get("name"),
                    "committer_email": committer.get("email"),
                    "committer_date": committer.get("timestamp"),
                    "html_url": commit.get("url"),  # Use 'url' from webhook payload
                    "parents": commit.get(
                        "parents", []
                    ),  # Typically a list of SHAs in webhook
                }

                bundle = Bundle.generate(rid=rid, contents=contents)
                # CORRECT USAGE: 'handle' makes the bundle available locally
                # in the sensor's cache/event queue for consumers to poll/fetch.
                # It does NOT push the application-specific GithubCommit bundle directly.
                logger.debug(
                    f"Making webhook commit bundle {rid} available locally via sensor API."
                )
                node.processor.handle(bundle=bundle)

                processed_new_commit = (
                    True  # Mark that we processed at least one new commit
                )

            except Exception as e:
                logger.error(
                    f"Error processing webhook commit {commit_sha} for {repo_full_name}: {e}",
                    exc_info=True,
                )
                # Decide whether to continue processing other commits in the push or stop
                continue  # Continue with next commit in the webhook push

        # Update state file only if we processed a new commit and have a valid SHA representing the push tip
        if processed_new_commit and sha_to_update_state:
            # Check again if the sha_to_update is different from the stored one before writing
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if sha_to_update_state != last_known_sha_for_repo:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. Updating state to SHA: {sha_to_update_state}"
                )
                update_state_file(repo_full_name, sha_to_update_state)
            else:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. State SHA {sha_to_update_state} already stored."
                )
        elif processed_new_commit:
            logger.warning(
                f"Webhook processing complete for {repo_full_name}. Processed new commit(s) but could not determine SHA for state update."
            )
        else:
            logger.info(
                f"Webhook processing complete for {repo_full_name}. No new commits processed or state updated."
            )

        return {"message": "Webhook processed successfully"}

    # Exception handlers are now correctly indented relative to the main 'try' block
    except HTTPException as he:
        # Re-raise HTTP exceptions to return proper status codes
        logger.warning(f"HTTP Exception during webhook processing: {he.detail}")
        raise he
    except Exception as e:
        logger.error(
            f"Unexpected error handling webhook event {x_github_event}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Internal server error handling webhook"
        )
</file>

<file path=".env.example">
# KOI-net GitHub Node Configuration

# Public URL where this GitHub node can be reached by other KOI-net nodes
URL="http://127.0.0.1:8001"

# URL of the coordinator node or another known KOI-net node
FIRST_CONTACT="http://127.0.0.1:8000/koi-net"

# Host the FastAPI server should listen on (0.0.0.0 allows external connections)
HOST="0.0.0.0"

# Port the FastAPI server should listen on
PORT="8001"

# GitHub Personal Access Token (replace with your own token)
GITHUB_TOKEN="your_github_token_here"

# Secret used to verify webhook signatures (generate a secure random string)
GITHUB_WEBHOOK_SECRET="your_webhook_secret_here"

# Comma-separated list of repositories to monitor (format: owner/repo)
MONITORED_REPOS="owner/repo1,owner/repo2"

# Path to the JSON file storing the last processed commit SHA
STATE_FILE_PATH="state.json"

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL="INFO"
</file>

<file path=".gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
</file>

<file path="Dockerfile">
FROM python:3.12-slim

# Install UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# Copy dependency files first for better caching
COPY pyproject.toml /app/

# Install dependencies using UV
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# Copy the application code
COPY . /app/

# Expose the correct KOI port
EXPOSE 8001

# Add correct HEALTHCHECK instruction
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8001/koi-net/health || exit 1

# Start service with correct entrypoint and port
CMD ["uvicorn", "github_sensor_node.server:app", "--host", "0.0.0.0", "--port", "8001"]
</file>

<file path="github-node.service">
[Unit]
Description=KOI-net Github Node Service
After=network.target

[Service]
WorkingDirectory=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/services/github
ExecStart=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/.venv/bin/python3 -m node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "github-sensor-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.3",
    "koi-net==1.0.0b12",
    "PyGithub",
    "aiohttp",
    "rich", # Added for logging
    "ruamel.yaml", # Added for YAML config loading
    "python-dotenv" # Added for loading .env in local runs
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

</files>
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path

# Import LOG_LEVEL from refactored config
from .config import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/hackmd")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "hackmd-sensor-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
logging.getLogger("uvicorn.error").setLevel(logging.WARNING)

logger.info(
    f"Logging configured (Level: {LOG_LEVEL}). Console via Rich, File: {log_file_path}"
)

# Logging level is now set globally based on config,
# specific logger level adjustments can happen in config.py
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/__main__.py">
import uvicorn
import asyncio
import logging
from .backfill import perform_backfill
from .config import HOST, PORT, POLL_INTERVAL, polling_state, save_polling_state
from .core import node
import threading
import time

logger = logging.getLogger(__name__)

stop_event = threading.Event()


def poll_hackmd():
    """Periodically polls HackMD using the backfill logic."""
    while not stop_event.is_set():
        logger.info("Polling HackMD...")
        try:
            perform_backfill(polling_state)
            save_polling_state()
        except Exception as e:
            logger.error(f"Error during HackMD polling loop: {e}", exc_info=True)

        # Wait for the configured interval or until stop event is set
        stop_event.wait(POLL_INTERVAL)


if __name__ == "__main__":
    logger.info(f"HackMD sensor node starting on {HOST}:{PORT}")
    logger.info(f"Polling HackMD every {POLL_INTERVAL} seconds.")

    # Start the KOI node processing thread
    node.start()

    # Start the polling thread
    polling_thread = threading.Thread(target=poll_hackmd, daemon=True)
    polling_thread.start()

    try:
        # Run the FastAPI server
        uvicorn.run(
            "hackmd_sensor_node.server:app",
            # Use HOST and PORT from config
            host=HOST,
            port=PORT,
            log_config=None,
            reload=True,
        )
    finally:
        # Signal threads to stop and wait for them
        logger.info("Shutting down HackMD sensor...")
        stop_event.set()
        polling_thread.join()
        node.stop()
        logger.info("HackMD sensor shut down gracefully.")
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/backfill.py">
import logging
from datetime import datetime, timedelta
from .hackmd_api import get_team_notes, get_note_details
from rid_types import HackMDNote
from rid_lib.ext import Bundle
from .core import node

# Import TEAM_PATH and the new TARGET_NOTE_IDS from config
from .config import TEAM_PATH, TARGET_NOTE_IDS

logger = logging.getLogger(__name__)


# Define the type for the state dictionary for clarity
StateType = dict[str, str]  # Maps note_id to last_modified_timestamp


def perform_backfill(state: StateType):
    """Fetches all notes for the configured team, compares with state, and bundles new/updated notes."""
    logger.info(f"Starting HackMD backfill for team path: '{TEAM_PATH}'")
    if not TEAM_PATH:
        logger.error("HackMD team path is not configured. Backfill skipped.")
        return

    try:
        processed_count = 0
        bundled_count = 0

        # Decide whether to process specific notes or all team notes
        if TARGET_NOTE_IDS:
            logger.info(
                f"Targeting specific HackMD notes for backfill: {TARGET_NOTE_IDS}"
            )
            # Process only specified notes
            for note_id in TARGET_NOTE_IDS:
                processed_count += 1
                logger.debug(f"Fetching targeted note ID: {note_id}")
                # Fetch the full note details, including content and metadata
                note_details = get_note_details(note_id)
                if not note_details:
                    logger.warning(
                        f"Could not fetch details for targeted note ID {note_id}. Skipping."
                    )
                    continue

                last_modified_str = note_details.get("lastChangedAt")
                title = note_details.get(
                    "title", f"Note {note_id}"
                )  # Use ID if title missing

                if not last_modified_str:
                    logger.warning(
                        f"Skipping targeted note {note_id} due to missing lastChangedAt."
                    )
                    continue

                # Check state for this specific note
                if note_id not in state or last_modified_str > state[note_id]:
                    logger.info(
                        f"Processing targeted note '{title}' (ID: {note_id}) - New or updated."
                    )
                    # Content is already part of note_details if get_note_details fetches everything
                    note_content = note_details.get("content")
                    if note_content is None:
                        logger.error(
                            f"Content missing for note ID {note_id} even after fetch. Skipping."
                        )
                        continue

                    # Bundle the note (logic similar to below, adapted for note_details)
                    try:
                        rid = HackMDNote(note_id=note_id)
                        contents = {
                            "id": note_id,
                            "title": title,
                            "content": note_content,
                            "createdAt": note_details.get("createdAt"),
                            "lastChangedAt": last_modified_str,
                            "publishLink": note_details.get("publishLink"),
                            "tags": note_details.get("tags", []),
                        }
                        bundle = Bundle.generate(rid=rid, contents=contents)
                        logger.debug(
                            f"Making backfill targeted note bundle {rid} available locally."
                        )
                        node.processor.handle(bundle=bundle)
                        bundled_count += 1
                        state[note_id] = last_modified_str  # Update state
                    except Exception as e:
                        logger.error(
                            f"Error bundling targeted note {note_id}: {e}",
                            exc_info=True,
                        )
                else:
                    logger.debug(
                        f"Skipping targeted note '{title}' (ID: {note_id}) - Already up-to-date."
                    )

        else:
            # Original logic: process all notes in the team
            logger.info(f"Processing all notes in team path: '{TEAM_PATH}'")
            team_notes = get_team_notes(TEAM_PATH)
            if not team_notes:
                logger.warning(
                    f"No notes found or error fetching notes for team '{TEAM_PATH}'. Backfill ending."
                )
                return

            for note_summary in team_notes:
                processed_count += 1
                note_id = note_summary.get("id")
                last_modified_str = note_summary.get("lastChangedAt")
                title = note_summary.get("title")

                if not note_id or not last_modified_str:
                    logger.warning(
                        f"Skipping note from team list due to missing ID or lastChangedAt: {note_summary}"
                    )
                    continue

                # Check if note needs processing based on state
                if note_id not in state or last_modified_str > state[note_id]:
                    logger.info(
                        f"Processing note '{title}' (ID: {note_id}) from team list - New or updated."
                    )
                    # Fetch full content only when needed
                    note_details = get_note_details(
                        note_id
                    )  # Use get_note_details to get full details
                    if note_details is None or note_details.get("content") is None:
                        logger.error(
                            f"Failed to fetch content/details for note ID {note_id} from team list. Skipping."
                        )
                        continue
                    note_content = note_details.get("content")

                    try:
                        rid = HackMDNote(note_id=note_id)
                        contents = {
                            "id": note_id,
                            "title": title,
                            "content": note_content,
                            "createdAt": note_summary.get(
                                "createdAt"
                            ),  # Use summary data where possible
                            "lastChangedAt": last_modified_str,
                            "publishLink": note_summary.get("publishLink"),
                            "tags": note_summary.get("tags", []),
                        }
                        bundle = Bundle.generate(rid=rid, contents=contents)
                        logger.debug(
                            f"Making backfill note bundle {rid} from team list available locally."
                        )
                        node.processor.handle(bundle=bundle)
                        bundled_count += 1
                        state[note_id] = last_modified_str  # Update state

                    except Exception as e:
                        logger.error(
                            f"Error creating/handling bundle for note {note_id} from team list: {e}",
                            exc_info=True,
                        )
                else:
                    logger.debug(
                        f"Skipping note '{title}' (ID: {note_id}) from team list - Already up-to-date."
                    )

        logger.info(
            f"HackMD backfill complete. Processed {processed_count} notes, bundled {bundled_count} new/updated notes."
        )

    except Exception as e:
        logger.error(f"Unexpected error during HackMD backfill: {e}", exc_info=True)


# Note: The state persistence (load/save) should be handled by the caller
# (e.g., the polling mechanism in server.py or __main__.py)

if __name__ == "__main__":
    # Example usage (requires node setup and async context)
    # import asyncio
    # node.start()
    # asyncio.run(perform_backfill({}))
    # node.stop()
    pass  # Keep placeholder for potential direct execution
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/hackmd_api.py">
import asyncio
import httpx
import logging
import requests
from .config import HACKMD_API_TOKEN

logger = logging.getLogger(__name__)

api_base_url = "https://api.hackmd.io/v1"


def request(path, method="GET"):
    resp = httpx.request(
        method=method,
        url=api_base_url + path,
        headers={"Authorization": "Bearer " + HACKMD_API_TOKEN},
    )

    if resp.status_code == 200:
        return resp.json()

    else:
        print(resp.status_code, resp.text)
        return


async def async_request(path, method="GET"):
    timeout = 60

    while True:
        async with httpx.AsyncClient() as client:

            resp = await client.request(
                method=method,
                url=api_base_url + path,
                headers={"Authorization": "Bearer " + HACKMD_API_TOKEN},
            )

        if resp.status_code == 200:
            return resp.json()

        elif resp.status_code == 429:
            print(resp.status_code, resp.text, f"retrying in {timeout} seconds")
            await asyncio.sleep(timeout)
            timeout *= 2
        else:
            print(resp.status_code, resp.text)
            return


def get_team_notes(team_path: str):
    """Fetch notes for a given HackMD team path."""
    # Check if token is available
    if not HACKMD_API_TOKEN:
        logger.error("HackMD API token is not configured. Cannot fetch team notes.")
        return []

    headers = {
        "Authorization": f"Bearer {HACKMD_API_TOKEN}",
    }
    try:
        # Use f-string for URL formatting
        response = requests.get(
            f"{api_base_url}/teams/{team_path}/notes", headers=headers
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        notes = response.json()
        logger.info(f"Successfully fetched {len(notes)} notes for team '{team_path}'")
        return notes
    except requests.exceptions.RequestException as e:
        logger.error(
            f"Error fetching HackMD notes for team '{team_path}': {e}", exc_info=True
        )
        return []
    except Exception as e:
        logger.error(f"Unexpected error fetching HackMD notes: {e}", exc_info=True)
        return []


def get_note_details(note_id: str):
    """Fetch the full details (including content) of a specific HackMD note by ID."""
    # Check if token is available
    if not HACKMD_API_TOKEN:
        logger.error("HackMD API token is not configured. Cannot fetch note details.")
        return None

    headers = {
        "Authorization": f"Bearer {HACKMD_API_TOKEN}",
    }
    try:
        # Use f-string for URL formatting
        response = requests.get(f"{api_base_url}/notes/{note_id}", headers=headers)
        response.raise_for_status()
        note_details = response.json()
        logger.debug(f"Successfully fetched details for note ID '{note_id}'")
        return note_details  # Return the full dictionary
    except requests.exceptions.RequestException as e:
        logger.error(f"Error fetching HackMD note details for ID '{note_id}': {e}")
        return None
    except Exception as e:
        logger.error(
            f"Unexpected error fetching HackMD note details: {e}", exc_info=True
        )
        return None
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/handlers.py">
import logging
from multiprocessing import process
from koi_net.processor.handler import HandlerType, STOP_CHAIN
from koi_net.processor.knowledge_object import KnowledgeSource, KnowledgeObject
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.ext import Bundle
from rid_lib.types import KoiNetNode

from rid_types import HackMDNote
from .core import node
from . import hackmd_api

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    # when I found out about a new node
    if kobj.normalized_event_type != EventType.NEW:
        return

    node_profile = kobj.bundle.validate_contents(NodeProfile)

    # looking for event provider of nodes
    if KoiNetNode not in node_profile.provides.event:
        return

    logger.debug("Identified a coordinator!")
    logger.debug("Proposing new edge")

    # queued for processing
    processor.handle(
        bundle=generate_edge_bundle(
            source=kobj.rid,
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode],
        )
    )

    logger.debug("Catching up on network state")

    rid_payload = processor.network.request_handler.fetch_rids(
        kobj.rid, rid_types=[KoiNetNode]
    )

    rids = [
        rid
        for rid in rid_payload.rids
        if rid != processor.identity.rid and not processor.cache.exists(rid)
    ]

    bundle_payload = processor.network.request_handler.fetch_bundles(
        kobj.rid, rids=rids
    )

    for bundle in bundle_payload.bundles:
        # marked as external since we are handling RIDs from another node
        # will fetch remotely instead of checking local cache
        processor.handle(bundle=bundle, source=KnowledgeSource.External)
    logger.debug("Done")


@node.processor.register_handler(HandlerType.Manifest)
def custom_manifest_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    if type(kobj.rid) == HackMDNote:
        logger.debug("Skipping HackMD note manifest handling")
        return

    prev_bundle = processor.cache.read(kobj.rid)

    if prev_bundle:
        if kobj.manifest.sha256_hash == prev_bundle.manifest.sha256_hash:
            logger.debug(
                "Hash of incoming manifest is same as existing knowledge, ignoring"
            )
            return STOP_CHAIN
        if kobj.manifest.timestamp <= prev_bundle.manifest.timestamp:
            logger.debug(
                "Timestamp of incoming manifest is the same or older than existing knowledge, ignoring"
            )
            return STOP_CHAIN

        logger.debug("RID previously known to me, labeling as 'UPDATE'")
        kobj.normalized_event_type = EventType.UPDATE

    else:
        logger.debug("RID previously unknown to me, labeling as 'NEW'")
        kobj.normalized_event_type = EventType.NEW

    return kobj


@node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
def custom_hackmd_bundle_handler(processor: ProcessorInterface, kobj: KnowledgeObject):
    assert type(kobj.rid) == HackMDNote

    prev_bundle = processor.cache.read(kobj.rid)

    if prev_bundle:
        prevChangedAt = prev_bundle.contents["lastChangedAt"]
        currChangedAt = kobj.contents["lastChangedAt"]
        logger.debug(f"Changed at {prevChangedAt} -> {currChangedAt}")
        if currChangedAt > prevChangedAt:
            logger.debug("Incoming note has been changed more recently!")
            kobj.normalized_event_type = EventType.UPDATE

        else:
            logger.debug("Incoming note is not newer")
            return STOP_CHAIN

    else:
        logger.debug("Incoming note is previously unknown to me")
        kobj.normalized_event_type = EventType.NEW

    logger.debug("Retrieving full note...")
    data = hackmd_api.request(f"/notes/{kobj.rid.note_id}")

    if not data:
        logger.debug("Failed.")
        return STOP_CHAIN

    logger.debug("Done.")

    full_note_bundle = Bundle.generate(rid=kobj.rid, contents=data)

    kobj.manifest = full_note_bundle.manifest
    kobj.contents = full_note_bundle.contents

    return kobj
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/server.py">
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node
from .backfill import perform_backfill


logger = logging.getLogger(__name__)


async def backfill_loop():
    while True:
        await asyncio.sleep(600)


@asynccontextmanager
async def lifespan(app: FastAPI):
    node.start()

    yield
    node.stop()


app = FastAPI(lifespan=lifespan, title="KOI-net Protocol API", version="1.0.0")


koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
def broadcast_events(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        logger.info(f"{event!r}")
        node.processor.handle(event=event, source=KnowledgeSource.External)


@koi_net_router.post(POLL_EVENTS_PATH)
def poll_events(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@koi_net_router.post(FETCH_RIDS_PATH)
def fetch_rids(req: FetchRids) -> RidsPayload:
    return node.network.response_handler.fetch_rids(req)


@koi_net_router.post(FETCH_MANIFESTS_PATH)
def fetch_manifests(req: FetchManifests) -> ManifestsPayload:
    return node.network.response_handler.fetch_manifests(req)


@koi_net_router.post(FETCH_BUNDLES_PATH)
def fetch_bundles(req: FetchBundles) -> BundlesPayload:
    return node.network.response_handler.fetch_bundles(req)


# Add health endpoint
@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed
    return {"status": "healthy"}


app.include_router(koi_net_router)
</file>

<file path="nodes/koi-net-hackmd-sensor-node/.gitignore">
rid_cache
identity.json
events_queue.json
venv
.env
*.json
__pycache__
</file>

<file path="nodes/koi-net-hackmd-sensor-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy pyproject and perform editable installation
COPY pyproject.toml /app/

# 3) install curl for healthcheck
RUN apt-get update && apt-get install -y curl

# 4) install dependencies using UV and pyproject.toml
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 5) copy in code
COPY . /app/

# 6) expose port and set environment
EXPOSE 8002

# 7) add healthcheck using the main app's endpoint
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8002/koi-net/health || exit 1

# 8) start the main service using shell form for ENV var substitution
CMD ["uvicorn", "hackmd_sensor_node.server:app", "--host", "0.0.0.0", "--port", "8002"]
</file>

<file path="nodes/koi-net-hackmd-sensor-node/koi-net-hackmd-sensor-node.service">
[Unit]
Description=KOI-net HackMD Sensor Node Service
After=network.target

[Service]
WorkingDirectory=/home/dev/koi-net-hackmd-sensor-node
ExecStart=/home/dev/koi-net-hackmd-sensor-node/venv/bin/python3 -m hackmd_sensor_node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="nodes/koi-net-hackmd-sensor-node/LICENSE">
MIT License

Copyright (c) 2025 BlockScience

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="nodes/koi-net-hackmd-sensor-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "koi-net-hackmd-sensor-node"
version = "0.1.0"
dependencies = [
    "koi-net==1.0.0b12",
    "rid-lib>=3.2.3",
    "rich",
    "fastapi",
    "uvicorn",
    "python-dotenv",
    "requests",
    "ruamel.yaml",
    "python-dotenv"
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="nodes/koi-net-hackmd-sensor-node/README.md">
# koi-net-hackmd-sensor-node
 HackMD sensor node implementation for BlockScience's KOI-net
</file>

<file path="nodes/koi-net-hackmd-sensor-node/requirements.txt">
koi-net==1.0.0b12
rid-lib>=3.2.3
rich
fastapi
uvicorn
python-dotenv
requests
ruamel.yaml
</file>

<file path="nodes/koi-net-hackmd-sensor-node/rid_types.py">
from rid_lib.core import ORN


class HackMDNote(ORN):
    namespace = "hackmd.note"

    def __init__(self, note_id: str):
        self.note_id = note_id

    @property
    def reference(self):
        return self.note_id

    @classmethod
    def from_reference(cls, reference):
        return cls(reference)
</file>

<file path="config/docker/coordinator.yaml">
runtime:
  base_url: http://coordinator:8080/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8080
</file>

<file path="config/local/coordinator.yaml">
runtime:
  base_url: http://127.0.0.1:8080/koi-net
  cache_dir: .koi/shared_cache
  host: 127.0.0.1
  log_level: INFO
  port: 8080
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/config_loader.py">
import logging
from ruamel.yaml import YAML
from pathlib import Path
import os

# Configure basic logging early
logging.basicConfig(level=logging.INFO, format="%(name)s - %(message)s")
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker based on KOI_CONFIG_MODE and RUN_CONTEXT ---
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
else:
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

print(f"CONFIG_DIR: {CONFIG_DIR}")
print(f"CONFIG_MODE: {CONFIG_MODE}")

CONFIG_PATH = CONFIG_DIR / "coordinator.yaml"
ENV_PATH = CONFIG_DIR / "global.env"

# Load env vars from the selected global.env
if ENV_PATH.is_file():
    with open(ENV_PATH) as f:
        for line in f:
            if line.strip() and not line.startswith('#') and '=' in line:
                k, v = line.strip().split('=', 1)
                os.environ.setdefault(k, v)

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None: # Handle empty file case
            CONFIG = {}
    except Exception as e:
        logging.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logging.error(f"Config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Default Values --- #
DEFAULT_RUNTIME = {
    "port": 8080,
    "host": "0.0.0.0",
    "log_level": "INFO",
    "base_url": None,
    "cache_dir": None,
}

# --- Determine Run Context & Set Defaults ---
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"
RUNTIME_CONFIG = {**DEFAULT_RUNTIME, **CONFIG.get("runtime", {})}

# Adjust paths based on context
LOCAL_DATA_BASE = Path(".koi/shared_cache") # Define a local base if needed
DOCKER_CACHE_DIR_DEFAULT = "/data/cache" # Default for Docker

# Determine Cache Dir
if is_docker:
    CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", DOCKER_CACHE_DIR_DEFAULT)
else:
    # For local, use the config value or construct relative path
    cache_dir_config = RUNTIME_CONFIG.get("cache_dir")
    if cache_dir_config:
        CACHE_DIR = str(Path(cache_dir_config)) # Respect config if set
    else:
        LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
        CACHE_DIR = str(LOCAL_DATA_BASE) # Fallback local path

# Ensure the resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# Export specific values for easier access
PORT = RUNTIME_CONFIG.get("port")
HOST = RUNTIME_CONFIG.get("host")
LOG_LEVEL = RUNTIME_CONFIG.get("log_level", "INFO").upper()
BASE_URL = RUNTIME_CONFIG.get("base_url")

logger.info(
    f"Coordinator configured with MODE={CONFIG_MODE}, PORT={PORT}, BASE_URL={BASE_URL}, LOG_LEVEL={LOG_LEVEL}, CACHE_DIR={CACHE_DIR}"
)

# Check for required config
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not CACHE_DIR:
    logger.critical(
        "Configuration error: runtime.cache_dir is not set or resolved correctly."
    )
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/core.py">
import shutil
import os
import logging  # Import logging
from rid_lib.types import KoiNetNode, KoiNetEdge
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import from the new config loader
from .config_loader import BASE_URL, PORT, CACHE_DIR  # Import BASE_URL, PORT, and CACHE_DIR

logger = logging.getLogger(__name__)  # Get logger instance

name = "coordinator"

# Keep identity/cache dir logic as it's node-specific state, not static config
identity_dir = f".koi/{name}"
# cache_dir = f".koi/{name}/rid_cache_{name}" # REMOVED - Using shared CACHE_DIR now

# Remove existing directories if they exist - REMOVED THESE LINES
# logger.info(
#     f"Attempting to clear existing state directories: {identity_dir}, {cache_dir}"
# )
# shutil.rmtree(identity_dir, ignore_errors=True)
# shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the directories - Ensure only identity_dir is created if needed
os.makedirs(identity_dir, exist_ok=True)
# os.makedirs(cache_dir, exist_ok=True) # REMOVED - Shared cache managed elsewhere
logger.info(f"Ensured identity directory exists: {identity_dir}")


node = NodeInterface(
    name="coordinator",
    profile=NodeProfile(
        # Use the base_url loaded from the YAML config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[KoiNetNode, KoiNetEdge], state=[KoiNetNode, KoiNetEdge]
        ),
    ),
    use_kobj_processor_thread=True,
    # Use resolved absolute paths for state files/dirs
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Corrected to use shared CACHE_DIR from config
    cache_directory_path=CACHE_DIR,
)

# Import handlers after node is initialized
from . import handlers
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/handlers.py">
import logging
import time
from rid_lib.types import KoiNetNode, KoiNetEdge
from koi_net.processor import ProcessorInterface
from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject
from koi_net.protocol.event import Event, EventType
from koi_net.protocol.helpers import generate_edge_bundle
from koi_net.protocol.edge import EdgeType
from .core import node

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handshake_handler(proc: ProcessorInterface, kobj: KnowledgeObject):
    logger.info("Handling node handshake")

    # only respond if node declares itself as NEW
    if kobj.event_type != EventType.NEW:
        return

    logger.info("Sharing this node's bundle with peer")
    proc.network.push_event_to(
        event=Event.from_bundle(EventType.NEW, proc.identity.bundle),
        node=kobj.rid,
        flush=False,
    )

    # Introduce a small delay to allow sensor network interface to potentially stabilize
    time.sleep(1)

    logger.info("Proposing new edge")
    # defer handling of proposed edge

    edge_bundle = generate_edge_bundle(
        source=kobj.rid,
        target=proc.identity.rid,
        edge_type=EdgeType.WEBHOOK,
        rid_types=[KoiNetNode, KoiNetEdge],
    )

    proc.handle(bundle=edge_bundle)
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/config.py">
import logging
import json
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import List, Dict, Any

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker based on KOI_CONFIG_MODE and RUN_CONTEXT ---
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
else:
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

CONFIG_PATH = CONFIG_DIR / "github-sensor.yaml"
ENV_PATH = CONFIG_DIR / "global.env"

# Load env vars from the selected global.env
if ENV_PATH.is_file():
    with open(ENV_PATH) as f:
        for line in f:
            if line.strip() and not line.startswith('#') and '=' in line:
                k, v = line.strip().split('=', 1)
                os.environ.setdefault(k, v) # Use setdefault to avoid overriding existing env vars

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None: # Handle empty file case
            CONFIG = {}
    except Exception as e:
        logging.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logging.error(f"Config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Determine Run Context ---
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"

# --- Extract specific configurations with defaults ---
SENSOR_CONFIG: Dict[str, Any] = CONFIG.get("sensor", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
WEBHOOK_CONFIG: Dict[str, Any] = CONFIG.get("webhook", {})

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/github")  # Relative to workspace root for local runs

# Base configuration values (URLs are now directly from the correct mode's YAML)
MONITORED_REPOS: List[str] = SENSOR_CONFIG.get("repos", [])
POLL_INTERVAL: int = SENSOR_CONFIG.get("poll_interval", 60)
SENSOR_MODE: str = SENSOR_CONFIG.get("mode", "webhook")
HOST: str = RUNTIME_CONFIG.get("host", "0.0.0.0" if not is_docker else "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8001)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()
COORDINATOR_URL = EDGES_CONFIG.get("coordinator_url")
BASE_URL = RUNTIME_CONFIG.get("base_url")

# Adjust Cache and State Paths based on run context
DOCKER_CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", "/data/cache") # Default Docker path
DOCKER_STATE_FILE = RUNTIME_CONFIG.get(
    "state_file", os.path.join(DOCKER_CACHE_DIR, "github_state.json")
)

if is_docker:
    CACHE_DIR = DOCKER_CACHE_DIR
    STATE_FILE_PATH = (
        Path(DOCKER_STATE_FILE)
        if Path(DOCKER_STATE_FILE).is_absolute()
        else Path("/app") / DOCKER_STATE_FILE # Assume /app if relative in Docker
    )
else:
    LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
    CACHE_DIR = str(LOCAL_DATA_BASE / "cache")
    STATE_FILE_PATH = LOCAL_DATA_BASE / "github_state.json"
    logger.debug(f"Using local CACHE_DIR: {CACHE_DIR}")
    logger.debug(f"Using local STATE_FILE_PATH: {STATE_FILE_PATH}")

# Ensure resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Load Secrets from Environment Variables ---
GITHUB_TOKEN: str | None = os.getenv("GITHUB_TOKEN")
WEBHOOK_SECRET_ENV_VAR: str | None = WEBHOOK_CONFIG.get("secret_env_var")
GITHUB_WEBHOOK_SECRET: str | None = None
if WEBHOOK_SECRET_ENV_VAR:
    GITHUB_WEBHOOK_SECRET = os.getenv(WEBHOOK_SECRET_ENV_VAR)
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning(
            f"Environment variable '{WEBHOOK_SECRET_ENV_VAR}' specified in config but not found in environment."
        )
else:
    logger.warning("webhook.secret_env_var not specified in github-sensor.yaml")

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL)
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logging.getLogger("github").setLevel(logging.WARNING)
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("GitHub Sensor Configuration Loaded:")
logger.info(f"  Config Mode: {CONFIG_MODE}")
logger.info(f"  Is Docker Context: {is_docker}")
logger.info(f"  Sensor Mode: {SENSOR_MODE}")
logger.info(f"  Monitored Repos: {MONITORED_REPOS}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  State File Path: {STATE_FILE_PATH}")
logger.info(f"  GitHub Token Loaded: {bool(GITHUB_TOKEN)}")
logger.info(f"  Webhook Secret Loaded: {bool(GITHUB_WEBHOOK_SECRET)}")

# Check required config
if not COORDINATOR_URL:
    logger.critical(
        "Configuration error: edges.coordinator_url is not set or resolved correctly."
    )
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not GITHUB_WEBHOOK_SECRET:
    logger.warning("Configuration warning: GitHub Webhook Secret not loaded.")

# --- State Management (Loading initial state & update function) ---
LAST_PROCESSED_SHA: Dict[str, str] = {}  # Dictionary mapping repo_name -> last_sha

def load_state():
    """Loads the last processed SHA state from the JSON file specified by STATE_FILE_PATH."""
    global LAST_PROCESSED_SHA
    state_path = STATE_FILE_PATH
    try:
        with open(state_path, "r") as f:
            LAST_PROCESSED_SHA = json.load(f)
        logger.info(
            f"Loaded state from '{state_path}': Repos {list(LAST_PROCESSED_SHA.keys())}"
        )
    except FileNotFoundError:
        logger.warning(
            f"State file '{state_path}' not found. This is expected on first run. Starting with empty state (will perform full backfill)."
        )
        LAST_PROCESSED_SHA = {}
    except json.JSONDecodeError:
        logger.error(
            f"Error decoding JSON from state file '{state_path}'. Starting with empty state."
        )
        LAST_PROCESSED_SHA = {}
    except Exception as e:
        logger.error(
            f"Unexpected error loading state file '{state_path}': {e}",
            exc_info=True,
        )
        LAST_PROCESSED_SHA = {}

def update_state_file(repo_name: str, last_sha: str):
    """Updates the state file with the latest processed SHA for a repo."""
    global LAST_PROCESSED_SHA
    LAST_PROCESSED_SHA[repo_name] = last_sha

    state_path = STATE_FILE_PATH
    try:
        state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(state_path, "w") as f:
            json.dump(LAST_PROCESSED_SHA, f, indent=4)
        logger.debug(
            f"Updated state file '{state_path}' for {repo_name} with SHA: {last_sha}"
        )
    except IOError as e:
        logger.error(f"Failed to write state file '{state_path}': {e}")
    except Exception as e:
        logger.error(
            f"Unexpected error writing state file '{state_path}': {e}",
            exc_info=True,
        )

# Load initial state when config module is imported
load_state()
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/core.py">
import os
import shutil
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the refactored config module
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
    LOG_LEVEL,
)  # Import necessary vars
from .types import GithubCommit

logger = logging.getLogger(__name__)

name = "github"

# Identity and cache directory paths remain relative for setup,
# but NodeInterface gets absolute paths derived from config
identity_dir = f".koi/{name}"
# Cache dir is now defined in config, use that path for NodeInterface
# cache_dir_setup = f".koi/{name}/rid_cache_{name}"

# Clear existing state directories logic remains the same
# Consider making this behavior optional or configurable
# logger.info(f"Attempting to clear existing identity directory: {identity_dir}")
# shutil.rmtree(identity_dir, ignore_errors=True)
# Cache dir managed by Docker volume, no need to clear here
# shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the identity directory
os.makedirs(identity_dir, exist_ok=True)
# Ensure cache directory exists within the container is handled by Docker volume mount implicitly
# os.makedirs(cache_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Ensure required config values are present
if not BASE_URL:
    raise ValueError("Runtime base_url is not configured in github-sensor.yaml")
if not COORDINATOR_URL:
    raise ValueError("Edges coordinator_url is not configured in github-sensor.yaml")

# Initialize the KOI-net Node Interface for the GitHub Sensor
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        # Use BASE_URL from config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[GithubCommit],
            state=[GithubCommit],
        ),
    ),
    use_kobj_processor_thread=True,
    # Use COORDINATOR_URL from config for first_contact
    first_contact=COORDINATOR_URL,
    # State file paths are now relative to the application root in the container
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Use CACHE_DIR from config (should be /data/cache)
    cache_directory_path=CACHE_DIR,
)

logger.info(f"Initialized NodeInterface: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/config.py">
import logging
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import Dict, Any
import json

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker based on KOI_CONFIG_MODE and RUN_CONTEXT ---
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
else:
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

CONFIG_PATH = CONFIG_DIR / "hackmd-sensor.yaml"
ENV_PATH = CONFIG_DIR / "global.env"

# Load env vars from the selected global.env
if ENV_PATH.is_file():
    with open(ENV_PATH) as f:
        for line in f:
            if line.strip() and not line.startswith('#') and '=' in line:
                k, v = line.strip().split('=', 1)
                os.environ.setdefault(k, v)

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None: # Handle empty file case
            CONFIG = {}
    except Exception as e:
        logging.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logging.error(f"Config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Determine Run Context ---
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"

# --- Extract specific configurations with defaults ---
SENSOR_CONFIG: Dict[str, Any] = CONFIG.get("sensor", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
API_CONFIG: Dict[str, Any] = CONFIG.get("api", {})

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/hackmd")

# Base config values (URLs are now directly from the correct mode's YAML)
TEAM_PATH: str = SENSOR_CONFIG.get("team_path", "blockscience")
POLL_INTERVAL: int = SENSOR_CONFIG.get("poll_interval", 300)
HOST: str = RUNTIME_CONFIG.get("host", "127.0.0.1" if not is_docker else "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8002)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()
TARGET_NOTE_IDS: list[str] = SENSOR_CONFIG.get("target_note_ids", [])
COORDINATOR_URL = EDGES_CONFIG.get("coordinator_url")
BASE_URL = RUNTIME_CONFIG.get("base_url")

# Adjust Cache and State Paths based on run context
DOCKER_CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", "/data/cache")
DOCKER_STATE_FILE = RUNTIME_CONFIG.get(
    "state_file", os.path.join(DOCKER_CACHE_DIR, "hackmd_state.json")
)

if is_docker:
    CACHE_DIR = DOCKER_CACHE_DIR
    STATE_FILE_PATH = (
        Path(DOCKER_STATE_FILE)
        if Path(DOCKER_STATE_FILE).is_absolute()
        else Path("/app") / DOCKER_STATE_FILE
    )
else:
    LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
    CACHE_DIR = str(LOCAL_DATA_BASE / "cache")
    STATE_FILE_PATH = LOCAL_DATA_BASE / "hackmd_state.json"
    logger.debug(f"Using local CACHE_DIR: {CACHE_DIR}")
    logger.debug(f"Using local STATE_FILE_PATH: {STATE_FILE_PATH}")

# Ensure resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Load Secrets from Environment Variables ---
TOKEN_ENV_VAR: str | None = API_CONFIG.get("token_env_var")
HACKMD_API_TOKEN: str | None = None
if TOKEN_ENV_VAR:
    HACKMD_API_TOKEN = os.getenv(TOKEN_ENV_VAR)
    if not HACKMD_API_TOKEN:
        logger.warning(
            f"Environment variable '{TOKEN_ENV_VAR}' specified in config but not found in environment."
        )
else:
    logger.warning("api.token_env_var not specified in hackmd-sensor.yaml")

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL)
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("HackMD Sensor Configuration Loaded:")
logger.info(f"  Config Mode: {CONFIG_MODE}")
logger.info(f"  Is Docker Context: {is_docker}")
logger.info(f"  Team Path: {TEAM_PATH}")
logger.info(f"  Poll Interval: {POLL_INTERVAL}s")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  State File Path: {STATE_FILE_PATH}")
logger.info(f"  HackMD Token Loaded: {bool(HACKMD_API_TOKEN)}")
logger.info(
    f"  Target Note IDs: {TARGET_NOTE_IDS if TARGET_NOTE_IDS else 'All notes in team'}"
)

# Check for required configuration
if not COORDINATOR_URL:
    logger.critical(
        "Configuration error: edges.coordinator_url is not set or resolved correctly."
    )
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not HACKMD_API_TOKEN:
    logger.warning(
        f"Configuration warning: HackMD API token not loaded (check '{TOKEN_ENV_VAR}' in global.env and config)"
    )

# --- State Management (Moved from __main__) ---
polling_state: Dict[str, str] = {}  # Maps note_id to last_modified_timestamp

def load_polling_state():
    """Loads the polling state from the JSON file specified by STATE_FILE_PATH."""
    global polling_state
    state_path = STATE_FILE_PATH
    try:
        with open(state_path, "r") as f:
            polling_state = json.load(f)
        logger.info(f"Loaded polling state from {state_path}")
    except FileNotFoundError:
        logger.warning(
            f"Polling state file not found at {state_path}. This is expected on first run. Starting fresh."
        )
        polling_state = {}
    except json.JSONDecodeError:
        logger.error(
            f"Error decoding JSON from state file {state_path}. Starting fresh."
        )
        polling_state = {}
    except Exception as e:
        logger.error(
            f"Unexpected error loading polling state file {state_path}: {e}",
            exc_info=True,
        )
        polling_state = {}

def save_polling_state():
    """Saves the current polling state to the JSON file specified by STATE_FILE_PATH."""
    global polling_state
    state_path = STATE_FILE_PATH
    try:
        state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(state_path, "w") as f:
            json.dump(polling_state, f, indent=4)
        logger.debug(f"Saved polling state to {state_path}")
    except IOError as e:
        logger.error(f"Failed to write polling state file {state_path}: {e}")
    except Exception as e:
        logger.error(
            f"Unexpected error writing polling state file {state_path}: {e}",
            exc_info=True,
        )

# Load initial state when config module is imported
load_polling_state()
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/core.py">
import logging
import shutil
import os
from rid_types import HackMDNote
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides
from koi_net.processor.default_handlers import (
    basic_rid_handler,
    edge_negotiation_handler,
    basic_network_output_filter,
)

# Import from refactored config
from .config import BASE_URL, COORDINATOR_URL, CACHE_DIR

logger = logging.getLogger(__name__)

name = "hackmd"

# Keep identity dir logic, cache dir comes from config
identity_dir = f".koi/{name}"
# cache_dir = f".koi/{name}/rid_cache_{name}" # Removed

# Clear existing identity directory - REMOVED
# logger.info(f"Attempting to clear existing identity directory: {identity_dir}")
# shutil.rmtree(identity_dir, ignore_errors=True)

# Recreate the identity directory - Ensure it exists
os.makedirs(identity_dir, exist_ok=True)
# os.makedirs(cache_dir, exist_ok=True) # Cache dir managed by Docker volume
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Ensure required config values are present (already logged in config.py)
if not BASE_URL or not COORDINATOR_URL:
    # Optional: Raise error here if critical config is missing
    logger.critical(
        "Required configuration (BASE_URL or COORDINATOR_URL) missing. Node might not function correctly."
    )

node = NodeInterface(
    name="hackmd-sensor",
    profile=NodeProfile(
        # Use BASE_URL from config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(event=[HackMDNote], state=[HackMDNote]),
    ),
    use_kobj_processor_thread=True,
    # Use COORDINATOR_URL from config for first_contact
    first_contact=COORDINATOR_URL,
    handlers=[basic_rid_handler, edge_negotiation_handler, basic_network_output_filter],
    # Use absolute paths for state files/dirs
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Use CACHE_DIR from config
    cache_directory_path=CACHE_DIR,
)

# Import handlers after node is initialized
from . import handlers
</file>

<file path="docker-compose.yaml">
services:
  coordinator:
    build:
      context: ./nodes/koi-net-coordinator-node
    ports:
      - "8080:8080"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
    volumes:
      - ./config:/config:ro
      - cache_data:/data/cache
      - coordinator_state_data:/app/.koi/coordinator
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  github-sensor:
    build:
      context: ./nodes/koi-net-github-sensor-node
    ports:
      - "8001:8001"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
    volumes:
      - ./config:/config:ro
      - cache_data:/data/cache
      - github_state_data:/app/.koi/github
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8001/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  hackmd-sensor:
    build:
      context: ./nodes/koi-net-hackmd-sensor-node
    ports:
      - "8002:8002"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
    volumes:
      - ./config:/config:ro
      - cache_data:/data/cache
      - hackmd_state_data:/app/.koi/hackmd
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8002/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

volumes:
  cache_data:
    driver: local
  coordinator_state_data:
    driver: local
  github_state_data:
    driver: local
  hackmd_state_data:
    driver: local

networks:
  koinet:
    driver: bridge
</file>

</files>
</file>

<file path="plan.md">
# Plan: Scaffold and Implement Processor Nodes

This plan outlines the steps to create and implement Processor A (Repo Indexer) and Processor B (Note Indexer) within the existing `nodes/` directory structure, integrating them into the overall system defined by the PRD and `Processor.md`.

## Phase 0: Creating a Shared RID Types Package

**Goal:** Create a centralized package that re-exports existing RID type definitions to be used across all nodes.

1. **Task: Create Shared RID Types Directory Structure**

   - **Action:** Create the directory `rid_types/` at the project root.
   - **Verification:** The directory structure exists.

2. **Task: Create Basic RID Types Package Files**

   - **Action:** Inside `rid_types/`, create the following files:
     - `__init__.py` (exports all RID types)
     - `github.py` (re-exports GitHub-related RID types)
     - `hackmd.py` (re-exports HackMD-related RID types)
   - **Verification:** All specified files exist in the correct locations.

3. **Task: Implement GitHub RID Types Re-export**

   - **Action:** In `rid_types/github.py`, import and re-export the existing `GithubCommit` RID class:

     ```python
     # Re-export the existing GithubCommit class
     from nodes.koi_net_github_sensor_node.github_sensor_node.types import GithubCommit

     __all__ = ["GithubCommit"]
     ```

   - **Verification:** The `GithubCommit` class is successfully imported and re-exported.

4. **Task: Implement HackMD RID Types Re-export**

   - **Action:** In `rid_types/hackmd.py`, import and re-export the existing `HackMDNote` RID class:

     ```python
     # Re-export the existing HackMDNote class
     from nodes.koi_net_hackmd_sensor_node.rid_types import HackMDNote

     __all__ = ["HackMDNote"]
     ```

   - **Verification:** The `HackMDNote` class is successfully imported and re-exported.

5. **Task: Implement Package Exports**

   - **Action:** In `rid_types/__init__.py`, export all RID classes:

     ```python
     from .github import GithubCommit
     from .hackmd import HackMDNote

     __all__ = ["GithubCommit", "HackMDNote"]
     ```

   - **Verification:** The package properly exports all RID classes.
</file>

<file path="pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "koi-demo"
version = "0.1.0"
description = "KOI-net Self-Forming Knowledge Mesh Demo"
authors = [
    {name = "KOI-net Team"}
]
readme = "README.md"
requires-python = ">=3.8"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.1",
    "koi-net==1.0.0b12",
    "ruamel.yaml"
]

[project.optional-dependencies]
dev = [
    "pytest",
]

[tool.setuptools]
package-dir = {"" = "."}

# Add setuptools configuration to control package discovery
[tool.setuptools.packages.find]
where = ["."]  # Search in the root directory
include = ["nodes*"]  # Include only 'nodes' and its sub-packages
exclude = ["config*"] # Exclude the 'config' directory
namespaces = false # Assume 'nodes' itself is not a namespace package
</file>

<file path="README.md">
# KOI-net: Self-Forming Knowledge Mesh

![KOI-net Architecture](docs/images/koi-net-arch.png)

## Overview

KOI-net is a self-forming knowledge mesh that connects specialized nodes to create a distributed knowledge management system. The system collects, processes, indexes, and makes discoverable information from various sources through a coordinated network of dedicated nodes.

## System Architecture

KOI-net is organized around the following components:

### 1. Coordinator Node

The central hub for node discovery and connection management:

- Facilitates node discovery and edge formation
- Maintains the network topology
- Serves as the initial contact point for all nodes

### 2. Sensor Nodes

Specialized nodes that connect to external data sources:

- **GitHub Sensor** (`koi-net-github-sensor-node`):

  - Monitors GitHub repositories for new commits
  - Extracts relevant metadata and content
  - Publishes commit information to the network

- **HackMD Sensor** (`koi-net-hackmd-sensor-node`):
  - Tracks changes to HackMD notes
  - Captures note metadata and content
  - Makes notes discoverable in the knowledge mesh

### 3. Processor Nodes

Nodes that transform, index, and expose knowledge:

- **Processor A - GitHub Repository Indexer** (`koi-net-processor-a-node`):

  - Consumes GitHub commit events
  - Builds a searchable index of repository information
  - Provides a search API for querying commit data

- **Processor B - HackMD Note Indexer** (`koi-net-processor-b-node`):
  - Processes HackMD note events
  - Creates a multi-faceted index of notes
  - Exposes an API for searching and discovering notes

## KOI Protocol

The system leverages the KOI protocol, which provides:

- **Resource Identifiers (RIDs)**: Unique identifiers for all system resources
- **Manifest/Bundle Model**: Efficient metadata and content exchange
- **Edge System**: Self-forming connections between nodes
- **Event Propagation**: Real-time updates across the mesh

## Getting Started

### Prerequisites

- Docker and Docker Compose
- Python 3.12+
- Access to GitHub and/or HackMD instances

### Configuration

1. Copy sample configuration files:

   ```
   cp -r config/sample/* config/local/
   cp -r config/sample/* config/docker/
   ```

2. Modify configuration files in `config/local/` and `config/docker/` according to your environment.

3. Set up environment variables in `.env` file:
   ```
   GITHUB_TOKEN=your_github_token
   HACKMD_API_KEY=your_hackmd_api_key
   ```

### Running with Docker

Launch the entire system using Docker Compose:

```
docker-compose up
```

This will start all nodes in the correct order with appropriate networking.

### Running Locally

For development, you can run individual nodes locally:

1. Install dependencies for a specific node:

   ```
   cd nodes/koi-net-processor-a-node
   pip install -e .
   ```

2. Run the node:
   ```
   python -m processor_a_node
   ```

Repeat for other nodes as needed.

## Node Interconnections

The KOI-net system forms a directed knowledge graph:

```
                     ┌─────────────┐
                     │ Coordinator │
                     └──────┬──────┘
                 ┌───────── ┼ ────────┐
                 │          │         │
         ┌───────▼──┐  ┌────▼───┐ ┌───▼────┐
         │  GitHub  │  │ HackMD │ │   ...  │
         │  Sensor  │  │ Sensor │ │ Future │
         └───────┬──┘  └────┬───┘ └───────┘
                 │          │
         ┌───────▼──┐  ┌────▼───┐
         │Processor │  │Processor│
         │    A     │  │    B    │
         └──────────┘  └────────┘
```

Each node automatically discovers and connects to relevant nodes based on the RID types they produce and consume.

## Development

### Adding a New Node

1. Create a directory in `nodes/` for your new node
2. Implement the KOI protocol interfaces
3. Define the RID types the node provides and consumes
4. Add appropriate configuration in `config/`
5. Update the Docker Compose file to include the new node

### Extending Existing Nodes

Refer to each node's README for specific instructions on extending their capabilities:

- [Processor A - GitHub Repository Indexer](nodes/koi-net-processor-a-node/README.md)
- [Processor B - HackMD Note Indexer](nodes/koi-net-processor-b-node/README.md)

## Documentation

- [KOI Protocol Specification](docs/koi-protocol.md)
- [RID Library Documentation](docs/rid-lib.md)
- [Configuration Guide](docs/configuration.md)
- [Docker Deployment](docs/docker.md)

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="config/docker/coordinator.yaml">
runtime:
  base_url: http://coordinator:8080/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8080
</file>

<file path="config/local/coordinator.yaml">
runtime:
  base_url: http://127.0.0.1:8080/koi-net
  cache_dir: .koi/shared_cache
  host: 127.0.0.1
  log_level: INFO
  port: 8080
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/config_loader.py">
import logging
from ruamel.yaml import YAML
from pathlib import Path
import os

# Configure basic logging early
logging.basicConfig(level=logging.INFO, format="%(name)s - %(message)s")
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker based on KOI_CONFIG_MODE and RUN_CONTEXT ---
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
else:
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

print(f"CONFIG_DIR: {CONFIG_DIR}")
print(f"CONFIG_MODE: {CONFIG_MODE}")

CONFIG_PATH = CONFIG_DIR / "coordinator.yaml"
ENV_PATH = CONFIG_DIR / "global.env"

# Load env vars from the selected global.env
if ENV_PATH.is_file():
    with open(ENV_PATH) as f:
        for line in f:
            if line.strip() and not line.startswith('#') and '=' in line:
                k, v = line.strip().split('=', 1)
                os.environ.setdefault(k, v)

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None: # Handle empty file case
            CONFIG = {}
    except Exception as e:
        logging.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logging.error(f"Config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Default Values --- #
DEFAULT_RUNTIME = {
    "port": 8080,
    "host": "0.0.0.0",
    "log_level": "INFO",
    "base_url": None,
    "cache_dir": None,
}

# --- Determine Run Context & Set Defaults ---
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"
RUNTIME_CONFIG = {**DEFAULT_RUNTIME, **CONFIG.get("runtime", {})}

# Adjust paths based on context
LOCAL_DATA_BASE = Path(".koi/shared_cache") # Define a local base if needed
DOCKER_CACHE_DIR_DEFAULT = "/data/cache" # Default for Docker

# Determine Cache Dir
if is_docker:
    CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", DOCKER_CACHE_DIR_DEFAULT)
else:
    # For local, use the config value or construct relative path
    cache_dir_config = RUNTIME_CONFIG.get("cache_dir")
    if cache_dir_config:
        CACHE_DIR = str(Path(cache_dir_config)) # Respect config if set
    else:
        LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
        CACHE_DIR = str(LOCAL_DATA_BASE) # Fallback local path

# Ensure the resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# Export specific values for easier access
PORT = RUNTIME_CONFIG.get("port")
HOST = RUNTIME_CONFIG.get("host")
LOG_LEVEL = RUNTIME_CONFIG.get("log_level", "INFO").upper()
BASE_URL = RUNTIME_CONFIG.get("base_url")

logger.info(
    f"Coordinator configured with MODE={CONFIG_MODE}, PORT={PORT}, BASE_URL={BASE_URL}, LOG_LEVEL={LOG_LEVEL}, CACHE_DIR={CACHE_DIR}"
)

# Check for required config
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not CACHE_DIR:
    logger.critical(
        "Configuration error: runtime.cache_dir is not set or resolved correctly."
    )
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/core.py">
import shutil
import os
import logging  # Import logging
from rid_lib.types import KoiNetNode, KoiNetEdge
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import from the new config loader
from .config_loader import BASE_URL, PORT, CACHE_DIR  # Import BASE_URL, PORT, and CACHE_DIR

logger = logging.getLogger(__name__)  # Get logger instance

name = "coordinator"

# Keep identity/cache dir logic as it's node-specific state, not static config
identity_dir = f".koi/{name}"
# cache_dir = f".koi/{name}/rid_cache_{name}" # REMOVED - Using shared CACHE_DIR now

# Remove existing directories if they exist - REMOVED THESE LINES
# logger.info(
#     f"Attempting to clear existing state directories: {identity_dir}, {cache_dir}"
# )
# shutil.rmtree(identity_dir, ignore_errors=True)
# shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the directories - Ensure only identity_dir is created if needed
os.makedirs(identity_dir, exist_ok=True)
# os.makedirs(cache_dir, exist_ok=True) # REMOVED - Shared cache managed elsewhere
logger.info(f"Ensured identity directory exists: {identity_dir}")


node = NodeInterface(
    name="coordinator",
    profile=NodeProfile(
        # Use the base_url loaded from the YAML config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[KoiNetNode, KoiNetEdge], state=[KoiNetNode, KoiNetEdge]
        ),
    ),
    use_kobj_processor_thread=True,
    # Use resolved absolute paths for state files/dirs
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Corrected to use shared CACHE_DIR from config
    cache_directory_path=CACHE_DIR,
)

# Import handlers after node is initialized
from . import handlers
</file>

<file path="nodes/koi-net-coordinator-node/coordinator_node/handlers.py">
import logging
import time
from rid_lib.types import KoiNetNode, KoiNetEdge
from koi_net.processor import ProcessorInterface
from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject
from koi_net.protocol.event import Event, EventType
from koi_net.protocol.helpers import generate_edge_bundle
from koi_net.protocol.edge import EdgeType
from .core import node

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handshake_handler(proc: ProcessorInterface, kobj: KnowledgeObject):
    logger.info("Handling node handshake")

    # only respond if node declares itself as NEW
    if kobj.event_type != EventType.NEW:
        return

    logger.info("Sharing this node's bundle with peer")
    proc.network.push_event_to(
        event=Event.from_bundle(EventType.NEW, proc.identity.bundle),
        node=kobj.rid,
        flush=False,
    )

    # Introduce a small delay to allow sensor network interface to potentially stabilize
    time.sleep(1)

    logger.info("Proposing new edge")
    # defer handling of proposed edge

    edge_bundle = generate_edge_bundle(
        source=kobj.rid,
        target=proc.identity.rid,
        edge_type=EdgeType.WEBHOOK,
        rid_types=[KoiNetNode, KoiNetEdge],
    )

    proc.handle(bundle=edge_bundle)
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/config.py">
import logging
import json
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import List, Dict, Any

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker based on KOI_CONFIG_MODE and RUN_CONTEXT ---
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
else:
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

CONFIG_PATH = CONFIG_DIR / "github-sensor.yaml"
ENV_PATH = CONFIG_DIR / "global.env"

# Load env vars from the selected global.env
if ENV_PATH.is_file():
    with open(ENV_PATH) as f:
        for line in f:
            if line.strip() and not line.startswith('#') and '=' in line:
                k, v = line.strip().split('=', 1)
                os.environ.setdefault(k, v) # Use setdefault to avoid overriding existing env vars

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None: # Handle empty file case
            CONFIG = {}
    except Exception as e:
        logging.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logging.error(f"Config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Determine Run Context ---
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"

# --- Extract specific configurations with defaults ---
SENSOR_CONFIG: Dict[str, Any] = CONFIG.get("sensor", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
WEBHOOK_CONFIG: Dict[str, Any] = CONFIG.get("webhook", {})

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/github")  # Relative to workspace root for local runs

# Base configuration values (URLs are now directly from the correct mode's YAML)
MONITORED_REPOS: List[str] = SENSOR_CONFIG.get("repos", [])
POLL_INTERVAL: int = SENSOR_CONFIG.get("poll_interval", 60)
SENSOR_MODE: str = SENSOR_CONFIG.get("mode", "webhook")
HOST: str = RUNTIME_CONFIG.get("host", "0.0.0.0" if not is_docker else "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8001)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()
COORDINATOR_URL = EDGES_CONFIG.get("coordinator_url")
BASE_URL = RUNTIME_CONFIG.get("base_url")

# Adjust Cache and State Paths based on run context
DOCKER_CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", "/data/cache") # Default Docker path
DOCKER_STATE_FILE = RUNTIME_CONFIG.get(
    "state_file", os.path.join(DOCKER_CACHE_DIR, "github_state.json")
)

if is_docker:
    CACHE_DIR = DOCKER_CACHE_DIR
    STATE_FILE_PATH = (
        Path(DOCKER_STATE_FILE)
        if Path(DOCKER_STATE_FILE).is_absolute()
        else Path("/app") / DOCKER_STATE_FILE # Assume /app if relative in Docker
    )
else:
    LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
    CACHE_DIR = str(LOCAL_DATA_BASE / "cache")
    STATE_FILE_PATH = LOCAL_DATA_BASE / "github_state.json"
    logger.debug(f"Using local CACHE_DIR: {CACHE_DIR}")
    logger.debug(f"Using local STATE_FILE_PATH: {STATE_FILE_PATH}")

# Ensure resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Load Secrets from Environment Variables ---
GITHUB_TOKEN: str | None = os.getenv("GITHUB_TOKEN")
WEBHOOK_SECRET_ENV_VAR: str | None = WEBHOOK_CONFIG.get("secret_env_var")
GITHUB_WEBHOOK_SECRET: str | None = None
if WEBHOOK_SECRET_ENV_VAR:
    GITHUB_WEBHOOK_SECRET = os.getenv(WEBHOOK_SECRET_ENV_VAR)
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning(
            f"Environment variable '{WEBHOOK_SECRET_ENV_VAR}' specified in config but not found in environment."
        )
else:
    logger.warning("webhook.secret_env_var not specified in github-sensor.yaml")

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL)
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logging.getLogger("github").setLevel(logging.WARNING)
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("GitHub Sensor Configuration Loaded:")
logger.info(f"  Config Mode: {CONFIG_MODE}")
logger.info(f"  Is Docker Context: {is_docker}")
logger.info(f"  Sensor Mode: {SENSOR_MODE}")
logger.info(f"  Monitored Repos: {MONITORED_REPOS}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  State File Path: {STATE_FILE_PATH}")
logger.info(f"  GitHub Token Loaded: {bool(GITHUB_TOKEN)}")
logger.info(f"  Webhook Secret Loaded: {bool(GITHUB_WEBHOOK_SECRET)}")

# Check required config
if not COORDINATOR_URL:
    logger.critical(
        "Configuration error: edges.coordinator_url is not set or resolved correctly."
    )
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not GITHUB_WEBHOOK_SECRET:
    logger.warning("Configuration warning: GitHub Webhook Secret not loaded.")

# --- State Management (Loading initial state & update function) ---
LAST_PROCESSED_SHA: Dict[str, str] = {}  # Dictionary mapping repo_name -> last_sha

def load_state():
    """Loads the last processed SHA state from the JSON file specified by STATE_FILE_PATH."""
    global LAST_PROCESSED_SHA
    state_path = STATE_FILE_PATH
    try:
        with open(state_path, "r") as f:
            LAST_PROCESSED_SHA = json.load(f)
        logger.info(
            f"Loaded state from '{state_path}': Repos {list(LAST_PROCESSED_SHA.keys())}"
        )
    except FileNotFoundError:
        logger.warning(
            f"State file '{state_path}' not found. This is expected on first run. Starting with empty state (will perform full backfill)."
        )
        LAST_PROCESSED_SHA = {}
    except json.JSONDecodeError:
        logger.error(
            f"Error decoding JSON from state file '{state_path}'. Starting with empty state."
        )
        LAST_PROCESSED_SHA = {}
    except Exception as e:
        logger.error(
            f"Unexpected error loading state file '{state_path}': {e}",
            exc_info=True,
        )
        LAST_PROCESSED_SHA = {}

def update_state_file(repo_name: str, last_sha: str):
    """Updates the state file with the latest processed SHA for a repo."""
    global LAST_PROCESSED_SHA
    LAST_PROCESSED_SHA[repo_name] = last_sha

    state_path = STATE_FILE_PATH
    try:
        state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(state_path, "w") as f:
            json.dump(LAST_PROCESSED_SHA, f, indent=4)
        logger.debug(
            f"Updated state file '{state_path}' for {repo_name} with SHA: {last_sha}"
        )
    except IOError as e:
        logger.error(f"Failed to write state file '{state_path}': {e}")
    except Exception as e:
        logger.error(
            f"Unexpected error writing state file '{state_path}': {e}",
            exc_info=True,
        )

# Load initial state when config module is imported
load_state()
</file>

<file path="nodes/koi-net-github-sensor-node/github_sensor_node/core.py">
import os
import shutil
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the refactored config module
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
    LOG_LEVEL,
)  # Import necessary vars
from .types import GithubCommit

logger = logging.getLogger(__name__)

name = "github"

# Identity and cache directory paths remain relative for setup,
# but NodeInterface gets absolute paths derived from config
identity_dir = f".koi/{name}"
# Cache dir is now defined in config, use that path for NodeInterface
# cache_dir_setup = f".koi/{name}/rid_cache_{name}"

# Clear existing state directories logic remains the same
# Consider making this behavior optional or configurable
# logger.info(f"Attempting to clear existing identity directory: {identity_dir}")
# shutil.rmtree(identity_dir, ignore_errors=True)
# Cache dir managed by Docker volume, no need to clear here
# shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the identity directory
os.makedirs(identity_dir, exist_ok=True)
# Ensure cache directory exists within the container is handled by Docker volume mount implicitly
# os.makedirs(cache_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Ensure required config values are present
if not BASE_URL:
    raise ValueError("Runtime base_url is not configured in github-sensor.yaml")
if not COORDINATOR_URL:
    raise ValueError("Edges coordinator_url is not configured in github-sensor.yaml")

# Initialize the KOI-net Node Interface for the GitHub Sensor
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        # Use BASE_URL from config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[GithubCommit],
            state=[GithubCommit],
        ),
    ),
    use_kobj_processor_thread=True,
    # Use COORDINATOR_URL from config for first_contact
    first_contact=COORDINATOR_URL,
    # State file paths are now relative to the application root in the container
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Use CACHE_DIR from config (should be /data/cache)
    cache_directory_path=CACHE_DIR,
)

logger.info(f"Initialized NodeInterface: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/config.py">
import logging
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import Dict, Any
import json

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker based on KOI_CONFIG_MODE and RUN_CONTEXT ---
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
else:
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

CONFIG_PATH = CONFIG_DIR / "hackmd-sensor.yaml"
ENV_PATH = CONFIG_DIR / "global.env"

# Load env vars from the selected global.env
if ENV_PATH.is_file():
    with open(ENV_PATH) as f:
        for line in f:
            if line.strip() and not line.startswith('#') and '=' in line:
                k, v = line.strip().split('=', 1)
                os.environ.setdefault(k, v)

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None: # Handle empty file case
            CONFIG = {}
    except Exception as e:
        logging.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logging.error(f"Config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Determine Run Context ---
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"

# --- Extract specific configurations with defaults ---
SENSOR_CONFIG: Dict[str, Any] = CONFIG.get("sensor", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
API_CONFIG: Dict[str, Any] = CONFIG.get("api", {})

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/hackmd")

# Base config values (URLs are now directly from the correct mode's YAML)
TEAM_PATH: str = SENSOR_CONFIG.get("team_path", "blockscience")
POLL_INTERVAL: int = SENSOR_CONFIG.get("poll_interval", 300)
HOST: str = RUNTIME_CONFIG.get("host", "127.0.0.1" if not is_docker else "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8002)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()
TARGET_NOTE_IDS: list[str] = SENSOR_CONFIG.get("target_note_ids", [])
COORDINATOR_URL = EDGES_CONFIG.get("coordinator_url")
BASE_URL = RUNTIME_CONFIG.get("base_url")

# Adjust Cache and State Paths based on run context
DOCKER_CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", "/data/cache")
DOCKER_STATE_FILE = RUNTIME_CONFIG.get(
    "state_file", os.path.join(DOCKER_CACHE_DIR, "hackmd_state.json")
)

if is_docker:
    CACHE_DIR = DOCKER_CACHE_DIR
    STATE_FILE_PATH = (
        Path(DOCKER_STATE_FILE)
        if Path(DOCKER_STATE_FILE).is_absolute()
        else Path("/app") / DOCKER_STATE_FILE
    )
else:
    LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
    CACHE_DIR = str(LOCAL_DATA_BASE / "cache")
    STATE_FILE_PATH = LOCAL_DATA_BASE / "hackmd_state.json"
    logger.debug(f"Using local CACHE_DIR: {CACHE_DIR}")
    logger.debug(f"Using local STATE_FILE_PATH: {STATE_FILE_PATH}")

# Ensure resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Load Secrets from Environment Variables ---
TOKEN_ENV_VAR: str | None = API_CONFIG.get("token_env_var")
HACKMD_API_TOKEN: str | None = None
if TOKEN_ENV_VAR:
    HACKMD_API_TOKEN = os.getenv(TOKEN_ENV_VAR)
    if not HACKMD_API_TOKEN:
        logger.warning(
            f"Environment variable '{TOKEN_ENV_VAR}' specified in config but not found in environment."
        )
else:
    logger.warning("api.token_env_var not specified in hackmd-sensor.yaml")

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL)
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("HackMD Sensor Configuration Loaded:")
logger.info(f"  Config Mode: {CONFIG_MODE}")
logger.info(f"  Is Docker Context: {is_docker}")
logger.info(f"  Team Path: {TEAM_PATH}")
logger.info(f"  Poll Interval: {POLL_INTERVAL}s")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  State File Path: {STATE_FILE_PATH}")
logger.info(f"  HackMD Token Loaded: {bool(HACKMD_API_TOKEN)}")
logger.info(
    f"  Target Note IDs: {TARGET_NOTE_IDS if TARGET_NOTE_IDS else 'All notes in team'}"
)

# Check for required configuration
if not COORDINATOR_URL:
    logger.critical(
        "Configuration error: edges.coordinator_url is not set or resolved correctly."
    )
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not HACKMD_API_TOKEN:
    logger.warning(
        f"Configuration warning: HackMD API token not loaded (check '{TOKEN_ENV_VAR}' in global.env and config)"
    )

# --- State Management (Moved from __main__) ---
polling_state: Dict[str, str] = {}  # Maps note_id to last_modified_timestamp

def load_polling_state():
    """Loads the polling state from the JSON file specified by STATE_FILE_PATH."""
    global polling_state
    state_path = STATE_FILE_PATH
    try:
        with open(state_path, "r") as f:
            polling_state = json.load(f)
        logger.info(f"Loaded polling state from {state_path}")
    except FileNotFoundError:
        logger.warning(
            f"Polling state file not found at {state_path}. This is expected on first run. Starting fresh."
        )
        polling_state = {}
    except json.JSONDecodeError:
        logger.error(
            f"Error decoding JSON from state file {state_path}. Starting fresh."
        )
        polling_state = {}
    except Exception as e:
        logger.error(
            f"Unexpected error loading polling state file {state_path}: {e}",
            exc_info=True,
        )
        polling_state = {}

def save_polling_state():
    """Saves the current polling state to the JSON file specified by STATE_FILE_PATH."""
    global polling_state
    state_path = STATE_FILE_PATH
    try:
        state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(state_path, "w") as f:
            json.dump(polling_state, f, indent=4)
        logger.debug(f"Saved polling state to {state_path}")
    except IOError as e:
        logger.error(f"Failed to write polling state file {state_path}: {e}")
    except Exception as e:
        logger.error(
            f"Unexpected error writing polling state file {state_path}: {e}",
            exc_info=True,
        )

# Load initial state when config module is imported
load_polling_state()
</file>

<file path="nodes/koi-net-hackmd-sensor-node/hackmd_sensor_node/core.py">
import logging
import shutil
import os
from rid_types import HackMDNote
from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides
from koi_net.processor.default_handlers import (
    basic_rid_handler,
    edge_negotiation_handler,
    basic_network_output_filter,
)

# Import from refactored config
from .config import BASE_URL, COORDINATOR_URL, CACHE_DIR

logger = logging.getLogger(__name__)

name = "hackmd"

# Keep identity dir logic, cache dir comes from config
identity_dir = f".koi/{name}"
# cache_dir = f".koi/{name}/rid_cache_{name}" # Removed

# Clear existing identity directory - REMOVED
# logger.info(f"Attempting to clear existing identity directory: {identity_dir}")
# shutil.rmtree(identity_dir, ignore_errors=True)

# Recreate the identity directory - Ensure it exists
os.makedirs(identity_dir, exist_ok=True)
# os.makedirs(cache_dir, exist_ok=True) # Cache dir managed by Docker volume
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Ensure required config values are present (already logged in config.py)
if not BASE_URL or not COORDINATOR_URL:
    # Optional: Raise error here if critical config is missing
    logger.critical(
        "Required configuration (BASE_URL or COORDINATOR_URL) missing. Node might not function correctly."
    )

node = NodeInterface(
    name="hackmd-sensor",
    profile=NodeProfile(
        # Use BASE_URL from config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(event=[HackMDNote], state=[HackMDNote]),
    ),
    use_kobj_processor_thread=True,
    # Use COORDINATOR_URL from config for first_contact
    first_contact=COORDINATOR_URL,
    handlers=[basic_rid_handler, edge_negotiation_handler, basic_network_output_filter],
    # Use absolute paths for state files/dirs
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Use CACHE_DIR from config
    cache_directory_path=CACHE_DIR,
)

# Import handlers after node is initialized
from . import handlers
</file>

<file path="Makefile">
.PHONY: setup install clean coordinator github hackmd demo-coordinator demo-github demo-hackmd docker-clean rebuild docker-rebuild clean-cache up down

setup:
	@echo "Creating virtual environment with uv..."
	uv venv --python 3.12
	@echo "Virtual environment created at .venv"
	@echo "Run: source .venv/bin/activate"

install:
	@echo "Installing root package..."
	uv pip install -e .
	@echo "Installing coordinator service..."
	uv pip install -e nodes/koi-net-coordinator-node/
	@echo "Installing github service..."
	uv pip install -e nodes/koi-net-github-sensor-node/
	@echo "Installing hackmd service..."
	uv pip install -e nodes/koi-net-hackmd-sensor-node/
	@echo "Installing rid_types package..."
	uv pip install -e rid_types/
	@echo "Installing processor-a-node..."
	uv pip install -e nodes/koi-net-processor-a-node/
	@echo "Installing processor-b-node..."
	uv pip install -e nodes/koi-net-processor-b-node/
	@echo "All packages installed successfully!"

clean:
	@echo "Removing virtual environment..."
	rm -rf .venv
	@echo "Removing build artifacts..."
	find . -type d -name '__pycache__' -exec rm -rf {} +
	find . -type d -name '*.egg-info' -exec rm -rf {} +
	@echo "Clean complete."

clean-cache:
	@echo "Removing cache..."
	rm -rf .koi
	@echo "Cache removed."

coordinator:
	@echo "Running Coordinator Node..."
	export KOI_CONFIG_MODE=local
	.venv/bin/python3 -m nodes.koi-net-coordinator-node.coordinator_node

github:
	@echo "Running Github Node..."
	export KOI_CONFIG_MODE=local
	.venv/bin/python3 -m nodes.koi-net-github-sensor-node.github_sensor_node

hackmd:
	@echo "Running HackMD Node..."
	export KOI_CONFIG_MODE=local
	.venv/bin/python3 -m nodes.koi-net-hackmd-sensor-node.hackmd_sensor_node

demo-coordinator:
	@echo "Starting Coordinator via Docker Compose..."
	docker compose build --no-cache coordinator
	docker compose up coordinator

demo-github:
	@echo "Starting GitHub sensor via Docker Compose..."
	docker compose build --no-cache github-sensor
	docker compose up -d github-sensor

demo-hackmd:
	@echo "Starting HackMD sensor via Docker Compose..."
	docker compose build --no-cache hackmd-sensor
	docker compose up -d hackmd-sensor

docker-clean:
	@echo "Cleaning up all Docker containers and images..."
	docker compose down --rmi all
	@echo "Docker cleanup complete."

docker-rebuild:
	@echo "Rebuilding Docker images with no cache..."
	docker compose build --no-cache
	@echo "Starting Docker services..."
	docker compose up -d

up:
	@echo "Starting Docker services..."
	docker compose up 

down:
	@echo "Stopping Docker services..."
	docker compose down
</file>

<file path="docker-compose.yaml">
services:
  coordinator:
    build:
      context: ./nodes/koi-net-coordinator-node
    ports:
      - "8080:8080"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
    volumes:
      - ./config:/config:ro
      - cache_data:/data/cache
      - coordinator_state_data:/app/.koi/coordinator
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  github-sensor:
    build:
      context: ./nodes/koi-net-github-sensor-node
    ports:
      - "8001:8001"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
    volumes:
      - ./config:/config:ro
      - cache_data:/data/cache
      - github_state_data:/app/.koi/github
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8001/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  hackmd-sensor:
    build:
      context: ./nodes/koi-net-hackmd-sensor-node
    ports:
      - "8002:8002"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
    volumes:
      - ./config:/config:ro
      - cache_data:/data/cache
      - hackmd_state_data:/app/.koi/hackmd
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8002/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  processor-a:
    build:
      context: ./nodes/koi-net-processor-a-node
    ports:
      - "8011:8011"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
    volumes:
      - ./config:/config:ro
      - cache_data:/data/cache
      - processor_a_state:/app/.koi/processor-a
    depends_on:
      coordinator:
        condition: service_healthy
      github-sensor:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8011/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  processor-b:
    build:
      context: ./nodes/koi-net-processor-b-node
    ports:
      - "8012:8012"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
    volumes:
      - ./config:/config:ro
      - cache_data:/data/cache
      - processor_b_state:/app/.koi/processor-b
    depends_on:
      coordinator:
        condition: service_healthy
      hackmd-sensor:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8012/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

volumes:
  cache_data:
    driver: local
  coordinator_state_data:
    driver: local
  github_state_data:
    driver: local
  hackmd_state_data:
    driver: local
  processor_a_state:
    driver: local
  processor_b_state:
    driver: local

networks:
  koinet:
    driver: bridge
</file>

</files>
