This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: nodes/koi-net-processor-a-node/**/*, nodes/koi-net-processor-b-node/**/*, docker-compose.yaml
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
nodes/
  koi-net-processor-a-node/
    processor_a_node/
      __init__.py
      __main__.py
      config.py
      core.py
      handlers.py
      server.py
    .gitignore
    Dockerfile
    pyproject.toml
    README.md
  koi-net-processor-b-node/
    processor_b_node/
      __init__.py
      __main__.py
      config.py
      core.py
      handlers.py
      server.py
    .gitignore
    Dockerfile
    pyproject.toml
    README.md
docker-compose.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="nodes/koi-net-processor-a-node/.gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Config/State
*.yaml
*.env
*.json
.koi/
</file>

<file path="nodes/koi-net-processor-a-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy shared rid_types package into a proper project root
COPY ./rid_types /app/rid_types

# 3) install rid_types in editable mode
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e /app/rid_types

# 4) ensure Python can always find rid_types
ENV PYTHONPATH="/app/rid_types:${PYTHONPATH}"

# 5) copy processor-a pyproject.toml
COPY ./nodes/koi-net-processor-a-node/pyproject.toml /app/

# 6) install curl for healthcheck (and clean up)
RUN apt-get update \
 && apt-get install -y curl \
 && rm -rf /var/lib/apt/lists/*

# 7) install processor-a (and its dependencies) in editable mode
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 8) copy in the rest of the processor-a code
COPY ./nodes/koi-net-processor-a-node/. /app/

# 9) expose port and add healthcheck
EXPOSE 8011
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8011/koi-net/health || exit 1

# 10) start the service
CMD ["uvicorn", "processor_a_node.server:app", "--host", "0.0.0.0", "--port", "8011"]
</file>

<file path="nodes/koi-net-processor-a-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "processor-a-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "httpx", 
    "python-dotenv",
    "rid-lib>=3.2.3", # Ensure compatible versions
    "koi-net==1.0.0b12", 
    "rich", 
    "ruamel.yaml",
    # Add any specific dependencies for Processor A later
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="nodes/koi-net-processor-b-node/.gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Config/State
*.yaml
*.env
*.json
.koi/
</file>

<file path="nodes/koi-net-processor-b-node/Dockerfile">
FROM python:3.12-slim

# 1) copy UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# 2) copy shared rid_types package into correct path
COPY ./rid_types /app/rid_types

# 3) install rid_types in editable mode
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e /app/rid_types

# 4) ensure Python can locate rid_types
ENV PYTHONPATH="/app/rid_types:${PYTHONPATH}"

# 5) copy processor‑b pyproject.toml
COPY ./nodes/koi-net-processor-b-node/pyproject.toml /app/

# 6) install curl for healthcheck (and clean up)
RUN apt-get update \
 && apt-get install -y curl \
 && rm -rf /var/lib/apt/lists/*

# 7) install processor‑b (and its deps) in editable mode
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# 8) copy in the rest of the processor‑b code
COPY ./nodes/koi-net-processor-b-node/. /app/

# 9) expose port and add healthcheck
EXPOSE 8012
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8012/koi-net/health || exit 1

# 10) start the service
CMD ["uvicorn", "processor_b_node.server:app", "--host", "0.0.0.0", "--port", "8012"]
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/core.py">
import os
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the final config loader
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
)

# Import the RID type this processor consumes
logger = logging.getLogger(__name__)


name = "processor-a"

# Identity directory setup - relative to where the node is run
identity_dir = f".koi/{name}"
os.makedirs(identity_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Check required config values (basic check)
if not BASE_URL or not COORDINATOR_URL or not CACHE_DIR:
    # Config loader already logs critical errors, this is an extra safeguard
    raise ValueError(
        "Essential configuration (BASE_URL, COORDINATOR_URL, CACHE_DIR) is missing or failed to load!"
    )

# Initialize the KOI-net Node Interface for Processor A
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(  # Processor A provides no new RIDs per PRD
            event=[], state=[]
        ),
        # Consumes GithubCommit implicitly via handlers
    ),
    use_kobj_processor_thread=True,  # Run processing in a separate thread
    first_contact=COORDINATOR_URL,
    # Use absolute paths for state files to avoid ambiguity
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,  # Use the resolved cache directory path
)

logger.info(f"Initialized NodeInterface for Processor A: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")

# Import handlers after node is initialized to allow decorator registration
from . import handlers
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/server.py">
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, HTTPException

from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from koi_net.processor.knowledge_object import KnowledgeSource

from .core import node  # Import the initialized node instance

# Import the query helper and index from handlers
from .handlers import query_search_index, search_index

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup and shutdown."""
    logger.info("Starting Processor A lifespan...")
    try:
        node.start()
        logger.info("Processor A KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        # Potentially exit or raise to prevent FastAPI from starting improperly
        raise RuntimeError("Failed to initialize KOI-net node") from e

    yield  # Application runs here

    logger.info("Shutting down Processor A...")
    try:
        node.stop()
        logger.info("Processor A KOI-net node stopped successfully.")
    except Exception as e:
        logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
    logger.info("Processor A shutdown complete.")


app = FastAPI(
    title="KOI-net Processor A Node (Repo Indexer)",
    description="Processes GitHub commits and provides a search API.",
    version="0.1.0",
    lifespan=lifespan,
)

# --- KOI Protocol Router ---
koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    # Basic validation
    if not req.events:
        logger.debug("Received empty broadcast event list.")
        return {}

    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    # Asynchronously handle each event to avoid blocking the endpoint
    for event in req.events:
        try:
            # Assuming node.processor.handle is thread-safe or handles async appropriately
            node.processor.handle(event=event, source=KnowledgeSource.External)
        except Exception as e:
            logger.error(
                f"Error handling broadcast event {event.rid}: {e}", exc_info=True
            )
            # Decide if one error should stop processing others
    return {}


@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH} for {req.rid}")
    try:
        events = node.network.flush_poll_queue(req.rid)
        logger.debug(f"Returning {len(events)} events for {req.rid}")
        return EventsPayload(events=events)
    except Exception as e:
        logger.error(f"Error polling events for {req.rid}: {e}", exc_info=True)
        return EventsPayload(events=[])  # Return empty list on error


@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    try:
        return node.network.response_handler.fetch_rids(req)
    except Exception as e:
        logger.error(f"Error fetching RIDs: {e}", exc_info=True)
        # Return empty payload or raise HTTP exception?
        return RidsPayload(rids=[])


@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    try:
        return node.network.response_handler.fetch_manifests(req)
    except Exception as e:
        logger.error(f"Error fetching Manifests: {e}", exc_info=True)
        return ManifestsPayload(manifests=[], not_found=req.rids or [])


@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    try:
        return node.network.response_handler.fetch_bundles(req)
    except Exception as e:
        logger.error(f"Error fetching Bundles: {e}", exc_info=True)
        return BundlesPayload(bundles=[], not_found=req.rids or [])


# Add health endpoint
@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed
    return {"status": "healthy"}


app.include_router(koi_net_router)

# --- Custom Search API Router ---
search_router = APIRouter()


@search_router.get("/search")
async def search_commits_endpoint(q: str):
    """Endpoint to search the indexed commit data."""
    if not q:
        raise HTTPException(status_code=400, detail="Query parameter 'q' is required.")

    logger.info(f"Search request received: q='{q}'")
    try:
        # Use the helper function from handlers
        results = query_search_index(q)
        logger.info(f"Search for '{q}' yielded {len(results)} results.")
        # Note: The current index only returns SHA and context.
        # A real implementation would likely reconstruct the full RID.
        return {"query": q, "results": results}
    except Exception as e:
        logger.error(f"Error during search for query '{q}': {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail="Internal server error during search."
        )


# Include the custom search router *without* the /koi-net prefix
app.include_router(search_router)

logger.info("Processor A FastAPI application configured with KOI and Search routers.")
</file>

<file path="nodes/koi-net-processor-a-node/README.md">
# Processor A - GitHub Repository Indexer

## Core Logic Overview

This section provides a deep dive into Processor A's implementation details, highlighting the core components and logic that power its functionality.

### 1. Key Constants & Classes

- **`GITHUB_SENSOR_RID`**: Optional configuration parameter that allows specifying a particular GitHub sensor node to connect to, rather than discovering any available GitHub sensor.
- **`GithubCommit`**: The primary RID class that Processor A consumes, representing GitHub commit data.
- **`search_index`**: In-memory structure that maps SHA hashes and keywords to commit information.
- **`KnowledgeSource.External`**: Constants used to indicate the source of knowledge objects during processing.

### 2. How KOI & RID-lib Are Used

#### Node Interface Initialization

```python
node = NodeInterface(
    name="processor-a",
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[],
            state=[]
        ),
        # Consumes GithubCommit implicitly via handlers
    ),
    use_kobj_processor_thread=True,
    first_contact=COORDINATOR_URL,
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,
)
```

#### Registered Event Handlers

1. **Network Handler**: Processes node discovery events

   ```python
   @node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
   def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Handles discovery of Coordinator and GitHub Sensor nodes
   ```

2. **Manifest Handler**: Processes commit manifest events

   ```python
   @node.processor.register_handler(HandlerType.Manifest, rid_types=[GithubCommit])
   def handle_commit_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Handles incoming commit manifests, triggers bundle fetch
   ```

3. **Bundle Handler**: Processes commit bundle data
   ```python
   @node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
   def handle_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Processes commit bundle contents and updates the search index
   ```

#### RID-lib Usage

Processor A uses RID-lib for:

- Handling standard KOI-net RIDs (`KoiNetNode`, `KoiNetEdge`)
- Processing GitHub commit RIDs represented as `GithubCommit` classes with `reference` properties
- Automatic bundle dereferencing using the processor's built-in mechanisms

### 3. Proprietary Analysis Logic

The core analysis logic resides in the bundle handler where GitHub commits are indexed:

```python
# Extract message for keyword indexing
message = contents.get("message", "")

# Index by keywords (simple example implementation)
keywords_processed = set()
for keyword in message.lower().split():
    # Basic filtering: length > 3, alphanumeric, avoid duplicates per message
    if len(keyword) > 3 and keyword.isalnum() and keyword not in keywords_processed:
        if keyword not in search_index:
            search_index[keyword] = []
        # Add SHA to keyword list if not already present
        if sha not in search_index[keyword]:
             search_index[keyword].append(sha)
        keywords_processed.add(keyword)
```

The indexing strategy includes:

- Direct SHA-based indexing for exact match lookups
- Keyword extraction from commit messages for content-based search
- Basic filtering rules (length > 3, alphanumeric content)
- Handling of re-indexing situations for updated commits

### 4. Custom Modules & Functions

The node is implemented in a modular structure:

| Module          | Key Functions                  | Purpose                                         |
| --------------- | ------------------------------ | ----------------------------------------------- |
| **core.py**     | `NodeInterface` initialization | Establishes the node's identity and connections |
| **handlers.py** | `handle_network_discovery()`   | Detects Coordinator and GitHub sensor nodes     |
|                 | `handle_commit_manifest()`     | Processes commit manifest events                |
|                 | `handle_commit_bundle()`       | Indexes commit data from bundles                |
|                 | `query_search_index()`         | Implements search functionality                 |
| **server.py**   | `broadcast_events_endpoint()`  | Receives events from other nodes                |
|                 | `search_commits_endpoint()`    | Exposes the search API                          |

#### Search Query Implementation

```python
def query_search_index(query: str) -> list:
    """Queries the in-memory search index."""
    results = []
    query_lower = query.lower()

    # Check if query is a SHA (full or partial)
    if len(query) >= 7:
        # Check full and partial SHA matches
        # ...

    # Check if query is a keyword
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        for sha in search_index[query_lower]:
            # Add keyword matches to results
            # ...

    return results
```

### 5. Configuration & Environment Variables

Key configuration parameters include:

| Parameter           | Default                                                                           | Purpose                      |
| ------------------- | --------------------------------------------------------------------------------- | ---------------------------- |
| `BASE_URL`          | http://{host}:{port}/koi-net                                                      | The node's public endpoint   |
| `COORDINATOR_URL`   | http://coordinator:8080/koi-net (Docker)<br>http://127.0.0.1:8080/koi-net (Local) | The Coordinator node URL     |
| `CACHE_DIR`         | /data/cache (Docker)<br>./.koi/processor-a/cache (Local)                          | Location for cached data     |
| `HOST`              | 0.0.0.0 (Docker)<br>127.0.0.1 (Local)                                             | Interface to bind to         |
| `PORT`              | 8011                                                                              | HTTP server port             |
| `LOG_LEVEL`         | INFO                                                                              | Logging verbosity            |
| `GITHUB_SENSOR_RID` | None                                                                              | Optional specific sensor RID |

The configuration is loaded from YAML files in `config/docker/` or `config/local/` depending on the deployment mode, which is determined by the `KOI_CONFIG_MODE` and `RUN_CONTEXT` environment variables.

### 6. Sample Snippet: Core Processing Flow

The following code shows the complete lifecycle of processing a GitHub commit:

```python
# Step 1: Discover and connect to GitHub sensor
@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
    # Validate discovered node
    profile = kobj.bundle.validate_contents(NodeProfile)

    # Check if node provides GitHub commits
    provides_github_commits = False
    rid_type_str = "orn:github.commit"
    provides_event = [str(rt) for rt in profile.provides.event if hasattr(profile.provides, 'event')]

    if rid_type_str in provides_event:
        provides_github_commits = True

    # If it's a GitHub sensor, propose an edge to receive commits
    if provides_github_commits:
        logger.info(f"Discovered GitHub Sensor: {kobj.rid}. Proposing edge.")
        edge_bundle = generate_edge_bundle(
            source=kobj.rid,  # Sensor is the source
            target=processor.identity.rid,  # We are the target
            edge_type=EdgeType.WEBHOOK,
            rid_types=[GithubCommit],  # We want GithubCommit events
        )
        processor.handle(bundle=edge_bundle)

# Step 2: Receive commit manifest and request bundle
@node.processor.register_handler(HandlerType.Manifest, rid_types=[GithubCommit])
def handle_commit_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    rid = kobj.rid
    logger.info(f"Received manifest for commit: {rid.reference}")

    # Request full bundle for indexing
    processor.handle(rid=rid, source=KnowledgeSource.External)

# Step 3: Process commit bundle and update index
@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    contents = kobj.contents
    sha = contents.get("sha")
    logger.info(f"Processing bundle for commit: {sha[:7]}")

    # Update search index
    message = contents.get("message", "")
    search_index[sha] = message  # Index by full SHA

    # Index by keywords from commit message
    for keyword in message.lower().split():
        if len(keyword) > 3 and keyword.isalnum():
            if keyword not in search_index:
                search_index[keyword] = []
            if sha not in search_index[keyword]:
                search_index[keyword].append(sha)
```

This code demonstrates how Processor A discovers GitHub sensors, establishes connections, receives commit data, and builds its search index - all using the KOI-net protocol's event-driven architecture.

## Overview

The Processor A node is a specialized KOI-net component that consumes GitHub commit data from the GitHub sensor node, builds a searchable index, and provides a search API for querying repository information. It serves as a critical component in the KOI-net knowledge mesh by transforming raw commit data into a queryable knowledge base.

## Features

- Automatically connects to the KOI-net Coordinator and GitHub sensor nodes
- Processes GitHub commit data in real-time
- Maintains an in-memory search index for quick lookup
- Exposes a simple HTTP API for searching indexed commits
- Follows the KOI protocol for data exchange and network operations
- Supports both Docker and local deployment modes

## Architecture

### Network Integration

Processor A establishes bidirectional connections with:

1. **Coordinator Node**: For network discovery and general communication
2. **GitHub Sensor**: To receive GitHub commit events

The node uses the KOI protocol's edge mechanism to establish these connections, with handlers registering for specific Resource ID (RID) types:

```
Coordinator <---> Processor A <--- GitHub Sensor
```

### Data Processing Flow

1. **Discovery**: The network handler detects GitHub sensor nodes on the network
2. **Connection**: Automatically proposes edges to receive GitHub commit events
3. **Manifest Processing**: Receives commit manifests and requests the full bundle
4. **Indexing**: Processes commit bundles to build and maintain a search index
5. **Search API**: Provides endpoint for searching through indexed commits

### Search Index

The search index uses a simple in-memory structure:

```
{
  sha: commit_message,              # Direct SHA lookup
  keyword: [sha1, sha2, ...],       # Keyword to SHAs mapping
  ...
}
```

This enables multiple search capabilities:

- Exact SHA lookups
- Partial SHA matching (≥7 characters)
- Keyword-based search

## API Endpoints

### KOI Protocol Endpoints

- `POST /koi-net/broadcast-events`: Receive events from other nodes
- `POST /koi-net/poll-events`: Provide events to nodes polling this one
- `POST /koi-net/fetch-rids`: Return RIDs matching requested types
- `POST /koi-net/fetch-manifests`: Return manifests for requested RIDs
- `POST /koi-net/fetch-bundles`: Return bundles for requested RIDs
- `GET /koi-net/health`: Check node health status

### Custom Endpoints

- `GET /search?q=<query>`: Search indexed commits
  - Query can be a SHA (full or partial), or keyword
  - Returns matching commit information

## Configuration

### Local Mode

Configuration is loaded from `config/local/processor-a.yaml`:

```yaml
runtime:
  base_url: http://127.0.0.1:8011/koi-net
  cache_dir: ./.koi/processor-a/cache
  host: 127.0.0.1
  log_level: DEBUG
  port: 8011

edges:
  coordinator_url: http://127.0.0.1:8080/koi-net
# Optional: specify a particular GitHub sensor RID
# processor_a:
#   github_sensor_rid: "..."
```

### Docker Mode

When running in Docker, configuration is loaded from `config/docker/processor-a.yaml`:

```yaml
runtime:
  base_url: http://processor-a:8011/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8011

edges:
  coordinator_url: http://coordinator:8080/koi-net
```

## Deployment

### Local Development

1. Ensure the Python dependencies are installed:

   ```
   pip install -e .
   ```

2. Run the node:
   ```
   python -m processor_a_node
   ```

### Docker Deployment

Use Docker Compose to deploy the entire KOI-net system:

```
docker-compose up
```

Or run just this node:

```
docker-compose up processor-a
```

## Development and Extension

The node is designed with the following module structure:

- `__init__.py`: Configures logging
- `__main__.py`: Entry point that starts the node
- `config.py`: Configuration loading and validation
- `core.py`: Core node initialization
- `handlers.py`: Event and data processing handlers
- `server.py`: FastAPI application and endpoint definitions

To extend the search capabilities:

1. Modify the indexing logic in `handlers.py`
2. Update the query implementation in `query_search_index()`
3. Adjust the search endpoint in `server.py` as needed

## Dependencies

- Python 3.12+
- FastAPI and Uvicorn
- KOI-net libraries (v1.0.0b12+)
- RID-lib (v3.2.3+)
- Additional libraries listed in pyproject.toml

## Limitations and Future Work

- The current implementation uses an in-memory index (data is lost on restart)
- Search is limited to SHA and basic keyword matching
- More advanced text analysis could be implemented for better search results
- Persistent storage could be added for the index
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/core.py">
import os
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the final config loader
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
)

# Import the RID type this processor consumes
logger = logging.getLogger(__name__)


name = "processor-b"

# Identity directory setup
identity_dir = f".koi/{name}"
os.makedirs(identity_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Check required config values
if not BASE_URL or not COORDINATOR_URL or not CACHE_DIR:
    raise ValueError(
        "Essential configuration (BASE_URL, COORDINATOR_URL, CACHE_DIR) is missing or failed to load!"
    )

# Initialize the KOI-net Node Interface for Processor B
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(  # Processor B provides no new RIDs per PRD
            event=[], state=[]
        ),
        # Consumes HackMDNote implicitly via handlers
    ),
    use_kobj_processor_thread=True,
    first_contact=COORDINATOR_URL,
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,
)

logger.info(f"Initialized NodeInterface for Processor B: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")

# Import handlers after node is initialized
from . import handlers
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/handlers.py">
import logging

from .core import node
from koi_net.processor import ProcessorInterface, HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode, KoiNetEdge
from rid_types.hackmd import HackMDNote

# Import config to potentially check for specific sensor RID
from .config import HACKMD_SENSOR_RID


logger = logging.getLogger(__name__)

# Simple in-memory index (as defined in Processor.md)
# Structure:
# search_index = { "tag": [rid_str1, rid_str2], "title_word": [rid_str1, rid_str3], note_id: [rid_str] }
# note_metadata = { rid_str: {"title": title, "tags": tags, "lastChangedAt": ts}}
search_index = {}
note_metadata = {}


# --- Network Handlers ---
@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of Coordinator and potential HackMD Sensor nodes."""
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if not profile or not profile.provides:
            logger.warning(
                f"Received KoiNetNode event for {kobj.rid} with invalid/missing profile."
            )
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    # --- Coordinator/Peer Handshake ---
    if kobj.rid != processor.identity.rid:
        logger.info(
            f"Discovered potential peer/coordinator: {kobj.rid}. Proposing edge."
        )
        try:
            edge_bundle = generate_edge_bundle(
                source=kobj.rid,
                target=processor.identity.rid,
                edge_type=EdgeType.WEBHOOK,
                rid_types=[KoiNetNode, KoiNetEdge],
            )
            processor.handle(bundle=edge_bundle)
        except Exception as e:
            logger.error(f"Failed edge proposal to {kobj.rid}: {e}", exc_info=True)

    # --- HackMD Sensor Discovery & Handshake ---
    provides_hackmd_notes = False
    if HackMDNote != object:  # Check if HackMDNote was imported successfully
        # Check if the node provides HackMDNote events or state
        if HackMDNote in profile.provides.event or HackMDNote in profile.provides.state:
            provides_hackmd_notes = True

    if provides_hackmd_notes:
        # Check if a specific sensor RID is configured
        if HACKMD_SENSOR_RID and str(kobj.rid) != HACKMD_SENSOR_RID:
            logger.debug(
                f"Discovered HackMD sensor {kobj.rid}, but configured to connect only to {HACKMD_SENSOR_RID}. Ignoring."
            )
            return

        logger.info(f"Discovered HackMD Sensor: {kobj.rid}. Proposing edge.")
        try:
            # Propose an edge TO the sensor to receive events
            edge_bundle = generate_edge_bundle(
                source=kobj.rid,  # Sensor is the source
                target=processor.identity.rid,  # We are the target
                edge_type=EdgeType.WEBHOOK,
                rid_types=[HackMDNote],  # We want HackMDNote events
            )
            processor.handle(bundle=edge_bundle)
            logger.info(f"Edge proposed to HackMD Sensor {kobj.rid}")
        except Exception as e:
            logger.error(
                f"Failed edge proposal to HackMD Sensor {kobj.rid}: {e}", exc_info=True
            )


# --- Manifest Handler ---
@node.processor.register_handler(HandlerType.Manifest, rid_types=[HackMDNote])
def handle_note_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles incoming note manifests, triggers bundle fetch for indexing."""
    if HackMDNote == object:
        logger.error("HackMDNote type not properly imported. Cannot process manifests.")
        return

    if not isinstance(kobj.rid, HackMDNote):
        logger.warning(f"Handler received non-HackMDNote RID: {kobj.rid}. Skipping.")
        return

    manifest = kobj.manifest
    rid: HackMDNote = kobj.rid
    logger.info(f"Received manifest for HackMD note: {rid.reference}")

    # PRD: Always dereference notes for indexing.
    logger.debug(f"Requesting bundle for {rid} for indexing.")
    try:
        processor.handle(
            rid=rid, source=KnowledgeSource.External
        )  # Trigger bundle fetch
    except Exception as e:
        logger.error(f"Error requesting bundle for {rid}: {e}", exc_info=True)


# --- Bundle Handler ---
@node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
def handle_note_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Processes note bundle contents and updates the search index."""
    if HackMDNote == object:
        logger.error("HackMDNote type not properly imported. Cannot process bundles.")
        return

    if not isinstance(kobj.rid, HackMDNote):
        logger.warning(f"Handler received non-HackMDNote RID: {kobj.rid}. Skipping.")
        return

    if not kobj.contents or not isinstance(kobj.contents, dict):
        logger.warning(f"Bundle for {kobj.rid} has no contents or invalid format.")
        return

    rid: HackMDNote = kobj.rid
    contents = kobj.contents
    rid_str = str(rid)  # Use string representation for dict keys
    note_id = rid.note_id
    title = contents.get("title", f"Note {note_id}")  # Use ID if title missing

    logger.info(f"Processing bundle for note: {note_id} - '{title}'")

    # --- Check Timestamp to Avoid Re-indexing ---
    last_changed = contents.get("lastChangedAt")
    if rid_str in note_metadata and last_changed == note_metadata[rid_str].get(
        "lastChangedAt"
    ):
        logger.debug(
            f"Note {note_id} has not changed ({last_changed}) since last index. Skipping."
        )
        return

    # --- Update Metadata Cache ---
    current_tags = contents.get("tags", [])
    note_metadata[rid_str] = {
        "title": title,
        "tags": current_tags,
        "lastChangedAt": last_changed,
    }

    # --- Update Search Index (Example: Tags, Title words, Note ID) ---
    # Clear old index entries for this note first
    for key, rid_list in list(search_index.items()):
        if isinstance(rid_list, list) and rid_str in rid_list:
            search_index[key].remove(rid_str)
            # Remove key if list becomes empty after removing the RID
            if not search_index[key]:
                del search_index[key]

    # Index by tags
    for tag in current_tags:
        tag_key = tag.lower()  # Case-insensitive tag indexing
        if tag_key not in search_index:
            search_index[tag_key] = []
        # Avoid adding duplicates if somehow the clear logic failed
        if rid_str not in search_index[tag_key]:
            search_index[tag_key].append(rid_str)

    # Index by title words
    for word in title.lower().split():
        if len(word) > 2:  # Basic filtering
            if word not in search_index:
                search_index[word] = []
            if rid_str not in search_index[word]:
                search_index[word].append(rid_str)

    # Index by note ID itself for direct lookup
    note_id_key = note_id  # Use the actual note ID as the key
    if note_id_key not in search_index:
        search_index[note_id_key] = []
    if rid_str not in search_index[note_id_key]:
        search_index[note_id_key].append(rid_str)

    # Note: Markdown content parsing is omitted as per simplified scope.
    # md_content = contents.get("content", "")
    # If implemented, parse md_content and add relevant keywords/entities to search_index.

    logger.debug(
        f"Updated search index for note {note_id}. Index size (keys): {len(search_index)}"
    )


# --- Helper for Search Endpoint ---
def query_note_index(query: str) -> list:
    """Queries the in-memory note search index."""
    results_rids = set()  # Use a set to automatically handle duplicates
    query_lower = query.lower()

    # 1. Check if query is a direct Note ID match
    if query in search_index and isinstance(search_index[query], list):
        results_rids.update(search_index[query])

    # 2. Check if query matches a tag (case-insensitive)
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        results_rids.update(search_index[query_lower])

    # 3. Check if query matches a title word (case-insensitive)
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        results_rids.update(search_index[query_lower])

    # Format results using metadata cache
    results = []
    for rid_str in results_rids:
        meta = note_metadata.get(rid_str, {})  # Get cached metadata
        results.append(
            {
                "rid": rid_str,
                "title": meta.get("title", "N/A"),
                "tags": meta.get("tags", []),
            }
        )

    # Optional: Sort results, e.g., alphabetically by title
    results.sort(key=lambda x: x.get("title", "").lower())

    return results


logger.info("Processor B handlers registered.")
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/server.py">
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, HTTPException

from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from koi_net.processor.knowledge_object import KnowledgeSource

from .core import node  # Import the initialized node instance

# Import the query helper from handlers
from .handlers import query_note_index

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup and shutdown."""
    logger.info("Starting Processor B lifespan...")
    try:
        node.start()
        logger.info("Processor B KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        raise RuntimeError("Failed to initialize KOI-net node") from e

    yield  # Application runs here

    logger.info("Shutting down Processor B...")
    try:
        node.stop()
        logger.info("Processor B KOI-net node stopped successfully.")
    except Exception as e:
        logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
    logger.info("Processor B shutdown complete.")


app = FastAPI(
    title="KOI-net Processor B Node (Note Indexer)",
    description="Processes HackMD notes and provides a search API.",
    version="0.1.0",
    lifespan=lifespan,
)

# --- KOI Protocol Router ---
koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    if not req.events:
        logger.debug("Received empty broadcast event list.")
        return {}
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        try:
            node.processor.handle(event=event, source=KnowledgeSource.External)
        except Exception as e:
            logger.error(
                f"Error handling broadcast event {event.rid}: {e}", exc_info=True
            )
    return {}


@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH} for {req.rid}")
    try:
        events = node.network.flush_poll_queue(req.rid)
        logger.debug(f"Returning {len(events)} events for {req.rid}")
        return EventsPayload(events=events)
    except Exception as e:
        logger.error(f"Error polling events for {req.rid}: {e}", exc_info=True)
        return EventsPayload(events=[])


@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    try:
        return node.network.response_handler.fetch_rids(req)
    except Exception as e:
        logger.error(f"Error fetching RIDs: {e}", exc_info=True)
        return RidsPayload(rids=[])


@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    try:
        return node.network.response_handler.fetch_manifests(req)
    except Exception as e:
        logger.error(f"Error fetching Manifests: {e}", exc_info=True)
        return ManifestsPayload(manifests=[], not_found=req.rids or [])


@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    try:
        return node.network.response_handler.fetch_bundles(req)
    except Exception as e:
        logger.error(f"Error fetching Bundles: {e}", exc_info=True)
        return BundlesPayload(bundles=[], not_found=req.rids or [])


# Add health endpoint
@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed
    return {"status": "healthy"}


app.include_router(koi_net_router)

# --- Custom Search API Router ---
search_router = APIRouter()


@search_router.get("/search")
async def search_notes_endpoint(q: str):
    """Endpoint to search the indexed note data."""
    if not q:
        raise HTTPException(status_code=400, detail="Query parameter 'q' is required.")

    logger.info(f"Search request received: q='{q}'")
    try:
        # Use the helper function from handlers
        results = query_note_index(q)
        logger.info(f"Search for '{q}' yielded {len(results)} results.")
        return {"query": q, "results": results}
    except Exception as e:
        logger.error(f"Error during search for query '{q}': {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail="Internal server error during search."
        )


app.include_router(search_router)

logger.info("Processor B FastAPI application configured with KOI and Search routers.")
</file>

<file path="nodes/koi-net-processor-b-node/pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "processor-b-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "httpx", 
    "python-dotenv",
    "rid-lib>=3.2.3", # Ensure compatible versions
    "koi-net==1.0.0b12", 
    "rich", 
    "ruamel.yaml",
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

<file path="nodes/koi-net-processor-b-node/README.md">
# Processor B - HackMD Note Indexer

## Core Logic Overview

This section provides an in-depth look at Processor B's implementation details, highlighting the key components and logic that drive its note indexing functionality.

### 1. Key Constants & Classes

- **`HACKMD_SENSOR_RID`**: Optional configuration parameter that allows specifying a particular HackMD sensor node to connect to, instead of auto-discovering available HackMD sensors.
- **`HackMDNote`**: The primary RID class that Processor B consumes, representing HackMD note data.
- **`search_index`**: In-memory structure that maps search terms (tags, title words, note IDs) to matching note RIDs.
- **`note_metadata`**: In-memory cache that stores metadata for indexed notes to avoid re-indexing unchanged content and enrich search results.

### 2. How KOI & RID-lib Are Used

#### Node Interface Initialization

```python
node = NodeInterface(
    name="processor-b",
    profile=NodeProfile(
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[],
            state=[]
        ),
        # Consumes HackMDNote implicitly via handlers
    ),
    use_kobj_processor_thread=True,
    first_contact=COORDINATOR_URL,
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    cache_directory_path=CACHE_DIR,
)
```

#### Registered Event Handlers

1. **Network Handler**: Processes node discovery events

   ```python
   @node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
   def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Handles discovery of Coordinator and HackMD Sensor nodes
   ```

2. **Manifest Handler**: Processes note manifest events

   ```python
   @node.processor.register_handler(HandlerType.Manifest, rid_types=[HackMDNote])
   def handle_note_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Handles incoming note manifests, triggers bundle fetch
   ```

3. **Bundle Handler**: Processes note bundle data
   ```python
   @node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
   def handle_note_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
       # Processes note bundle contents and updates the search index
   ```

#### RID-lib Usage

Processor B uses RID-lib for:

- Handling standard KOI-net RIDs (`KoiNetNode`, `KoiNetEdge`)
- Processing HackMD note RIDs represented as `HackMDNote` classes with `note_id` properties
- Automatic bundle dereferencing using the processor's built-in mechanisms
- RID string conversion for indexing: `rid_str = str(rid)`

### 3. Proprietary Analysis Logic

The core analysis logic centers around efficient note indexing and change detection:

```python
# Check if note has changed since last indexing
last_changed = contents.get("lastChangedAt")
if rid_str in note_metadata and last_changed == note_metadata[rid_str].get("lastChangedAt"):
    logger.debug(f"Note {note_id} has not changed ({last_changed}) since last index. Skipping.")
    return

# Update metadata cache
current_tags = contents.get("tags", [])
note_metadata[rid_str] = {
    "title": title,
    "tags": current_tags,
    "lastChangedAt": last_changed
}

# Index by tags (critical for topic-based search)
for tag in current_tags:
    tag_key = tag.lower()  # Case-insensitive tag indexing
    if tag_key not in search_index:
        search_index[tag_key] = []
    if rid_str not in search_index[tag_key]:
        search_index[tag_key].append(rid_str)

# Index by title words (enables title-based discovery)
for word in title.lower().split():
    if len(word) > 2:  # Basic filtering
        if word not in search_index:
            search_index[word] = []
        if rid_str not in search_index[word]:
            search_index[word].append(rid_str)
```

The indexing strategy includes:

- Change detection using `lastChangedAt` timestamps to avoid redundant processing
- Tag-based indexing for topical organization and search
- Title word indexing for content relevance
- Direct note ID indexing for exact lookups
- Metadata caching for enriched search results

### 4. Custom Modules & Functions

The node is implemented with the following modular structure:

| Module          | Key Functions                  | Purpose                                     |
| --------------- | ------------------------------ | ------------------------------------------- |
| **core.py**     | `NodeInterface` initialization | Establishes node identity and connections   |
| **handlers.py** | `handle_network_discovery()`   | Detects Coordinator and HackMD sensor nodes |
|                 | `handle_note_manifest()`       | Processes note manifest events              |
|                 | `handle_note_bundle()`         | Indexes note data from bundles              |
|                 | `query_note_index()`           | Implements search functionality             |
| **server.py**   | `broadcast_events_endpoint()`  | Receives events from other nodes            |
|                 | `search_notes_endpoint()`      | Exposes the search API                      |

#### Search Query Implementation

```python
def query_note_index(query: str) -> list:
    """Queries the in-memory note search index."""
    results_rids = set()  # Use a set to automatically handle duplicates
    query_lower = query.lower()

    # 1. Check if query is a direct Note ID match
    if query in search_index and isinstance(search_index[query], list):
         results_rids.update(search_index[query])

    # 2. Check if query matches a tag (case-insensitive)
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        results_rids.update(search_index[query_lower])

    # 3. Check if query matches a title word (case-insensitive)
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        results_rids.update(search_index[query_lower])

    # Format results using metadata cache for rich responses
    results = []
    for rid_str in results_rids:
        meta = note_metadata.get(rid_str, {})
        results.append({
            "rid": rid_str,
            "title": meta.get("title", "N/A"),
            "tags": meta.get("tags", [])
        })

    # Sort results alphabetically by title
    results.sort(key=lambda x: x.get("title", "").lower())

    return results
```

### 5. Configuration & Environment Variables

Key configuration parameters include:

| Parameter           | Default                                                                           | Purpose                      |
| ------------------- | --------------------------------------------------------------------------------- | ---------------------------- |
| `BASE_URL`          | http://{host}:{port}/koi-net                                                      | The node's public endpoint   |
| `COORDINATOR_URL`   | http://coordinator:8080/koi-net (Docker)<br>http://127.0.0.1:8080/koi-net (Local) | The Coordinator node URL     |
| `CACHE_DIR`         | /data/cache (Docker)<br>./.koi/processor-b/cache (Local)                          | Location for cached data     |
| `HOST`              | 0.0.0.0 (Docker)<br>127.0.0.1 (Local)                                             | Interface to bind to         |
| `PORT`              | 8012                                                                              | HTTP server port             |
| `LOG_LEVEL`         | INFO                                                                              | Logging verbosity            |
| `HACKMD_SENSOR_RID` | None                                                                              | Optional specific sensor RID |

The configuration is loaded from YAML files in `config/docker/` or `config/local/` depending on the deployment mode, which is controlled by the `KOI_CONFIG_MODE` and `RUN_CONTEXT` environment variables.

### 6. Sample Snippet: Core Processing Flow

The following code demonstrates the complete lifecycle of processing a HackMD note:

```python
# Step 1: Discover and connect to HackMD sensor
@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
    # Validate discovered node
    profile = kobj.bundle.validate_contents(NodeProfile)

    # Check if the node provides HackMD notes
    provides_hackmd_notes = False
    if HackMDNote in profile.provides.event or HackMDNote in profile.provides.state:
        provides_hackmd_notes = True

    # If it's a HackMD sensor, propose an edge to receive notes
    if provides_hackmd_notes:
        # Check if we're configured to use a specific sensor
        if HACKMD_SENSOR_RID and str(kobj.rid) != HACKMD_SENSOR_RID:
            logger.debug(f"Discovered HackMD sensor {kobj.rid}, but configured to connect only to {HACKMD_SENSOR_RID}. Ignoring.")
            return

        logger.info(f"Discovered HackMD Sensor: {kobj.rid}. Proposing edge.")
        edge_bundle = generate_edge_bundle(
            source=kobj.rid,  # Sensor is the source
            target=processor.identity.rid,  # We are the target
            edge_type=EdgeType.WEBHOOK,
            rid_types=[HackMDNote],  # We want HackMDNote events
        )
        processor.handle(bundle=edge_bundle)

# Step 2: Receive note manifest and request bundle
@node.processor.register_handler(HandlerType.Manifest, rid_types=[HackMDNote])
def handle_note_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    rid = kobj.rid
    logger.info(f"Received manifest for HackMD note: {rid.note_id}")

    # Request full bundle for indexing
    processor.handle(rid=rid, source=KnowledgeSource.External)

# Step 3: Process note bundle and update index
@node.processor.register_handler(HandlerType.Bundle, rid_types=[HackMDNote])
def handle_note_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    rid = kobj.rid
    contents = kobj.contents
    rid_str = str(rid)
    note_id = rid.note_id
    title = contents.get("title", f"Note {note_id}")

    logger.info(f"Processing bundle for note: {note_id} - '{title}'")

    # Skip unchanged notes using timestamp comparison
    last_changed = contents.get("lastChangedAt")
    if rid_str in note_metadata and last_changed == note_metadata[rid_str].get("lastChangedAt"):
        return

    # Update metadata and index the note
    current_tags = contents.get("tags", [])
    note_metadata[rid_str] = {
        "title": title,
        "tags": current_tags,
        "lastChangedAt": last_changed
    }

    # Index by tags, title words, and note ID
    # ... [indexing logic as shown earlier] ...
```

This code demonstrates how Processor B discovers HackMD sensors, establishes connections, receives note data, and builds its search index - all within the KOI-net protocol's event-driven architecture.

## Overview

The Processor B node is a specialized KOI-net component that consumes HackMD note data from the HackMD sensor node, constructs a comprehensive search index, and provides a search API for discovering and retrieving notes. As an integral part of the KOI-net knowledge mesh, it transforms raw note data into an easily searchable knowledge repository.

## Features

- Seamlessly connects to the KOI-net Coordinator and HackMD sensor nodes
- Processes HackMD note data as it arrives
- Builds a multi-faceted search index based on note metadata
- Offers a simple HTTP API for searching through indexed notes
- Fully implements the KOI protocol for network operations
- Supports both Docker and local deployment configurations

## Architecture

### Network Integration

Processor B establishes and maintains connections with:

1. **Coordinator Node**: For network discovery and mesh communication
2. **HackMD Sensor**: To receive HackMD note events and updates

The node leverages the KOI protocol's edge mechanism to establish these connections, automatically registering for the relevant Resource ID (RID) types:

```
Coordinator <---> Processor B <--- HackMD Sensor
```

### Data Processing Flow

1. **Network Discovery**: The network handler identifies HackMD sensor nodes
2. **Edge Establishment**: Automatically proposes edges to receive note events
3. **Manifest Processing**: Receives note manifests and requests full bundle data
4. **Index Construction**: Processes note bundles to build and update the search index
5. **Search Service**: Provides endpoints for querying the indexed notes

### Search Index Structure

The search index uses a dual in-memory structure:

```python
# Map search terms to matching RIDs
search_index = {
    "tag": [rid_str1, rid_str2],         # Tag-based lookup
    "title_word": [rid_str1, rid_str3],  # Title word lookup
    "note_id": [rid_str]                 # Direct ID lookup
}

# Store note metadata for rich search results
note_metadata = {
    rid_str: {
        "title": "Note Title",
        "tags": ["tag1", "tag2"],
        "lastChangedAt": timestamp
    }
}
```

This enables multiple search capabilities:

- Tag-based search
- Title word search
- Direct note ID lookup
- Change tracking to avoid reindexing unchanged notes

## API Endpoints

### KOI Protocol Endpoints

- `POST /koi-net/broadcast-events`: Receive events from other nodes
- `POST /koi-net/poll-events`: Provide events to nodes polling this one
- `POST /koi-net/fetch-rids`: Return RIDs matching requested types
- `POST /koi-net/fetch-manifests`: Return manifests for requested RIDs
- `POST /koi-net/fetch-bundles`: Return bundles for requested RIDs
- `GET /koi-net/health`: Check node health status

### Custom Endpoints

- `GET /search?q=<query>`: Search indexed notes
  - Query can be a note ID, tag, or word from a note title
  - Returns matching notes with titles and tags

## Configuration

### Local Mode

Configuration is loaded from `config/local/processor-b.yaml`:

```yaml
runtime:
  base_url: http://127.0.0.1:8012/koi-net
  cache_dir: ./.koi/processor-b/cache
  host: 127.0.0.1
  log_level: DEBUG
  port: 8012

edges:
  coordinator_url: http://127.0.0.1:8080/koi-net
# Optional: specify a particular HackMD sensor
# processor_b:
#   hackmd_sensor_rid: "..."
```

### Docker Mode

When running in Docker, configuration is loaded from `config/docker/processor-b.yaml`:

```yaml
runtime:
  base_url: http://processor-b:8012/koi-net
  cache_dir: /data/cache
  host: 0.0.0.0
  log_level: INFO
  port: 8012

edges:
  coordinator_url: http://coordinator:8080/koi-net
```

## Deployment

### Local Development

1. Install the required dependencies:

   ```
   pip install -e .
   ```

2. Start the node:
   ```
   python -m processor_b_node
   ```

### Docker Deployment

Use Docker Compose to deploy the entire KOI-net system:

```
docker-compose up
```

Or run only this specific node:

```
docker-compose up processor-b
```

## Development and Extension

The node follows a modular structure:

- `__init__.py`: Logging configuration
- `__main__.py`: Entry point for node execution
- `config.py`: Configuration loading and environment setup
- `core.py`: KOI-net node initialization
- `handlers.py`: Event processing and index management
- `server.py`: FastAPI application and endpoints

To enhance search capabilities:

1. Extend the indexing logic in `handlers.py`
2. Modify the query implementation in `query_note_index()`
3. Enhance the search endpoint response in `server.py`

Current indexing is based on:

- Note IDs (direct lookup)
- Tags (keyword categorization)
- Title words (content relevance)

## Dependencies

- Python 3.12+
- FastAPI and Uvicorn
- KOI-net libraries (v1.0.0b12+)
- RID-lib (v3.2.3+)
- Additional dependencies as listed in pyproject.toml

## Limitations and Future Work

- Current implementation uses in-memory storage (no persistence between restarts)
- Full-text search of note content is not implemented but could be added
- No pagination for search results (returns all matches)
- Could be extended with:
  - Persistent storage for the index
  - Advanced text analysis (stemming, entity extraction)
  - Support for more complex query syntax
  - Note content parsing and keyword extraction
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path

# Import LOG_LEVEL from config (will be defined there)
from .config import LOG_LEVEL

# Temporary log level until config is implemented
# TEMP_LOG_LEVEL = "INFO"

# Get the root logger
logger = logging.getLogger()
# Set the base level
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler
log_dir = Path(".koi/processor-a")  # Adjust node name
log_dir.mkdir(parents=True, exist_ok=True)
log_file_path = log_dir / "processor-a-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries later in config.py
# logging.getLogger("uvicorn.error").setLevel(logging.WARNING)

logger.info(
    f"Logging configured (Level: {LOG_LEVEL}). Console via Rich, File: {log_file_path}"
)
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/__main__.py">
import uvicorn
import logging

# Import HOST and PORT from config (will be defined there)
from .config import HOST, PORT

# Temporary values until config is implemented
# TEMP_HOST = "0.0.0.0"
# TEMP_PORT = 8011  # Default port for Processor A

logger = logging.getLogger(__name__)

logger.info(f"Processor A node starting on {HOST}:{PORT}")
uvicorn.run(
    "processor_a_node.server:app",  # Adjust app path
    host=HOST,
    port=PORT,
    log_config=None,
    reload=False,  # Enable reload for development
)
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/config.py">
import logging
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import Dict, Any

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker ---
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
    # In Docker, ensure KOI_CONFIG_MODE is set to docker if RUN_CONTEXT is docker
    if CONFIG_MODE != "docker":
        logger.warning(
            f"RUN_CONTEXT=docker but KOI_CONFIG_MODE='{CONFIG_MODE}'. Forcing KOI_CONFIG_MODE to 'docker'."
        )
        CONFIG_MODE = "docker"
else:
    # Assume local run, base path relative to this file's parent's parent
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

CONFIG_PATH = CONFIG_DIR / "processor-a.yaml"  # Specific config file
ENV_PATH = CONFIG_DIR / "global.env"

logger.info(f"Attempting to load config from: {CONFIG_PATH}")
logger.info(f"Attempting to load env from: {ENV_PATH}")

# Load env vars from the selected global.env first
if ENV_PATH.is_file():
    from dotenv import load_dotenv

    load_dotenv(dotenv_path=ENV_PATH, override=True)
    logger.info(f"Loaded environment variables from {ENV_PATH}")
else:
    logger.warning(f"Global environment file not found at {ENV_PATH}")

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None:  # Handle empty file case
            CONFIG = {}
        logger.info(f"Successfully loaded YAML config from {CONFIG_PATH}")
    except Exception as e:
        logger.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logger.error(f"Processor A config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Determine Run Context & Extract Settings ---
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"

RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
PROCESSOR_A_CONFIG: Dict[str, Any] = CONFIG.get(
    "processor_a", {}
)  # Processor specific settings

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/processor-a")  # Standard local path base
DOCKER_CACHE_DIR_DEFAULT = "/data/cache"  # Default for shared cache in Docker

# Base configuration values (URLs are now directly from the correct mode's YAML)
HOST: str = RUNTIME_CONFIG.get("host", "127.0.0.1" if not is_docker else "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8011)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()
BASE_URL = RUNTIME_CONFIG.get("base_url")  # Should be defined in yaml
COORDINATOR_URL = EDGES_CONFIG.get("coordinator_url")  # Should be defined in yaml

# Optional specific sensor RID
GITHUB_SENSOR_RID: str | None = PROCESSOR_A_CONFIG.get("github_sensor_rid")

# Determine Cache Dir
# Prioritize environment variable, then YAML, then fallback
env_cache_dir = os.getenv("RID_CACHE_DIR")
yaml_cache_dir = RUNTIME_CONFIG.get("cache_dir") # This might contain ${RID_CACHE_DIR}

if env_cache_dir:
    CACHE_DIR = env_cache_dir
elif yaml_cache_dir and "${RID_CACHE_DIR}" not in yaml_cache_dir: # If YAML is set and NOT the placeholder
    CACHE_DIR = yaml_cache_dir
else:
    # Fallback logic if neither env var nor direct YAML value is set
    if is_docker:
        CACHE_DIR = DOCKER_CACHE_DIR_DEFAULT # Should not happen if env var is set in compose
    else:
        LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
        CACHE_DIR = str(LOCAL_DATA_BASE / "cache") # Default local path
    logger.warning(
        f"RID_CACHE_DIR env var not set and yaml cache_dir missing or is placeholder. Falling back to default: {CACHE_DIR}"
    )

# Ensure the resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL.upper())
    # Set level for uvicorn access logs specifically if needed
    # logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("Processor A Configuration Loaded:")
logger.info(f"  Config Mode: {CONFIG_MODE}")
logger.info(f"  Is Docker Context: {is_docker}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Specific GitHub Sensor RID: {GITHUB_SENSOR_RID or 'Not Set'}")

# Check required config
if not BASE_URL:
    logger.critical("Configuration error: runtime.base_url is not set.")
if not COORDINATOR_URL:
    logger.critical("Configuration error: edges.coordinator_url is not set.")
if not CACHE_DIR:
    logger.critical(
        "Configuration error: runtime.cache_dir is not set or resolved correctly."
    )
</file>

<file path="nodes/koi-net-processor-a-node/processor_a_node/handlers.py">
import logging

from .core import node
from koi_net.processor import ProcessorInterface
from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode, KoiNetEdge
from rid_types.github import GithubCommit

# Import config to potentially check for specific sensor RID
from .config import GITHUB_SENSOR_RID

logger = logging.getLogger(__name__)

# Simple in-memory index (as defined in Processor.md)
# Structure: { sha: commit_message, keyword: [sha1, sha2, ...], ... }
search_index = {}


# --- Network Handlers ---
@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def handle_network_discovery(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of Coordinator and potential GitHub Sensor nodes."""
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    # Basic validation of the discovered node's profile
    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if not profile or not profile.provides:
            logger.warning(
                f"Received KoiNetNode event for {kobj.rid} with invalid/missing profile."
            )
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    # --- Coordinator Handshake ---
    # Assume any node providing KoiNetNode *could* be a coordinator/peer
    # Propose edge back for bidirectional comms if it's not ourselves
    if kobj.rid != processor.identity.rid:
        logger.info(
            f"Discovered potential peer/coordinator: {kobj.rid}. Proposing edge."
        )
        try:
            edge_bundle = generate_edge_bundle(
                source=kobj.rid,  # The discovered node is the source
                target=processor.identity.rid,  # We are the target
                edge_type=EdgeType.WEBHOOK,
                rid_types=[KoiNetNode, KoiNetEdge],  # What we expect to exchange
            )
            processor.handle(bundle=edge_bundle)
        except Exception as e:
            logger.error(f"Failed edge proposal to {kobj.rid}: {e}", exc_info=True)

    # --- GitHub Sensor Discovery & Handshake ---
    # Check if the discovered node provides the RIDs we need (GithubCommit)
    # Note: Using string comparison since we don't have the actual RID class
    provides_github_commits = False

    # Use string comparison for RID type detection
    rid_type_str = "orn:github.commit"
    provides_event = [
        str(rt) for rt in profile.provides.event if hasattr(profile.provides, "event")
    ]
    provides_state = [
        str(rt) for rt in profile.provides.state if hasattr(profile.provides, "state")
    ]

    if rid_type_str in provides_event or rid_type_str in provides_state:
        provides_github_commits = True

    if provides_github_commits:
        # Check if a specific sensor RID is configured
        if GITHUB_SENSOR_RID and str(kobj.rid) != GITHUB_SENSOR_RID:
            logger.debug(
                f"Discovered GitHub sensor {kobj.rid}, but configured to connect only to {GITHUB_SENSOR_RID}. Ignoring."
            )
            return

        logger.info(f"Discovered GitHub Sensor: {kobj.rid}. Proposing edge.")
        try:
            # Propose an edge TO the sensor to receive events
            edge_bundle = generate_edge_bundle(
                source=kobj.rid,  # Sensor is the source of events
                target=processor.identity.rid,  # We are the target
                edge_type=EdgeType.WEBHOOK,
                rid_types=[GithubCommit],  # Specify we want GithubCommit events
            )
            processor.handle(bundle=edge_bundle)
            logger.info(f"Edge proposed to GitHub Sensor {kobj.rid}")
        except Exception as e:
            logger.error(
                f"Failed edge proposal to GitHub Sensor {kobj.rid}: {e}", exc_info=True
            )


# --- Manifest Handler ---


@node.processor.register_handler(HandlerType.Manifest, rid_types=[GithubCommit])
def handle_commit_manifest(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles incoming commit manifests, triggers bundle fetch for indexing."""
    # Check if we can work with the RID
    try:
        rid = kobj.rid
        if not hasattr(rid, "reference"):
            logger.warning(
                f"Handler received RID without reference attribute: {rid}. Skipping."
            )
            return
    except Exception as e:
        logger.warning(f"Error checking RID {kobj.rid}: {e}")
        return

    manifest = kobj.manifest
    logger.info(f"Received manifest for commit: {rid.reference}")

    # PRD: Optionally dereference bundles. For indexing, we need the content.
    # Decision: Always dereference for this implementation.
    logger.debug(f"Requesting bundle for {rid} for indexing.")
    try:
        processor.handle(
            rid=rid, source=KnowledgeSource.External
        )  # Trigger bundle fetch
    except Exception as e:
        logger.error(f"Error requesting bundle for {rid}: {e}", exc_info=True)


# --- Bundle Handler ---


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Processes commit bundle contents and updates the search index."""
    # Check if we can work with the RID
    try:
        rid = kobj.rid
        if not hasattr(rid, "reference"):
            logger.warning(
                f"Handler received RID without reference attribute: {rid}. Skipping."
            )
            return
    except Exception as e:
        logger.warning(f"Error checking RID {kobj.rid}: {e}")
        return

    if not kobj.contents or not isinstance(kobj.contents, dict):
        logger.warning(f"Bundle for {kobj.rid} has no contents or invalid format.")
        return

    contents = kobj.contents
    sha = contents.get("sha")
    if not sha:
        logger.warning(
            f"Commit bundle {kobj.rid} missing SHA in contents. Skipping index update."
        )
        return

    logger.info(f"Processing bundle for commit: {sha[:7]}")

    # --- Update Search Index (Example Implementation) ---
    message = contents.get("message", "")

    # Clear old keyword associations for this SHA first if re-indexing
    for keyword, sha_list in list(search_index.items()):
        if isinstance(sha_list, list) and sha in sha_list:
            search_index[keyword].remove(sha)
            if not search_index[keyword]:  # Remove keyword if list becomes empty
                del search_index[keyword]

    # Index by full SHA
    search_index[sha] = message

    # Index by keywords (simple example)
    # Consider stemming, stop words, etc. for a real implementation
    keywords_processed = set()
    for keyword in message.lower().split():
        # Basic filtering: length > 3, alphanumeric, avoid duplicates per message
        if len(keyword) > 3 and keyword.isalnum() and keyword not in keywords_processed:
            if keyword not in search_index:
                search_index[keyword] = []
            # Add SHA to keyword list if not already present
            if sha not in search_index[keyword]:
                search_index[keyword].append(sha)
            keywords_processed.add(keyword)

    logger.debug(
        f"Updated search index for SHA: {sha[:7]}. Index size (keys): {len(search_index)}"
    )


# --- Helper for Search Endpoint ---
def query_search_index(query: str) -> list:
    """Queries the in-memory search index."""
    results = []
    query_lower = query.lower()

    # 1. Check if query is a SHA (full or partial >= 7 chars)
    if len(query) >= 7:
        # Check full SHA match
        if query in search_index and isinstance(search_index[query], str):
            # Need owner/repo to construct full RID - requires better index storage
            # For now, return SHA and message
            results.append(
                {
                    "sha": query,
                    "match_context": search_index[query][:100]
                    + "...",  # Truncate message
                }
            )
            return results  # Exact SHA match takes precedence

        # Check partial SHA match (less efficient)
        for sha_key, msg in search_index.items():
            # Ensure it's a SHA key (check length or type if index structure is mixed)
            if isinstance(msg, str) and sha_key.startswith(query):
                results.append({"sha": sha_key, "match_context": msg[:100] + "..."})
                # Optionally break after first partial match or collect all

    # 2. Check if query is a keyword
    if query_lower in search_index and isinstance(search_index[query_lower], list):
        for sha in search_index[query_lower]:
            # Avoid adding duplicates if already found via partial SHA match
            if not any(r["sha"] == sha for r in results):
                message = search_index.get(sha, "")  # Get message associated with SHA
                results.append({"sha": sha, "match_context": message[:100] + "..."})

    # 3. (Optional) Search within commit messages (less efficient)
    # for sha, message in search_index.items():
    #     if isinstance(message, str) and query_lower in message.lower():
    #         # Add logic to avoid duplicates
    #         results.append(...)

    return results


logger.info("Processor A handlers registered.")
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path

# Import LOG_LEVEL from config (will be defined there)
from .config import LOG_LEVEL

# Temporary log level until config is implemented
# TEMP_LOG_LEVEL = "INFO"

# Get the root logger
logger = logging.getLogger()
# Set the base level
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler
log_dir = Path(".koi/processor-b")  # Adjust node name
log_dir.mkdir(parents=True, exist_ok=True)
log_file_path = log_dir / "processor-b-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries later in config.py
# logging.getLogger("uvicorn.error").setLevel(logging.WARNING)

logger.info(
    f"Logging configured (Level: {LOG_LEVEL}). Console via Rich, File: {log_file_path}"
)
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/__main__.py">
import uvicorn
import logging

# Import HOST and PORT from config (will be defined there)
from .config import HOST, PORT

# Temporary values until config is implemented
# TEMP_HOST = "0.0.0.0"
# TEMP_PORT = 8012  # Default port for Processor B

logger = logging.getLogger(__name__)

logger.info(f"Processor B node starting on {HOST}:{PORT}")
uvicorn.run(
    "processor_b_node.server:app",  # Adjust app path
    host=HOST,
    port=PORT,
    log_config=None,
    reload=False,  # Enable reload for development
)
</file>

<file path="nodes/koi-net-processor-b-node/processor_b_node/config.py">
import logging
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import Dict, Any, List

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Config Loader: Supports config/local and config/docker ---
CONFIG_MODE = os.environ.get("KOI_CONFIG_MODE", "local")
if os.environ.get("RUN_CONTEXT") == "docker":
    CONFIG_BASE = Path("/config")
    if CONFIG_MODE != "docker":
        logger.warning(
            f"RUN_CONTEXT=docker but KOI_CONFIG_MODE='{CONFIG_MODE}'. Forcing KOI_CONFIG_MODE to 'docker'."
        )
        CONFIG_MODE = "docker"
else:
    CONFIG_BASE = Path(__file__).parent.parent.parent.parent / "config"
CONFIG_DIR = CONFIG_BASE / CONFIG_MODE

CONFIG_PATH = CONFIG_DIR / "processor-b.yaml"  # Specific config file
ENV_PATH = CONFIG_DIR / "global.env"

logger.info(f"Attempting to load config from: {CONFIG_PATH}")
logger.info(f"Attempting to load env from: {ENV_PATH}")

# Load env vars from the selected global.env first
if ENV_PATH.is_file():
    from dotenv import load_dotenv

    load_dotenv(dotenv_path=ENV_PATH, override=True)
    logger.info(f"Loaded environment variables from {ENV_PATH}")
else:
    logger.warning(f"Global environment file not found at {ENV_PATH}")

# Load YAML config using ruamel.yaml
CONFIG = {}
if CONFIG_PATH.is_file():
    try:
        yaml_loader = YAML(typ="safe")
        with open(CONFIG_PATH) as f:
            CONFIG = yaml_loader.load(f)
        if CONFIG is None:  # Handle empty file case
            CONFIG = {}
        logger.info(f"Successfully loaded YAML config from {CONFIG_PATH}")
    except Exception as e:
        logger.error(f"Error loading YAML config from {CONFIG_PATH}: {e}")
else:
    logger.error(f"Processor B config file not found: {CONFIG_PATH}")

# --- End Config Loader ---

# --- Determine Run Context & Extract Settings ---
is_docker = os.getenv("RUN_CONTEXT") == "docker" or CONFIG_MODE == "docker"

RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
PROCESSOR_B_CONFIG: Dict[str, Any] = CONFIG.get(
    "processor_b", {}
)  # Processor specific settings

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/processor-b")  # Standard local path base
DOCKER_CACHE_DIR_DEFAULT = "/data/cache"  # Default for shared cache in Docker

# Base configuration values
HOST: str = RUNTIME_CONFIG.get("host", "127.0.0.1" if not is_docker else "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8012)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()
BASE_URL = RUNTIME_CONFIG.get("base_url")  # Should be defined in yaml
COORDINATOR_URL = EDGES_CONFIG.get("coordinator_url")  # Should be defined in yaml

# Optional specific sensor RID
HACKMD_SENSOR_RID: str | None = PROCESSOR_B_CONFIG.get("hackmd_sensor_rid")

# Determine Cache Dir
# Prioritize environment variable, then YAML, then fallback
env_cache_dir = os.getenv("RID_CACHE_DIR")
yaml_cache_dir = RUNTIME_CONFIG.get("cache_dir")

if env_cache_dir:
    CACHE_DIR = env_cache_dir
elif yaml_cache_dir and "${RID_CACHE_DIR}" not in yaml_cache_dir:
    CACHE_DIR = yaml_cache_dir
else:
    if is_docker:
        CACHE_DIR = DOCKER_CACHE_DIR_DEFAULT
    else:
        LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)
        CACHE_DIR = str(LOCAL_DATA_BASE / "cache")
    logger.warning(
        f"RID_CACHE_DIR env var not set and yaml cache_dir missing or is placeholder. Falling back to default: {CACHE_DIR}"
    )

# Ensure the resolved CACHE_DIR exists
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL.upper())
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config ---
logger.info("Processor B Configuration Loaded:")
logger.info(f"  Config Mode: {CONFIG_MODE}")
logger.info(f"  Is Docker Context: {is_docker}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Specific HackMD Sensor RID: {HACKMD_SENSOR_RID or 'Not Set'}")

# Check required config
if not BASE_URL:
    logger.critical("Configuration error: runtime.base_url is not set.")
if not COORDINATOR_URL:
    logger.critical("Configuration error: edges.coordinator_url is not set.")
if not CACHE_DIR:
    logger.critical(
        "Configuration error: runtime.cache_dir is not set or resolved correctly."
    )
</file>

<file path="docker-compose.yaml">
services:
  coordinator:
    build:
      context: ./nodes/koi-net-coordinator-node
    ports:
      - "8080:8080"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
      - RID_CACHE_DIR=/data/cache
    volumes:
      - ./config:/config:ro
      - cache_data:${RID_CACHE_DIR:-/data/cache}
      - coordinator_state_data:/app/.koi/coordinator
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  github-sensor:
    build:
      context: ./nodes/koi-net-github-sensor-node
    ports:
      - "8001:8001"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
      - RID_CACHE_DIR=/data/cache
    volumes:
      - ./config:/config:ro
      - cache_data:${RID_CACHE_DIR:-/data/cache}
      - github_state_data:/app/.koi/github
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8001/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  hackmd-sensor:
    build:
      context: ./nodes/koi-net-hackmd-sensor-node
    ports:
      - "8002:8002"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
      - RID_CACHE_DIR=/data/cache
    volumes:
      - ./config:/config:ro
      - cache_data:${RID_CACHE_DIR:-/data/cache}
      - hackmd_state_data:/app/.koi/hackmd
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8002/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  processor-a:
    build:
      context: .
      dockerfile: ./nodes/koi-net-processor-a-node/Dockerfile
    ports:
      - "8011:8011"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
      - RID_CACHE_DIR=/data/cache
    volumes:
      - ./config:/config:ro
      - cache_data:${RID_CACHE_DIR:-/data/cache}
      - processor_a_state:/app/.koi/processor-a
    depends_on:
      coordinator:
        condition: service_healthy
      github-sensor:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8011/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  processor-b:
    build:
      context: .
      dockerfile: ./nodes/koi-net-processor-b-node/Dockerfile
    ports:
      - "8012:8012"
    env_file:
      - ./config/docker/global.env
    environment:
      - RUN_CONTEXT=docker
      - KOI_CONFIG_MODE=docker
      - RID_CACHE_DIR=/data/cache
    volumes:
      - ./config:/config:ro
      - cache_data:${RID_CACHE_DIR:-/data/cache}
      - processor_b_state:/app/.koi/processor-b
    depends_on:
      coordinator:
        condition: service_healthy
      hackmd-sensor:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - koinet
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8012/koi-net/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

volumes:
  cache_data:
    driver: local
  coordinator_state_data:
    driver: local
  github_state_data:
    driver: local
  hackmd_state_data:
    driver: local
  processor_a_state:
    driver: local
  processor_b_state:
    driver: local

networks:
  koinet:
    driver: bridge
</file>

</files>
