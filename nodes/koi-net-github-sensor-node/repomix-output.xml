This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
github_sensor_node/
  handlers/
    github.py
  __init__.py
  __main__.py
  backfill.py
  config.py
  core.py
  loader.py
  server.py
  types.py
  webhook.py
.env.example
.gitignore
Dockerfile
github-node.service
pyproject.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="github_sensor_node/handlers/github.py">
import logging
from ..core import node
from ..types import GithubCommit

from koi_net.processor.handler import HandlerType
from koi_net.processor.knowledge_object import KnowledgeObject, KnowledgeSource
from koi_net.processor.interface import ProcessorInterface
from koi_net.protocol.node import NodeProfile
from koi_net.protocol.event import EventType
from koi_net.protocol.edge import EdgeType
from koi_net.protocol.helpers import generate_edge_bundle
from rid_lib.types import KoiNetNode

logger = logging.getLogger(__name__)


@node.processor.register_handler(HandlerType.Network, rid_types=[KoiNetNode])
def coordinator_contact(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Handles discovery of the coordinator node (or other nodes providing KoiNetNode events).

    On discovering a NEW coordinator, proposes a WEBHOOK edge for bidirectional
    communication and requests a list of other known nodes (sync).
    (Based on refactor.md example)
    """
    if kobj.normalized_event_type != EventType.NEW:
        logger.debug(f"Ignoring non-NEW event for KoiNetNode {kobj.rid}")
        return

    try:
        profile = kobj.bundle.validate_contents(NodeProfile)
        if KoiNetNode not in profile.provides.event:
            logger.debug(
                f"Node {kobj.rid} does not provide KoiNetNode events. Ignoring."
            )
            return
    except Exception as e:
        logger.warning(f"Could not validate NodeProfile for {kobj.rid}: {e}")
        return

    logger.info(
        f"Identified potential coordinator/peer: {kobj.rid}; proposing WEBHOOK edge"
    )
    try:

        edge_bundle = generate_edge_bundle(
            source=kobj.rid,
            target=node.identity.rid,
            edge_type=EdgeType.WEBHOOK,
            rid_types=[KoiNetNode],
        )
        processor.handle(bundle=edge_bundle)
    except Exception as e:
        logger.error(
            f"Failed to generate or handle WEBHOOK edge bundle for {kobj.rid}: {e}",
            exc_info=True,
        )

    logger.info(f"Syncing network nodes from {kobj.rid}")
    try:
        payload = processor.network.request_handler.fetch_rids(
            kobj.rid, rid_types=[KoiNetNode]
        )
        if not payload or not payload.rids:
            logger.warning(f"Received empty RIDs payload from {kobj.rid} during sync.")
            return

        logger.debug(f"Received {len(payload.rids)} RIDs from {kobj.rid}")
        for rid in payload.rids:

            if rid == processor.identity.rid or processor.cache.exists(rid):
                continue
            logger.debug(f"Handling discovered RID from sync: {rid}")

            processor.handle(rid=rid, source=KnowledgeSource.External)
    except Exception as e:
        logger.error(f"Failed during network sync with {kobj.rid}: {e}", exc_info=True)


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit(processor: ProcessorInterface, kobj: KnowledgeObject):
    """
    Basic handler for processing GithubCommit bundles.
    Currently just logs information.

    Args:
        bundle: The Bundle object containing the GithubCommit RID and contents.
    """
    try:

        bundle = kobj.bundle
        rid: GithubCommit = bundle.rid
        contents: dict = bundle.contents

        logger.info(
            f"Processing commit: {rid} (Normalized Type: {kobj.normalized_event_type}, Source: {kobj.source})"
        )

        logger.debug(
            f"  Author: {contents.get('author_name')} <{contents.get('author_email')}>"
        )
        logger.debug(
            f"  Message: {contents.get('message', '').splitlines()[0][:80]}..."
        )
        logger.debug(f"  URL: {contents.get('html_url')}")

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


@node.processor.register_handler(HandlerType.Bundle, rid_types=[GithubCommit])
def handle_github_commit_bundle(processor: ProcessorInterface, kobj: KnowledgeObject):
    """Example handler demonstrating processing before cache write."""
    logger.debug(f"Handling GithubCommit bundle PRE-CACHE for {kobj.rid}")

    try:

        if kobj.contents and "message" in kobj.contents:
            logger.info(
                f"Processing commit {kobj.rid.sha[:7]}: {kobj.contents['message'].splitlines()[0]}"
            )

        return None

    except Exception as e:
        logger.error(
            f"Error handling GithubCommit bundle {kobj.rid}: {e}", exc_info=True
        )

        return None


logger.info("GithubCommit Bundle handler registered.")
</file>

<file path="github_sensor_node/__init__.py">
import logging
import logging.handlers
from rich.logging import RichHandler
from pathlib import Path  # Import Path

# Import LOG_LEVEL from refactored config
from .config import LOG_LEVEL

# Get the root logger
logger = logging.getLogger()
# Set the base level from config
logger.setLevel(LOG_LEVEL)

# Create Rich Handler for console (use configured level)
rich_handler = RichHandler(rich_tracebacks=True)
rich_handler.setLevel(LOG_LEVEL)
rich_format = "%(name)s - %(message)s"
rich_datefmt = "%Y-%m-%d %H:%M:%S"
rich_formatter = logging.Formatter(rich_format, datefmt=rich_datefmt)
rich_handler.setFormatter(rich_formatter)

# Create File Handler (use configured level, more verbose format)
log_dir = Path(".koi/github")
log_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
log_file_path = log_dir / "github-sensor-node-log.txt"

file_handler = logging.handlers.RotatingFileHandler(
    log_file_path, maxBytes=10 * 1024 * 1024, backupCount=3
)  # 10MB, 3 backups
file_handler.setLevel(LOG_LEVEL)
file_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
file_datefmt = "%Y-%m-%d %H:%M:%S"
file_formatter = logging.Formatter(file_format, datefmt=file_datefmt)
file_handler.setFormatter(file_formatter)

# Clear existing handlers (optional, prevents duplicate handlers on reload)
if logger.hasHandlers():
    logger.handlers.clear()

# Add the handlers
logger.addHandler(rich_handler)
logger.addHandler(file_handler)

# Optional: Set specific levels for noisy libraries
logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
logging.getLogger("github").setLevel(logging.WARNING)  # PyGithub can be verbose
</file>

<file path="github_sensor_node/__main__.py">
import uvicorn
import logging
from .config import HOST, PORT

logger = logging.getLogger(__name__)

logger.info(f"GitHub sensor node starting on {HOST}:{PORT}")
uvicorn.run(
    "github_sensor_node.server:app",
    host=HOST,
    port=PORT,
    log_config=None,
    reload=True,
)
</file>

<file path="github_sensor_node/backfill.py">
import logging
from github import Github, GithubException, RateLimitExceededException
from github.Commit import Commit
from rid_lib.ext import Bundle

# Assuming GithubCommit RID type is accessible
from .types import GithubCommit
from .core import node

# Import necessary config values
from .config import GITHUB_TOKEN, MONITORED_REPOS, LAST_PROCESSED_SHA, update_state_file

logger = logging.getLogger(__name__)

# Initialize GitHub client (authenticated if token provided)
# Use the GITHUB_TOKEN loaded from config
github_client = Github(GITHUB_TOKEN) if GITHUB_TOKEN else Github()
logger.info(f"GitHub client initialized. Authenticated: {bool(GITHUB_TOKEN)}")


def perform_backfill():
    """
    One-time startup backfill: fetch all commits since LAST_PROCESSED_SHA
    for each monitored repo, bundle them as NEW, and persist the latest SHA processed.
    Processes commits oldest-to-newest after fetching.
    """
    logger.info("Starting GitHub backfill process...")

    # Load the state dictionary once before the loop
    current_state = (
        LAST_PROCESSED_SHA.copy()
    )  # Use a copy to avoid modifying during iteration if needed
    newest_sha_processed_overall_map = (
        {}
    )  # Track newest SHA per repo processed in this run

    if not MONITORED_REPOS:
        logger.warning(
            "No repositories configured in MONITORED_REPOS. Backfill skipped."
        )
        return

    for repo_full_name in MONITORED_REPOS:
        try:
            owner, repo_name_only = repo_full_name.split("/")
            # Get the last processed SHA for *this specific repository*
            last_sha_for_repo = current_state.get(repo_full_name)
            logger.info(
                f"Backfilling repository: {repo_full_name} since SHA: {last_sha_for_repo or 'beginning'}"
            )

            gh_repo = github_client.get_repo(repo_full_name)
            commits_to_process_buffer: list[Commit] = []

            # Iterate commits newest-first until we find the last processed one
            paginated_commits = gh_repo.get_commits()
            logger.debug(f"Fetching commits for {repo_full_name}...")
            commit_count = 0
            for commit in paginated_commits:
                commit_count += 1
                # Check against the specific SHA for this repo
                if last_sha_for_repo and commit.sha == last_sha_for_repo:
                    logger.info(
                        f"Found last processed SHA {last_sha_for_repo} in {repo_full_name}. Stopping fetch for this repo."
                    )
                    break
                commits_to_process_buffer.append(commit)
                # Safety break for potentially huge repos without a known SHA
                # Adjust limit as needed
                if commit_count % 100 == 0:
                    logger.debug(
                        f"Fetched {commit_count} commits for {repo_full_name} so far..."
                    )
                # if commit_count > 1000:
                #    logger.warning(f"Reached fetch limit (1000) for {repo_full_name}. Consider adjusting.")
                #    break

            logger.info(
                f"Found {len(commits_to_process_buffer)} new commits in {repo_full_name} to backfill."
            )

            # Process commits oldest → newest
            newest_sha_processed_in_repo = None
            for commit in reversed(commits_to_process_buffer):
                try:
                    rid = GithubCommit(owner=owner, repo=repo_name_only, sha=commit.sha)
                    # Extract commit details carefully, handling potential missing attributes
                    author = commit.commit.author
                    committer = commit.commit.committer

                    contents = {
                        "sha": commit.sha,
                        "message": commit.commit.message,
                        "author_name": author.name if author else None,
                        "author_email": author.email if author else None,
                        "author_date": (
                            author.date.isoformat() if author and author.date else None
                        ),
                        "committer_name": committer.name if committer else None,
                        "committer_email": committer.email if committer else None,
                        "committer_date": (
                            committer.date.isoformat()
                            if committer and committer.date
                            else None
                        ),
                        "html_url": commit.html_url,
                        "parents": [
                            p.sha for p in commit.parents
                        ],  # List of parent SHAs
                    }
                    bundle = Bundle.generate(rid=rid, contents=contents)
                    # CORRECT USAGE: 'handle' makes the bundle available locally
                    # in the sensor's cache/event queue for consumers to poll/fetch.
                    # It does NOT push the application-specific GithubCommit bundle directly.
                    logger.debug(
                        f"Making backfill commit bundle {rid} available locally via sensor API."
                    )
                    node.processor.handle(bundle=bundle)

                    # Track the newest SHA processed in this run for this repo
                    newest_sha_processed_in_repo = commit.sha

                except Exception as e:
                    logger.error(
                        f"Error processing commit {commit.sha} in {repo_full_name}: {e}",
                        exc_info=True,
                    )

            # Store the newest SHA processed for this repo during this run
            if newest_sha_processed_in_repo:
                newest_sha_processed_overall_map[repo_full_name] = (
                    newest_sha_processed_in_repo
                )
                logger.debug(
                    f"Newest SHA processed for {repo_full_name} in this run: {newest_sha_processed_in_repo}"
                )

        except RateLimitExceededException:
            logger.error(
                f"GitHub API rate limit exceeded while backfilling {repo_full_name}. Aborting backfill. Try again later or use a GITHUB_TOKEN."
            )
            # Depending on requirements, could wait and retry, but for now, we stop.
            return  # Stop the entire backfill process
        except GithubException as e:
            logger.error(
                f"GitHub API error for repository {repo_full_name}: {e}. Skipping this repo."
            )
            continue  # Skip to the next repository
        except Exception as e:
            logger.error(
                f"Unexpected error backfilling repository {repo_full_name}: {e}",
                exc_info=True,
            )
            continue  # Skip to the next repository

    # After processing all repos, persist the newest SHAs found (if changed)
    updated_count = 0
    for repo_full_name, newest_sha in newest_sha_processed_overall_map.items():
        if newest_sha != current_state.get(repo_full_name):
            update_state_file(
                repo_full_name, newest_sha
            )  # Call function from config.py
            updated_count += 1
        else:
            logger.debug(
                f"No state update needed for {repo_full_name}, newest SHA {newest_sha} is same as stored."
            )

    if updated_count > 0:
        logger.info(
            f"Backfill complete. Updated state for {updated_count} repositories."
        )
    else:
        logger.info(
            f"Backfill complete. No new commits found or state changes required across monitored repositories."
        )


if __name__ == "__main__":
    # Example of how to run backfill directly for testing
    # Requires node to be started if handle() depends on active components
    # In practice, this is called by server.py during startup
    logging.basicConfig(level=logging.INFO)
    logger.info("Running backfill directly for testing...")
    # node.start() # Might be needed depending on node.processor.handle implementation
    perform_backfill()
    # node.stop()
</file>

<file path="github_sensor_node/config.py">
import logging
import json
import os
from ruamel.yaml import YAML
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv

# Configure basic logging early
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Load Environment Variables First ---
# Define potential paths for global.env
DOCKER_ENV_PATH = Path("/app/config/global.env")  # Path inside docker
LOCAL_ENV_PATH = Path(__file__).parent.parent.parent.parent / "config" / "global.env"

if DOCKER_ENV_PATH.is_file():
    load_dotenv(dotenv_path=DOCKER_ENV_PATH, override=True)
    logger.debug(f"Loaded environment variables from Docker path: {DOCKER_ENV_PATH}")
elif LOCAL_ENV_PATH.is_file():
    load_dotenv(dotenv_path=LOCAL_ENV_PATH, override=True)
    logger.debug(f"Loaded environment variables from Local path: {LOCAL_ENV_PATH}")
else:
    logger.warning(
        f"global.env file not found at Docker path {DOCKER_ENV_PATH} or local path {LOCAL_ENV_PATH}. Secrets might be missing."
    )
# --- End Environment Variable Loading ---


# Define potential configuration file paths for github-sensor.yaml
DOCKER_CONFIG_PATH = Path("/app/config/github-sensor.yaml")
LOCAL_CONFIG_PATH = (
    Path(__file__).parent.parent.parent.parent / "config" / "github-sensor.yaml"
)


def find_config_path() -> Path | None:
    """Finds the valid configuration file path."""
    if DOCKER_CONFIG_PATH.is_file():
        logger.debug(f"Using Docker config path: {DOCKER_CONFIG_PATH}")
        return DOCKER_CONFIG_PATH
    elif LOCAL_CONFIG_PATH.is_file():
        logger.debug(f"Using local config path: {LOCAL_CONFIG_PATH}")
        return LOCAL_CONFIG_PATH
    else:
        logger.error(
            f"Configuration file not found at Docker path {DOCKER_CONFIG_PATH} or local path {LOCAL_CONFIG_PATH}"
        )
        return None


def load_yaml_config() -> Dict[str, Any]:
    """Loads configuration from the YAML file."""
    config_path = find_config_path()
    if not config_path:
        logger.error("No valid YAML configuration file found. Using empty default.")
        return {}

    try:
        yaml = YAML(typ="safe")
        with open(config_path, "r") as f:
            config_data = yaml.load(f)
        logger.info(f"Successfully loaded YAML configuration from {config_path}")
        return config_data
    except Exception as e:
        logger.exception(f"Error loading YAML configuration from {config_path}: {e}")
        return {}


# Load YAML configuration
CONFIG = load_yaml_config()

# --- Determine Run Context ---
is_docker = os.getenv("RUN_CONTEXT") == "docker"

# --- Extract specific configurations with defaults ---
SENSOR_CONFIG: Dict[str, Any] = CONFIG.get("sensor", {})
EDGES_CONFIG: Dict[str, Any] = CONFIG.get("edges", {})
RUNTIME_CONFIG: Dict[str, Any] = CONFIG.get("runtime", {})
WEBHOOK_CONFIG: Dict[str, Any] = CONFIG.get("webhook", {})

# --- Context-Aware Configuration ---
LOCAL_DATA_BASE = Path("./.koi/github")  # Relative to workspace root for local runs

# Base configuration values
MONITORED_REPOS: List[str] = SENSOR_CONFIG.get("repos", [])
POLL_INTERVAL: int = SENSOR_CONFIG.get("poll_interval", 60)
SENSOR_MODE: str = SENSOR_CONFIG.get("mode", "webhook")
HOST: str = RUNTIME_CONFIG.get("host", "0.0.0.0")
PORT: int = RUNTIME_CONFIG.get("port", 8001)
LOG_LEVEL: str = RUNTIME_CONFIG.get("log_level", "INFO").upper()

# Adjust URLs
raw_coordinator_url = EDGES_CONFIG.get("coordinator_url")
if not is_docker and raw_coordinator_url:
    COORDINATOR_URL = raw_coordinator_url.replace(
        "http://coordinator:", f"http://localhost:"
    )
    logger.debug(f"Adjusted Coordinator URL for local run: {COORDINATOR_URL}")
else:
    COORDINATOR_URL = raw_coordinator_url

raw_base_url = RUNTIME_CONFIG.get("base_url")
if not is_docker and raw_base_url:
    BASE_URL = raw_base_url.replace("http://github-sensor:", f"http://localhost:")
    logger.debug(f"Adjusted Base URL for local run: {BASE_URL}")
else:
    BASE_URL = raw_base_url

# Adjust Cache and State Paths
DOCKER_CACHE_DIR = RUNTIME_CONFIG.get("cache_dir", "/data/cache")
# Read state file path from YAML, default to a path within CACHE_DIR if not specified
DOCKER_STATE_FILE = RUNTIME_CONFIG.get(
    "state_file", os.path.join(DOCKER_CACHE_DIR, "github_state.json")
)

if is_docker:
    CACHE_DIR = DOCKER_CACHE_DIR
    # Ensure state file path is absolute for Docker
    STATE_FILE_PATH = (
        Path(DOCKER_STATE_FILE)
        if Path(DOCKER_STATE_FILE).is_absolute()
        else Path("/app") / DOCKER_STATE_FILE
    )
else:
    LOCAL_DATA_BASE.mkdir(parents=True, exist_ok=True)  # Ensure local base dir exists
    CACHE_DIR = str(LOCAL_DATA_BASE / "cache")
    STATE_FILE_PATH = LOCAL_DATA_BASE / "github_state.json"  # Standardized local path
    logger.debug(f"Using local CACHE_DIR: {CACHE_DIR}")
    logger.debug(f"Using local STATE_FILE_PATH: {STATE_FILE_PATH}")

# Ensure resolved CACHE_DIR exists, especially for local runs
Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)

# --- Load Secrets from Environment Variables ---
GITHUB_TOKEN: str | None = os.getenv("GITHUB_TOKEN")
WEBHOOK_SECRET_ENV_VAR: str | None = WEBHOOK_CONFIG.get("secret_env_var")
GITHUB_WEBHOOK_SECRET: str | None = None
if WEBHOOK_SECRET_ENV_VAR:
    GITHUB_WEBHOOK_SECRET = os.getenv(WEBHOOK_SECRET_ENV_VAR)
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning(
            f"Environment variable '{WEBHOOK_SECRET_ENV_VAR}' specified in config but not found in environment."
        )
else:
    logger.warning("webhook.secret_env_var not specified in github-sensor.yaml")

# --- Update Logging Level Based on Config ---
try:
    logging.getLogger().setLevel(LOG_LEVEL)
    # Optionally set levels for other loggers
    logging.getLogger("uvicorn.error").setLevel(logging.WARNING)
    logging.getLogger("github").setLevel(logging.WARNING)  # PyGithub can be verbose
    logger.info(f"Logging level set to {LOG_LEVEL}")
except ValueError:
    logger.error(
        f"Invalid LOG_LEVEL '{LOG_LEVEL}' in configuration. Defaulting to INFO."
    )
    logging.getLogger().setLevel(logging.INFO)

# --- Log Loaded Config (Excluding Secrets) ---
logger.info("GitHub Sensor Configuration Loaded:")
logger.info(f"  Sensor Mode: {SENSOR_MODE}")
logger.info(f"  Monitored Repos: {MONITORED_REPOS}")
logger.info(f"  Coordinator URL: {COORDINATOR_URL}")
logger.info(f"  Runtime Base URL: {BASE_URL}")
logger.info(f"  Runtime Host: {HOST}")
logger.info(f"  Runtime Port: {PORT}")
logger.info(f"  Cache Dir: {CACHE_DIR}")
logger.info(f"  State File Path: {STATE_FILE_PATH}")  # Log the final path
logger.info(f"  GitHub Token Loaded: {bool(GITHUB_TOKEN)}")
logger.info(f"  Webhook Secret Loaded: {bool(GITHUB_WEBHOOK_SECRET)}")

# Check required config
if not COORDINATOR_URL:
    logger.critical(
        "Configuration error: edges.coordinator_url is not set or resolved correctly."
    )
if not BASE_URL:
    logger.critical(
        "Configuration error: runtime.base_url is not set or resolved correctly."
    )
if not GITHUB_WEBHOOK_SECRET:
    logger.warning("Configuration warning: GitHub Webhook Secret not loaded.")

# --- State Management (Loading initial state & update function) ---
LAST_PROCESSED_SHA: Dict[str, str] = {}  # Dictionary mapping repo_name -> last_sha


def load_state():
    """Loads the last processed SHA state from the JSON file specified by STATE_FILE_PATH."""
    global LAST_PROCESSED_SHA
    # Path is now guaranteed to be a Path object
    state_path = STATE_FILE_PATH
    try:
        with open(state_path, "r") as f:
            LAST_PROCESSED_SHA = json.load(f)
        logger.info(
            f"Loaded state from '{state_path}': Repos {list(LAST_PROCESSED_SHA.keys())}"
        )
    except FileNotFoundError:
        logger.warning(
            f"State file '{state_path}' not found. Starting with empty state."
        )
        LAST_PROCESSED_SHA = {}
    except json.JSONDecodeError:
        logger.error(
            f"Error decoding JSON from state file '{state_path}'. Starting with empty state."
        )
        LAST_PROCESSED_SHA = {}
    except Exception as e:
        logger.error(
            f"Unexpected error loading state file '{state_path}': {e}",
            exc_info=True,
        )
        LAST_PROCESSED_SHA = {}


def update_state_file(repo_name: str, last_sha: str):
    """Updates the state file with the latest processed SHA for a repo."""
    global LAST_PROCESSED_SHA
    LAST_PROCESSED_SHA[repo_name] = last_sha

    state_path = STATE_FILE_PATH  # Use the context-aware path
    try:
        # Ensure directory exists before writing
        state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(state_path, "w") as f:
            json.dump(LAST_PROCESSED_SHA, f, indent=4)
        logger.debug(
            f"Updated state file '{state_path}' for {repo_name} with SHA: {last_sha}"
        )
    except IOError as e:
        logger.error(f"Failed to write state file '{state_path}': {e}")
    except Exception as e:
        logger.error(
            f"Unexpected error writing state file '{state_path}': {e}",
            exc_info=True,
        )


# Load initial state when config module is imported
load_state()
</file>

<file path="github_sensor_node/core.py">
import os
import shutil
import logging

from koi_net import NodeInterface
from koi_net.protocol.node import NodeProfile, NodeType, NodeProvides

# Import necessary config values from the refactored config module
from .config import (
    BASE_URL,
    COORDINATOR_URL,
    CACHE_DIR,
    LOG_LEVEL,
)  # Import necessary vars
from .types import GithubCommit

logger = logging.getLogger(__name__)

name = "github"

# Identity and cache directory paths remain relative for setup,
# but NodeInterface gets absolute paths derived from config
identity_dir = f".koi/{name}"
# Cache dir is now defined in config, use that path for NodeInterface
# cache_dir_setup = f".koi/{name}/rid_cache_{name}"

# Clear existing state directories logic remains the same
# Consider making this behavior optional or configurable
logger.info(f"Attempting to clear existing identity directory: {identity_dir}")
shutil.rmtree(identity_dir, ignore_errors=True)
# Cache dir managed by Docker volume, no need to clear here
# shutil.rmtree(cache_dir, ignore_errors=True)

# Recreate the identity directory
os.makedirs(identity_dir, exist_ok=True)
# Ensure cache directory exists within the container is handled by Docker volume mount implicitly
# os.makedirs(cache_dir, exist_ok=True)
logger.info(f"Ensured identity directory exists: {identity_dir}")

# Ensure required config values are present
if not BASE_URL:
    raise ValueError("Runtime base_url is not configured in github-sensor.yaml")
if not COORDINATOR_URL:
    raise ValueError("Edges coordinator_url is not configured in github-sensor.yaml")

# Initialize the KOI-net Node Interface for the GitHub Sensor
node = NodeInterface(
    name=name,
    profile=NodeProfile(
        # Use BASE_URL from config
        base_url=BASE_URL,
        node_type=NodeType.FULL,
        provides=NodeProvides(
            event=[GithubCommit],
            state=[GithubCommit],
        ),
    ),
    use_kobj_processor_thread=True,
    # Use COORDINATOR_URL from config for first_contact
    first_contact=COORDINATOR_URL,
    # State file paths are now relative to the application root in the container
    identity_file_path=os.path.abspath(f"{identity_dir}/{name}_identity.json"),
    event_queues_file_path=os.path.abspath(f"{identity_dir}/{name}_event_queues.json"),
    # Use CACHE_DIR from config (should be /data/cache)
    cache_directory_path=CACHE_DIR,
)

logger.info(f"Initialized NodeInterface: {node.identity.rid}")
logger.info(f"Node attempting first contact with: {COORDINATOR_URL}")
</file>

<file path="github_sensor_node/loader.py">
from .core import node
from .handlers import github


def register_handlers():
    print("Registering GITHUB handlers...")
    _ = github
</file>

<file path="github_sensor_node/server.py">
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter, Request, Body, Header, HTTPException
from koi_net.processor.knowledge_object import KnowledgeSource
from koi_net.protocol.api_models import (
    PollEvents,
    FetchRids,
    FetchManifests,
    FetchBundles,
    EventsPayload,
    RidsPayload,
    ManifestsPayload,
    BundlesPayload,
)
from koi_net.protocol.consts import (
    BROADCAST_EVENTS_PATH,
    POLL_EVENTS_PATH,
    FETCH_RIDS_PATH,
    FETCH_MANIFESTS_PATH,
    FETCH_BUNDLES_PATH,
)
from .core import node
from .webhook import router as github_router
from .backfill import perform_backfill
from .loader import register_handlers

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage node startup, backfill, and shutdown."""
    logger.info("Starting FastAPI application lifespan...")
    # Start the KOI-net node
    try:
        register_handlers()
        node.start()
        logger.info("KOI-net node started successfully.")
    except Exception as e:
        logger.error(f"Failed to start KOI-net node: {e}", exc_info=True)
        raise RuntimeError("Failed to initialize KOI-net node") from e

    logger.info("Scheduling initial GitHub backfill...")
    backfill_task = asyncio.to_thread(perform_backfill)

    try:
        yield  # Application runs here
    finally:
        logger.info("Shutting down FastAPI application...")
        # Attempt to gracefully cancel the backfill if it's still running
        # This might require more sophisticated task management if backfill is long-running
        # if backfill_task and not backfill_task.done():
        #     try:
        #         backfill_task.cancel()
        #         await backfill_task
        #     except asyncio.CancelledError:
        #         logger.info("Backfill task cancelled.")
        #     except Exception as e:
        #         logger.error(f"Error cancelling backfill task: {e}", exc_info=True)

        try:
            node.stop()
            logger.info("KOI-net node stopped successfully.")
        except Exception as e:
            logger.error(f"Error stopping KOI-net node: {e}", exc_info=True)
        logger.info("FastAPI application shutdown complete.")


app = FastAPI(
    title="KOI-net GitHub Sensor Node",
    description="Listens for GitHub webhooks and performs backfill to ingest commit data.",
    version="0.1.0",
    lifespan=lifespan,
)

koi_net_router = APIRouter(prefix="/koi-net")


@koi_net_router.post(BROADCAST_EVENTS_PATH)
async def broadcast_events_endpoint(req: EventsPayload):
    logger.info(
        f"Request to {BROADCAST_EVENTS_PATH}, received {len(req.events)} event(s)"
    )
    for event in req.events:
        node.processor.handle(event=event, source=KnowledgeSource.External)
    return {}


@koi_net_router.post(POLL_EVENTS_PATH)
async def poll_events_endpoint(req: PollEvents) -> EventsPayload:
    logger.info(f"Request to {POLL_EVENTS_PATH}")
    events = node.network.flush_poll_queue(req.rid)
    return EventsPayload(events=events)


@koi_net_router.post(FETCH_RIDS_PATH)
async def fetch_rids_endpoint(req: FetchRids) -> RidsPayload:
    logger.info(f"Request to {FETCH_RIDS_PATH} for types {req.rid_types}")
    return node.network.response_handler.fetch_rids(req)


@koi_net_router.post(FETCH_MANIFESTS_PATH)
async def fetch_manifests_endpoint(req: FetchManifests) -> ManifestsPayload:
    logger.info(
        f"Request to {FETCH_MANIFESTS_PATH} for types {req.rid_types}, rids {req.rids}"
    )
    manifests_payload = node.network.response_handler.fetch_manifests(req)
    return manifests_payload  # The default handler already includes not_found


@koi_net_router.post(FETCH_BUNDLES_PATH)
async def fetch_bundles_endpoint(req: FetchBundles) -> BundlesPayload:
    logger.info(f"Request to {FETCH_BUNDLES_PATH} for rids {req.rids}")
    bundles_payload = node.network.response_handler.fetch_bundles(req)
    return bundles_payload


@koi_net_router.get("/health")
async def health():
    """Basic health check endpoint for Docker."""
    # Add more sophisticated checks if needed, e.g., node.is_running()
    return {"status": "healthy"}


app.include_router(koi_net_router)  # KOI-net API endpoints
app.include_router(github_router)  # GitHub webhook endpoint

logger.info("FastAPI application configured with webhook and KOI-net routers.")
</file>

<file path="github_sensor_node/types.py">
from rid_lib.core import ORN


class GithubCommit(ORN):
    """
    Resource Identifier (RID) for a specific GitHub commit.

    Format: orn:github.commit:<owner>/<repo>/<sha>
    Example: orn:github.commit:microsoft/vscode/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
    """

    namespace = "github.commit"

    def __init__(self, owner: str, repo: str, sha: str):
        """
        Initialize a GitHub commit RID.

        Args:
            owner: The repository owner (user or organization)
            repo: The repository name
            sha: The commit SHA (full 40-character or shortened)
        """
        if not owner or not repo or not sha:
            raise ValueError("Owner, repo, and SHA cannot be empty")

        if "/" in owner or "/" in repo:
            raise ValueError("Owner and repo cannot contain '/' character")

        self.owner = owner
        self.repo = repo
        self.sha = sha

    @property
    def reference(self) -> str:
        """Returns the reference part of the RID: '<owner>/<repo>/<sha>'."""
        return f"{self.owner}/{self.repo}/{self.sha}"

    @property
    def repository_full_name(self) -> str:
        """Returns the full repository name: '<owner>/<repo>'."""
        return f"{self.owner}/{self.repo}"

    @property
    def html_url(self) -> str:
        """Returns the HTML URL to view this commit on GitHub."""
        return f"https://github.com/{self.owner}/{self.repo}/commit/{self.sha}"

    @property
    def api_url(self) -> str:
        """Returns the GitHub API URL for this commit."""
        return (
            f"https://api.github.com/repos/{self.owner}/{self.repo}/commits/{self.sha}"
        )

    @classmethod
    def from_reference(cls, reference: str) -> "GithubCommit":
        """
        Creates a GithubCommit instance from its reference string.

        Args:
            reference: String in format '<owner>/<repo>/<sha>'

        Returns:
            GithubCommit instance

        Raises:
            ValueError: If the reference format is invalid
        """
        try:
            parts = reference.split("/", maxsplit=2)
            if len(parts) != 3:
                raise ValueError("Reference must contain exactly two '/' separators")

            owner, repo, sha = parts

            if not owner or not repo or not sha:
                raise ValueError("Owner, repo, and SHA parts cannot be empty")

            # Basic SHA length check
            if len(sha) < 7:  # Minimum length for a short SHA
                raise ValueError(f"SHA part seems too short: {sha}")

            return cls(owner=owner, repo=repo, sha=sha)

        except ValueError as e:
            raise ValueError(
                f"Invalid reference format for GithubCommit. Expected '<owner>/<repo>/<sha>', got '{reference}'. Error: {e}"
            ) from e
        except Exception as e:
            raise TypeError(
                f"Unexpected error parsing GithubCommit reference '{reference}': {e}"
            ) from e
</file>

<file path="github_sensor_node/webhook.py">
import logging
import hmac
import hashlib
import json
from fastapi import APIRouter, Request, Header, HTTPException, Body
from rid_lib.ext import Bundle
from .types import GithubCommit
from .core import node
from .config import (
    GITHUB_WEBHOOK_SECRET,
    MONITORED_REPOS,
    update_state_file,
    LAST_PROCESSED_SHA,
)

logger = logging.getLogger(__name__)

router = APIRouter()


async def verify_signature(request: Request, x_hub_signature_256: str = Header(None)):
    """Verify the GitHub webhook signature."""
    if not GITHUB_WEBHOOK_SECRET:
        logger.warning("Webhook verification skipped: GITHUB_WEBHOOK_SECRET not set.")
        return

    body = await request.body()
    hash_object = hmac.new(
        GITHUB_WEBHOOK_SECRET.encode("utf-8"), msg=body, digestmod=hashlib.sha256
    )
    expected_signature = "sha256=" + hash_object.hexdigest()

    if not hmac.compare_digest(expected_signature, x_hub_signature_256):
        logger.error(
            f"Webhook verification failed: Invalid signature. Expected: {expected_signature}, Got: {x_hub_signature_256}"
        )
        raise HTTPException(status_code=403, detail="Invalid signature")

    logger.debug("Webhook signature verified successfully.")


@router.post("/github/webhook", status_code=202)  # Use 202 Accepted as we process async
async def github_webhook(
    request: Request,
    x_github_event: str = Header(...),  # Required header
    x_hub_signature_256: str = Header(...),  # Required for verification
):
    """Handle incoming GitHub webhook events (specifically 'push')."""
    logger.info(f"Received GitHub webhook event: {x_github_event}")

    try:
        # --- Signature Verification (Optional) ---
        # await verify_signature(request, x_hub_signature_256)

        # --- Parse JSON Payload ---
        raw_body = await request.body()
        try:
            payload = json.loads(raw_body)
        except json.JSONDecodeError:
            logger.error("Invalid JSON in GitHub webhook payload")
            raise HTTPException(status_code=400, detail="Invalid JSON")

        # --- Event Handling ---
        if x_github_event == "ping":
            logger.info("Received 'ping' event from GitHub. Responding OK.")
            return {"message": "Pong!"}

        if x_github_event != "push":
            logger.debug(f"Ignoring non-'push' event: {x_github_event}")
            return {"message": f"Ignoring event type: {x_github_event}"}

        logger.info(f"Processing 'push' event: {payload}")

        # --- Process 'push' Event ---
        repo_info = payload.get("repository", {})
        repo_full_name = repo_info.get("full_name")
        repo_owner = repo_info.get("owner", {}).get("login") or repo_info.get(
            "owner", {}
        ).get("name")
        repo_name = repo_info.get("name")
        commits = payload.get("commits", [])
        head_commit = payload.get("head_commit", {})

        if not repo_full_name or not repo_owner or not repo_name:
            logger.error(f"Webhook payload missing repository details: {repo_info}")
            raise HTTPException(
                status_code=400, detail="Missing repository information in payload"
            )

        # Check if the repository is monitored
        if repo_full_name not in MONITORED_REPOS:
            logger.debug(
                f"Ignoring push event for non-monitored repository: {repo_full_name}"
            )
            return {"message": f"Repository {repo_full_name} not monitored"}

        if not commits and not head_commit:
            logger.warning(
                f"'push' event for {repo_full_name} received without 'commits' or 'head_commit' data. Possibly a branch deletion or tag push? Payload head: {payload.get('ref', '')}"
            )
            return {"message": "No commit data found in push event"}

        # Determine the commit(s) to process and the SHA to potentially update state with
        commits_to_process = []
        sha_to_update_state = None

        head_commit_id = head_commit.get("id")
        if head_commit_id:
            commits_to_process = [head_commit]  # Use head_commit as the primary source
            sha_to_update_state = (
                head_commit_id  # This is the SHA representing the push tip
            )
            logger.debug(f"Processing head_commit: {head_commit_id}")
        elif commits:
            commits_to_process = commits  # Fallback to commits list
            # If using commits list, the last commit's SHA is the best candidate for state update
            if commits:
                sha_to_update_state = commits[-1].get("id")
            logger.debug(
                f"Processing commits list (count: {len(commits)}). Potential state update SHA: {sha_to_update_state}"
            )
        else:
            # This case should ideally not be reached due to the check above, but included for completeness
            logger.warning(
                f"No processable commit data found in push event for {repo_full_name}. Skipping."
            )
            return {"message": "No processable commit data"}

        processed_new_commit = False
        for commit in commits_to_process:
            commit_sha = commit.get("id")
            if not commit_sha:
                logger.warning("Skipping commit in payload with missing 'id'.")
                continue

            # Avoid reprocessing the last known SHA for this repo
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if last_known_sha_for_repo and commit_sha == last_known_sha_for_repo:
                logger.debug(
                    f"Skipping commit {commit_sha} for {repo_full_name} as it matches last known SHA {last_known_sha_for_repo}."
                )
                continue

            # Inner try-except for processing individual commits within the push
            try:
                # Construct RID
                rid = GithubCommit(owner=repo_owner, repo=repo_name, sha=commit_sha)

                # Extract details - ensure keys exist
                author = commit.get("author", {})
                committer = commit.get("committer", {})

                contents = {
                    "sha": commit_sha,
                    "message": commit.get("message"),
                    "author_name": author.get("name"),
                    "author_email": author.get("email"),
                    "author_date": commit.get(
                        "timestamp"
                    ),  # GitHub often uses 'timestamp'
                    "committer_name": committer.get("name"),
                    "committer_email": committer.get("email"),
                    "committer_date": committer.get("timestamp"),
                    "html_url": commit.get("url"),  # Use 'url' from webhook payload
                    "parents": commit.get(
                        "parents", []
                    ),  # Typically a list of SHAs in webhook
                }

                bundle = Bundle.generate(rid=rid, contents=contents)
                # CORRECT USAGE: 'handle' makes the bundle available locally
                # in the sensor's cache/event queue for consumers to poll/fetch.
                # It does NOT push the application-specific GithubCommit bundle directly.
                logger.debug(
                    f"Making webhook commit bundle {rid} available locally via sensor API."
                )
                node.processor.handle(bundle=bundle)

                processed_new_commit = (
                    True  # Mark that we processed at least one new commit
                )

            except Exception as e:
                logger.error(
                    f"Error processing webhook commit {commit_sha} for {repo_full_name}: {e}",
                    exc_info=True,
                )
                # Decide whether to continue processing other commits in the push or stop
                continue  # Continue with next commit in the webhook push

        # Update state file only if we processed a new commit and have a valid SHA representing the push tip
        if processed_new_commit and sha_to_update_state:
            # Check again if the sha_to_update is different from the stored one before writing
            last_known_sha_for_repo = LAST_PROCESSED_SHA.get(repo_full_name)
            if sha_to_update_state != last_known_sha_for_repo:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. Updating state to SHA: {sha_to_update_state}"
                )
                update_state_file(repo_full_name, sha_to_update_state)
            else:
                logger.info(
                    f"Webhook processing complete for {repo_full_name}. State SHA {sha_to_update_state} already stored."
                )
        elif processed_new_commit:
            logger.warning(
                f"Webhook processing complete for {repo_full_name}. Processed new commit(s) but could not determine SHA for state update."
            )
        else:
            logger.info(
                f"Webhook processing complete for {repo_full_name}. No new commits processed or state updated."
            )

        return {"message": "Webhook processed successfully"}

    # Exception handlers are now correctly indented relative to the main 'try' block
    except HTTPException as he:
        # Re-raise HTTP exceptions to return proper status codes
        logger.warning(f"HTTP Exception during webhook processing: {he.detail}")
        raise he
    except Exception as e:
        logger.error(
            f"Unexpected error handling webhook event {x_github_event}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Internal server error handling webhook"
        )
</file>

<file path=".env.example">
# KOI-net GitHub Node Configuration

# Public URL where this GitHub node can be reached by other KOI-net nodes
URL="http://127.0.0.1:8001"

# URL of the coordinator node or another known KOI-net node
FIRST_CONTACT="http://127.0.0.1:8000/koi-net"

# Host the FastAPI server should listen on (0.0.0.0 allows external connections)
HOST="0.0.0.0"

# Port the FastAPI server should listen on
PORT="8001"

# GitHub Personal Access Token (replace with your own token)
GITHUB_TOKEN="your_github_token_here"

# Secret used to verify webhook signatures (generate a secure random string)
GITHUB_WEBHOOK_SECRET="your_webhook_secret_here"

# Comma-separated list of repositories to monitor (format: owner/repo)
MONITORED_REPOS="owner/repo1,owner/repo2"

# Path to the JSON file storing the last processed commit SHA
STATE_FILE_PATH="state.json"

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL="INFO"
</file>

<file path=".gitignore">
# Python build artifacts
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
.venv/
.env/

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
</file>

<file path="Dockerfile">
FROM python:3.12-slim

# Install UV package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# Copy dependency files first for better caching
COPY pyproject.toml /app/

# Install dependencies using UV
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -e .

# Copy the application code
COPY . /app/

# Expose the correct KOI port
EXPOSE 8001

# Add correct HEALTHCHECK instruction
HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
  CMD curl --fail http://localhost:8001/koi-net/health || exit 1

# Start service with correct entrypoint and port
CMD ["uvicorn", "github_sensor_node.server:app", "--host", "0.0.0.0", "--port", "8001"]
</file>

<file path="github-node.service">
[Unit]
Description=KOI-net Github Node Service
After=network.target

[Service]
WorkingDirectory=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/services/github
ExecStart=/Users/sayertindall/Documents/GitHub/block.science/koinets/koi-github-node/.venv/bin/python3 -m node
Restart=always

[Install]
WantedBy=multi-user.target
</file>

<file path="pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "github-sensor-node"
version = "0.1.0"
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "pydantic-settings",
    "httpx",
    "python-dotenv",
    "rid-lib>=3.2.3",
    "koi-net==1.0.0b12",
    "PyGithub",
    "aiohttp",
    "rich", # Added for logging
    "ruamel.yaml", # Added for YAML config loading
    "python-dotenv" # Added for loading .env in local runs
]

[tool.setuptools]
package-dir = {"" = "."}
</file>

</files>
